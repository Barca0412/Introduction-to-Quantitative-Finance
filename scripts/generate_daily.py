#!/usr/bin/env python3
"""
ç”Ÿæˆæ¯æ—¥è®ºæ–‡ Markdown é¡µé¢
è¾“å‡ºåˆ° è®ºæ–‡/AIé‡‘èè®ºæ–‡æ•´ç†/ ç›®å½• (ç”¨äº GitHub README å±•ç¤º)
"""

import json
from datetime import datetime
from pathlib import Path
from collections import defaultdict

# é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "papers" / "processed"
RAW_DATA_DIR = PROJECT_ROOT / "data" / "papers"
OUTPUT_DIR = PROJECT_ROOT / "è®ºæ–‡" / "AIé‡‘èè®ºæ–‡æ•´ç†"

# æ‰©å±•çš„æ ‡ç­¾åˆ°åˆ†ç±»æ˜ å°„
TAG_CATEGORIES = {
    # LLM & Agent
    "LLM": "LLM & Agent",
    "NLP": "LLM & Agent",
    "Sentiment Analysis": "LLM & Agent",
    "Financial Agent": "LLM & Agent",
    # èµ„äº§å®šä»·
    "Asset Pricing": "Asset Pricing",
    "Factor Model": "Asset Pricing",
    "Anomaly": "Asset Pricing",
    # å› å­æŒ–æ˜
    "Factor Mining": "Factor Mining",
    # è¡Œä¸ºé‡‘è
    "Behavioral Finance": "Behavioral Finance",
    "Investor Sentiment": "Behavioral Finance",
    # æœºå™¨å­¦ä¹ 
    "Deep Learning": "Machine Learning",
    "Reinforcement Learning": "Machine Learning",
    "Time Series": "Machine Learning",
    "Graph Neural Network": "Machine Learning",
    "Transformer": "Machine Learning",
    # æŠ•èµ„ç»„åˆä¸é£æ§
    "Portfolio Optimization": "Portfolio & Risk",
    "Risk Management": "Portfolio & Risk",
    # äº¤æ˜“ä¸å¸‚åœºå¾®è§‚ç»“æ„
    "Algorithmic Trading": "Trading & Microstructure",
    "High Frequency": "Trading & Microstructure",
    "Market Microstructure": "Trading & Microstructure",
    "Execution": "Trading & Microstructure",
    "Market Making": "Trading & Microstructure",
    # è¡ç”Ÿå“
    "Volatility": "Derivatives",
    "Options": "Derivatives",
    # å…¶ä»–
    "Benchmark": "Benchmark & Evaluation",
    "Quantitative Finance": "Other",
}


def generate_paper_markdown(paper: dict) -> str:
    """ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„ Markdown"""
    
    title = paper.get("chinese_title") or paper["title"]
    md = f"### [{title}]({paper['arxiv_url']})\n\n"
    
    if paper.get("chinese_title"):
        md += f"**åŸæ–‡**: {paper['title']}\n\n"
    
    authors = ", ".join(paper["authors"][:5])
    if len(paper["authors"]) > 5:
        authors += " et al."
    md += f"**ä½œè€…**: {authors}\n\n"
    
    tags = paper.get("tags", paper.get("categories", []))[:5]
    if tags:
        tag_str = " ".join([f"`{tag}`" for tag in tags])
        md += f"**æ ‡ç­¾**: {tag_str}\n\n"
    
    if paper.get("chinese_summary"):
        md += f"**ä¸­æ–‡æ¦‚è¿°**: {paper['chinese_summary']}\n\n"
    
    contributions = paper.get("key_contributions", [])
    if contributions:
        md += "**æ ¸å¿ƒè´¡çŒ®**:\n"
        for i, contrib in enumerate(contributions, 1):
            md += f"  {i}. {contrib}\n"
        md += "\n"
    
    md += "---\n\n"
    return md


def categorize_papers(papers: list) -> dict:
    """å°†è®ºæ–‡æŒ‰ç±»åˆ«åˆ†ç»„"""
    categories = defaultdict(list)
    
    for paper in papers:
        tags = paper.get("tags", [])
        
        category = "Other"
        for tag in tags:
            if tag in TAG_CATEGORIES:
                category = TAG_CATEGORIES[tag]
                break
        
        categories[category].append(paper)
    
    return dict(categories)


def generate_daily_page(date: str = None):
    """ç”Ÿæˆæ¯æ—¥è®ºæ–‡é¡µé¢"""
    if date is None:
        date = datetime.now().strftime("%Y-%m-%d")
    
    input_file = DATA_DIR / f"{date}.json"
    if not input_file.exists():
        input_file = RAW_DATA_DIR / f"{date}.json"
        if not input_file.exists():
            print(f"No papers found for {date}")
            return None
    
    with open(input_file, 'r', encoding='utf-8') as f:
        papers = json.load(f)
    
    if not papers:
        print(f"No papers to generate for {date}")
        return None
    
    daily_dir = OUTPUT_DIR / "daily"
    daily_dir.mkdir(parents=True, exist_ok=True)
    
    md = f"""# {date} AI+é‡‘èè®ºæ–‡æ—¥æŠ¥

> å…±æ”¶å½• **{len(papers)}** ç¯‡è®ºæ–‡ | [è¿”å›ç´¢å¼•](../README.md)

"""
    
    categorized = categorize_papers(papers)
    
    category_order = [
        "LLM & Agent",
        "Asset Pricing",
        "Factor Mining",
        "Machine Learning",
        "Portfolio & Risk",
        "Behavioral Finance",
        "Trading & Microstructure",
        "Derivatives",
        "Benchmark & Evaluation",
        "Other",
    ]
    
    for category in category_order:
        if category in categorized:
            md += f"## {category}\n\n"
            for paper in categorized[category]:
                md += generate_paper_markdown(paper)
    
    output_file = daily_dir / f"{date}.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md)
    
    print(f"Generated: {output_file}")
    
    update_main_index()
    
    return output_file


def update_main_index():
    """æ›´æ–°ä¸»ç´¢å¼•é¡µé¢"""
    daily_dir = OUTPUT_DIR / "daily"
    
    daily_files = sorted(daily_dir.glob("*.md"), reverse=True)
    
    stats = []
    for f in daily_files[:30]:
        date = f.stem
        try:
            data_file = DATA_DIR / f"{date}.json"
            if not data_file.exists():
                data_file = RAW_DATA_DIR / f"{date}.json"
            
            if data_file.exists():
                with open(data_file, 'r', encoding='utf-8') as df:
                    papers = json.load(df)
                    stats.append((date, len(papers)))
            else:
                stats.append((date, "?"))
        except:
            stats.append((date, "?"))
    
    md = """# AI+é‡‘èè®ºæ–‡æ•´ç†

æ¯æ—¥è‡ªåŠ¨æ›´æ–° arXiv ä¸Š AI+é‡‘èç›¸å…³è®ºæ–‡ï¼Œä½¿ç”¨ LLM ç”Ÿæˆä¸­æ–‡æ¦‚è¿°ä¸æ ‡ç­¾ã€‚

- ğŸ“° **æ¯æ—¥æ›´æ–°**: è‡ªåŠ¨æŠ“å– q-finã€cs.LG+finance ç›¸å…³è®ºæ–‡
- ğŸ¤– **æ™ºèƒ½åˆ†æ**: ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸­æ–‡æ‘˜è¦å’Œå…³é”®è´¡çŒ®
- ğŸ·ï¸ **ä¸»é¢˜åˆ†ç±»**: LLM & Agentã€Asset Pricingã€Factor Miningã€RLã€Trading ç­‰æ ‡ç­¾
- ğŸ”„ **å¢é‡æ›´æ–°**: è‡ªåŠ¨å»é‡ï¼Œåªå¤„ç†æ–°è®ºæ–‡

---

## ğŸ“… æ¯æ—¥æ›´æ–°

| æ—¥æœŸ | è®ºæ–‡æ•° | é“¾æ¥ |
|------|--------|------|
"""
    
    for date, count in stats:
        md += f"| {date} | {count} | [æŸ¥çœ‹](./daily/{date}.md) |\n"
    
    md += """
---

## ğŸ·ï¸ ä¸»é¢˜åˆ†ç±»

- [LLM & Agent](./topics/llm-agent.md) - å¤§è¯­è¨€æ¨¡å‹ä¸é‡‘èæ™ºèƒ½ä½“
- [Asset Pricing](./topics/asset-pricing.md) - èµ„äº§å®šä»·ä¸å› å­æ¨¡å‹
- [Factor Mining](./topics/factor-mining.md) - å› å­æŒ–æ˜
- [Machine Learning](./topics/machine-learning.md) - æœºå™¨å­¦ä¹ æ–¹æ³• (DL/RL/GNN/Transformer)
- [Portfolio & Risk](./topics/portfolio-risk.md) - æŠ•èµ„ç»„åˆä¸é£é™©ç®¡ç†
- [Trading & Microstructure](./topics/trading.md) - äº¤æ˜“ä¸å¸‚åœºå¾®è§‚ç»“æ„
- [Behavioral Finance](./topics/behavioral-finance.md) - è¡Œä¸ºé‡‘èå­¦
- [Derivatives](./topics/derivatives.md) - è¡ç”Ÿå“ä¸æ³¢åŠ¨ç‡
- [Benchmark](./topics/benchmark.md) - åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°

---

*è‡ªåŠ¨æ›´æ–°äº """ + datetime.now().strftime("%Y-%m-%d %H:%M") + """*
"""
    
    index_file = OUTPUT_DIR / "README.md"
    with open(index_file, 'w', encoding='utf-8') as f:
        f.write(md)
    
    print(f"Updated index: {index_file}")


def main():
    print("=" * 60)
    print(f"Daily Page Generator - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    generate_daily_page()
    
    print("\nDone!")


if __name__ == "__main__":
    main()
