[
  {
    "id": "2512.16411v1",
    "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection",
    "abstract": "Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.",
    "authors": [
      "Matthieu Garcin",
      "Louis Perot"
    ],
    "published": "2025-12-18",
    "categories": [
      "stat.ME",
      "math.ST",
      "q-fin.ST",
      "q-fin.TR",
      "stat.AP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16411v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16411v1",
    "fetched_at": "2025-12-19T08:34:17.858751"
  },
  {
    "id": "2512.16396v1",
    "title": "Global universal approximation with Brownian signatures",
    "abstract": "We establish $L^p$-type universal approximation theorems for general and non-anticipative functionals on suitable rough path spaces, showing that linear functionals acting on signatures of time-extended rough paths are dense with respect to an $L^p$-distance. To that end, we derive global universal approximation theorems for weighted rough path spaces. We demonstrate that these $L^p$-type universal approximation theorems apply in particular to Brownian motion. As a consequence, linear functionals on the signature of the time-extended Brownian motion can approximate any $p$-integrable stochastic process adapted to the Brownian filtration, including solutions to stochastic differential equations.",
    "authors": [
      "Mihriban Ceylan",
      "David J. Prömel"
    ],
    "published": "2025-12-18",
    "categories": [
      "math.PR",
      "cs.LG",
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16396v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16396v1",
    "fetched_at": "2025-12-19T08:34:17.858787"
  },
  {
    "id": "2512.16251v1",
    "title": "Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model",
    "abstract": "We introduce the \\textit{Consensus-Bottleneck Asset Pricing Model} (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this ``bottleneck'' to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and GRS-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.",
    "authors": [
      "Bong-Gyu Jang",
      "Younwoo Jeong",
      "Changeun Kim"
    ],
    "published": "2025-12-18",
    "categories": [
      "q-fin.PR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16251v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16251v1",
    "fetched_at": "2025-12-19T08:34:17.858813"
  },
  {
    "id": "2512.16115v1",
    "title": "An Efficient Machine Learning Framework for Option Pricing via Fourier Transform",
    "abstract": "The increasing need for rapid recalibration of option pricing models in dynamic markets places stringent computational demands on data generation and valuation algorithms. In this work, we propose a hybrid algorithmic framework that integrates the smooth offset algorithm (SOA) with supervised machine learning models for the fast pricing of multiple path-independent options under exponential Lévy dynamics. Building upon the SOA-generated dataset, we train neural networks, random forests, and gradient boosted decision trees to construct surrogate pricing operators. Extensive numerical experiments demonstrate that, once trained, these surrogates achieve order-of-magnitude acceleration over direct SOA evaluation. Importantly, the proposed framework overcomes key numerical limitations inherent to fast Fourier transform-based methods, including the consistency of input data and the instability in deep out-of-the-money option pricing.",
    "authors": [
      "Liying Zhang",
      "Ying Gao"
    ],
    "published": "2025-12-18",
    "categories": [
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16115v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16115v1",
    "fetched_at": "2025-12-19T08:34:17.858834"
  },
  {
    "id": "2512.16080v1",
    "title": "Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities",
    "abstract": "In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.",
    "authors": [
      "Tianyi Ma"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.CR",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16080v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16080v1",
    "fetched_at": "2025-12-19T08:34:17.858852"
  },
  {
    "id": "2512.16277v1",
    "title": "Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data",
    "abstract": "Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.",
    "authors": [
      "Jialiang Wang",
      "Xueyan Bao",
      "Hao Wu"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16277v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16277v1",
    "fetched_at": "2025-12-19T08:34:24.378979"
  },
  {
    "id": "2512.16453v1",
    "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
    "abstract": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
    "authors": [
      "Jiayang Yang",
      "Chunhui Zhao",
      "Martin Guay",
      "Zhixing Cao"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16453v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16453v1",
    "fetched_at": "2025-12-19T08:34:31.125207"
  },
  {
    "id": "2512.16037v1",
    "title": "Explainable AI in Big Data Fraud Detection",
    "abstract": "Big Data has become central to modern applications in finance, insurance, and cybersecurity, enabling machine learning systems to perform large-scale risk assessments and fraud detection. However, the increasing dependence on automated analytics introduces important concerns about transparency, regulatory compliance, and trust. This paper examines how explainable artificial intelligence (XAI) can be integrated into Big Data analytics pipelines for fraud detection and risk management. We review key Big Data characteristics and survey major analytical tools, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based approaches, and ensemble classifiers. We also present a structured review of widely used XAI methods, including LIME, SHAP, counterfactual explanations, and attention mechanisms, and analyze their strengths and limitations when deployed at scale. Based on these findings, we identify key research gaps related to scalability, real-time processing, and explainability for graph and temporal models. To address these challenges, we outline a conceptual framework that integrates scalable Big Data infrastructure with context-aware explanation mechanisms and human feedback. The paper concludes with open research directions in scalable XAI, privacy-aware explanations, and standardized evaluation methods for explainable fraud detection systems.",
    "authors": [
      "Ayush Jain",
      "Rahul Kulkarni",
      "Siyi Lin"
    ],
    "published": "2025-12-17",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16037v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16037v1",
    "fetched_at": "2025-12-19T08:34:31.125237"
  },
  {
    "id": "2512.16813v1",
    "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
    "abstract": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
    "authors": [
      "Bahman Abolhassani",
      "Tugba Erpek",
      "Kemal Davaslioglu",
      "Yalin E. Sagduyu",
      "Sastry Kompella"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16813v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16813v1",
    "fetched_at": "2025-12-19T08:35:00.596885"
  },
  {
    "id": "2512.16733v1",
    "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
    "abstract": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
    "authors": [
      "Daniel Bramblett",
      "Rushang Karia",
      "Adrian Ciotinga",
      "Ruthvick Suresh",
      "Pulkit Verma",
      "YooJung Choi",
      "Siddharth Srivastava"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16733v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16733v1",
    "fetched_at": "2025-12-19T08:35:00.596921"
  },
  {
    "id": "2512.16676v1",
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "abstract": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
    "authors": [
      "Hao Liang",
      "Xiaochen Ma",
      "Zhou Liu",
      "Zhen Hao Wong",
      "Zhengyang Zhao",
      "Zimo Meng",
      "Runming He",
      "Chengyu Shen",
      "Qifeng Cai",
      "Zhaoyang Han",
      "Meiyi Qiang",
      "Yalin Feng",
      "Tianyi Bai",
      "Zewei Pan",
      "Ziyi Guo",
      "Yizhen Jiang",
      "Jingwen Deng",
      "Qijie You",
      "Peichao Lai",
      "Tianyu Guo",
      "Chi Hsu Tsai",
      "Hengyi Feng",
      "Rui Hu",
      "Wenkai Yu",
      "Junbo Niu",
      "Bohan Zeng",
      "Ruichuan An",
      "Lu Ma",
      "Jihao Huang",
      "Yaowei Zheng",
      "Conghui He",
      "Linpeng Tang",
      "Bin Cui",
      "Weinan E",
      "Wentao Zhang"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16676v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16676v1",
    "fetched_at": "2025-12-19T08:35:00.597002"
  },
  {
    "id": "2512.16650v1",
    "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
    "abstract": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
    "authors": [
      "Jirui Yang",
      "Hengqi Guo",
      "Zhihui Lu",
      "Yi Zhao",
      "Yuansen Zhang",
      "Shijing Hu",
      "Qiang Duan",
      "Yinggui Wang",
      "Tao Wei"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16650v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16650v1",
    "fetched_at": "2025-12-19T08:35:00.597034"
  },
  {
    "id": "2512.16491v1",
    "title": "Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network",
    "abstract": "Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.",
    "authors": [
      "Theresa Eimer",
      "Lennart Schäpermeier",
      "André Biedenkapp",
      "Alexander Tornede",
      "Lars Kotthoff",
      "Pieter Leyman",
      "Matthias Feurer",
      "Katharina Eggensperger",
      "Kaitlin Maile",
      "Tanja Tornede",
      "Anna Kozak",
      "Ke Xue",
      "Marcel Wever",
      "Mitra Baratchi",
      "Damir Pulatov",
      "Heike Trautmann",
      "Haniye Kashgarani",
      "Marius Lindauer"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16491v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16491v1",
    "fetched_at": "2025-12-19T08:35:00.597081"
  },
  {
    "id": "2512.16301v1",
    "title": "Adaptation of Agentic AI",
    "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
    "authors": [
      "Pengcheng Jiang",
      "Jiacheng Lin",
      "Zhiyi Shi",
      "Zifeng Wang",
      "Luxi He",
      "Yichen Wu",
      "Ming Zhong",
      "Peiyang Song",
      "Qizheng Zhang",
      "Heng Wang",
      "Xueqiang Xu",
      "Hanwen Xu",
      "Pengrui Han",
      "Dylan Zhang",
      "Jiashuo Sun",
      "Chaoqi Yang",
      "Kun Qian",
      "Tian Wang",
      "Changran Hu",
      "Manling Li",
      "Quanzheng Li",
      "Hao Peng",
      "Sheng Wang",
      "Jingbo Shang",
      "Chao Zhang",
      "Jiaxuan You",
      "Liyuan Liu",
      "Pan Lu",
      "Yu Zhang",
      "Heng Ji",
      "Yejin Choi",
      "Dawn Song",
      "Jimeng Sun",
      "Jiawei Han"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16301v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16301v1",
    "fetched_at": "2025-12-19T08:35:00.597154"
  },
  {
    "id": "2512.16300v1",
    "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
    "abstract": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
    "authors": [
      "Fanrui Zhang",
      "Qiang Zhang",
      "Sizhuo Zhou",
      "Jianwen Sun",
      "Chuanhao Li",
      "Jiaxin Ai",
      "Yukang Feng",
      "Yujie Zhang",
      "Wenjie Li",
      "Zizhen Li",
      "Yifan Chang",
      "Jiawei Liu",
      "Kaipeng Zhang"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16300v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16300v1",
    "fetched_at": "2025-12-19T08:35:00.597191"
  },
  {
    "id": "2512.16295v1",
    "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
    "abstract": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
    "authors": [
      "Zhenyu Wu",
      "Jingjing Xie",
      "Zehao Li",
      "Bowen Yang",
      "Qiushi Sun",
      "Zhaoyang Liu",
      "Zhoumianze Liu",
      "Yu Qiao",
      "Xiangyu Yue",
      "Zun Wang",
      "Zichen Ding"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16295v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16295v1",
    "fetched_at": "2025-12-19T08:35:00.597225"
  },
  {
    "id": "2512.16280v1",
    "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
    "abstract": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.   We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
    "authors": [
      "Gilad Gressel",
      "Rahul Pankajakshan",
      "Shir Rozenfeld",
      "Ling Li",
      "Ivan Franceschini",
      "Krishnahsree Achuthan",
      "Yisroel Mirsky"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16280v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16280v1",
    "fetched_at": "2025-12-19T08:35:00.597255"
  },
  {
    "id": "2512.16262v1",
    "title": "Learning to Wait: Synchronizing Agents with the Physical World",
    "abstract": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
    "authors": [
      "Yifei She",
      "Ping Zhang",
      "He Liu",
      "Yanmin Jia",
      "Yang Jing",
      "Zijun Liu",
      "Peng Sun",
      "Xiangbin Li",
      "Xiaohe Hu"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16262v1",
    "fetched_at": "2025-12-19T08:35:00.597300"
  },
  {
    "id": "2512.16237v1",
    "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis",
    "abstract": "Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.",
    "authors": [
      "Zhi Helu",
      "Huang Jingjing",
      "Xu Wang",
      "Xu Yangbin",
      "Zhang Wanyue",
      "Jiang Baoyang",
      "Deng Shirui",
      "Zhu Liang",
      "Li Fangfang",
      "Zhao Tiejun",
      "Lin Yankai",
      "Yao Yuan"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16237v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16237v1",
    "fetched_at": "2025-12-19T08:35:00.597336"
  },
  {
    "id": "2512.16134v1",
    "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
    "abstract": "The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.",
    "authors": [
      "Jian Tian",
      "Shuailong Li",
      "Yang Cao",
      "Wenbo Cui",
      "Minghan Zhu",
      "Wenkang Wu",
      "Jianming Zhang",
      "Yanpeng Wang",
      "Zhiwen Xiao",
      "Zhenyu Hou",
      "Dou Shen"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.16134v1",
    "arxiv_url": "https://arxiv.org/abs/2512.16134v1",
    "fetched_at": "2025-12-19T08:35:00.597370"
  },
  {
    "id": "2512.15980v1",
    "title": "Embedding Software Intent: Lightweight Java Module Recovery",
    "abstract": "As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.",
    "authors": [
      "Yirui He",
      "Yuqi Huai",
      "Xingyu Chen",
      "Joshua Garcia"
    ],
    "published": "2025-12-17",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.15980v1",
    "arxiv_url": "https://arxiv.org/abs/2512.15980v1",
    "fetched_at": "2025-12-19T08:35:00.597392"
  },
  {
    "id": "2512.15946v1",
    "title": "AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines",
    "abstract": "Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts quantized models imported from high-level tools such as hls4ml or PyTorch while preserving bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments.",
    "authors": [
      "Dimitrios Danopoulos",
      "Enrico Lupi",
      "Chang Sun",
      "Sebastian Dittmeier",
      "Michael Kagan",
      "Vladimir Loncar",
      "Maurizio Pierini"
    ],
    "published": "2025-12-17",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.15946v1",
    "arxiv_url": "https://arxiv.org/abs/2512.15946v1",
    "fetched_at": "2025-12-19T08:35:00.597419"
  },
  {
    "id": "2512.15943v1",
    "title": "Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning",
    "abstract": "As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\\%), ToolLLaMA-DFS (30.18\\%), and ToolLLaMA-CoT (16.27\\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.",
    "authors": [
      "Polaris Jhandi",
      "Owais Kazi",
      "Shreyas Subramanian",
      "Neel Sendas"
    ],
    "published": "2025-12-17",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.15943v1",
    "arxiv_url": "https://arxiv.org/abs/2512.15943v1",
    "fetched_at": "2025-12-19T08:35:00.597441"
  }
]