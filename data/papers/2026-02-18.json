[
  {
    "id": "2602.15474v1",
    "title": "Quantum Reservoir Computing for Statistical Classification in a Superconducting Quantum Circuit",
    "abstract": "We analyze numerically the performance of Quantum Reservoir Computing (QRC) for statistical and financial problems. We use a reservoir composed of two superconducting islands coupled via their charge degrees of freedom. The key non-linear elements that provide the reservoir with rich and complex dynamics are the Josephson junctions that connect each island to the ground. We show that QRC implemented in this circuit can accurately classify complex probability distributions, including those with heavy tails, and identify regimes in correlated time series, such as periods of high volatility generated by standard econometric models. We find QRC to outperform some of the best classical methods when the amount of information is limited. This demonstrates its potential to be a noise-resilient quantum learning approach capable of tackling real-world problems within currently available superconducting platforms. We further discuss how to improve our QRC algorithm in real superconducting hardware to benefit from a much larger Hilbert space.",
    "authors": [
      "J. J. Prieto-Garcia",
      "A. G. del Pozo-Martín",
      "M. Pino"
    ],
    "published": "2026-02-17",
    "categories": [
      "quant-ph",
      "cond-mat.supr-con",
      "q-fin.ST"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15474v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15474v1",
    "fetched_at": "2026-02-18T08:52:25.773534"
  },
  {
    "id": "2602.15385v1",
    "title": "From Chain-Ladder to Individual Claims Reserving",
    "abstract": "The chain-ladder (CL) method is the most widely used claims reserving technique in non-life insurance. This manuscript introduces a novel approach to computing the CL reserves based on a fundamental restructuring of the data utilization for the CL prediction procedure. Instead of rolling forward the cumulative claims with estimated CL factors, we estimate multi-period factors that project the latest observations directly to the ultimate claims. This alternative perspective on CL reserving creates a natural pathway for the application of machine learning techniques to individual claims reserving. As a proof of concept, we present a small-scale real data application employing neural networks for individual claims reserving.",
    "authors": [
      "Ronald Richman",
      "Mario V. Wüthrich"
    ],
    "published": "2026-02-17",
    "categories": [
      "stat.AP",
      "q-fin.RM",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15385v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15385v1",
    "fetched_at": "2026-02-18T08:52:25.773570"
  },
  {
    "id": "2602.15248v1",
    "title": "Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models",
    "abstract": "Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.",
    "authors": [
      "Pavel Koptev",
      "Vishnu Kumar",
      "Konstantin Malkov",
      "George Shapiro",
      "Yury Vikhanov"
    ],
    "published": "2026-02-16",
    "categories": [
      "cs.AI",
      "math.OC",
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15248v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15248v1",
    "fetched_at": "2026-02-18T08:52:25.773598"
  },
  {
    "id": "2602.15182v1",
    "title": "Autodeleveraging as Online Learning",
    "abstract": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism used by perpetual futures venues when liquidation and insurance buffers are insufficient to restore solvency. Despite the scale of perpetual futures markets, ADL has received limited formal treatment as a sequential control problem. This paper provides a concise formalization of ADL as online learning on a PNL-haircut domain: at each round, the venue selects a solvency budget and a set of profitable trader accounts. The profitable accounts are liquidated to cover shortfalls up to the solvency budget, with the aim of recovering exchange-wide solvency. In this model, ADL haircuts apply to positive PNL (unrealized gains), not to posted collateral principal. Using our online learning model, we provide robustness results and theoretical upper bounds on how poorly a mechanism can perform at recovering solvency. We apply our model to the October 10, 2025 Hyperliquid stress episode. The regret caused by Hyperliquid's production ADL queue is about 50\\% of an upper bound on regret, calibrated to this event, while our optimized algorithm achieves about 2.6\\% of the same bound. In dollar terms, the production ADL model over liquidates trader profits by up to \\$51.7M. We also counterfactually evaluated algorithms inspired by our online learning framework that perform better and found that the best algorithm reduces overshoot to \\$3M. Our results provide simple, implementable mechanisms for improving ADL in live perpetuals exchanges.",
    "authors": [
      "Tarun Chitra",
      "Nagu Thogiti",
      "Mauricio Jean Pieer Trujillo Ramirez",
      "Victor Xu"
    ],
    "published": "2026-02-16",
    "categories": [
      "cs.GT",
      "q-fin.RM",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15182v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15182v1",
    "fetched_at": "2026-02-18T08:52:25.773624"
  },
  {
    "id": "2602.15177v1",
    "title": "Optimal investment under capital gains taxes",
    "abstract": "We generalize classical results on the existence of optimal portfolios in discrete time frictionless market models to models with capital gains taxes. We consider the realistic but mathematically challenging rule that losses do not trigger negative taxes but can only be offset against potential gains in the future. Central to the analysis is a well-known phenomenon from arbitrage-free markets with proportional transaction costs that does not exist in arbitrage-free frictionless markets: an investment in specific quantities of stocks that is completely riskless but may provide an advantage over holding money in the bank account. As a result of this phenomenon, on an infinite probability space, no-arbitrage does not imply that the set of attainable terminal wealth is closed in probability. We show closedness under the slightly stronger {\\em no unbounded non-substitutable investment with bounded risk} condition.   As a by-product, we provide a proof that in discrete time frictionless models with short-selling constraints, no-arbitrage implies that the set of attainable terminal wealth is closed in probability -- even if there are redundant stocks.",
    "authors": [
      "Alexander Dimitrov",
      "Christoph Kühn"
    ],
    "published": "2026-02-16",
    "categories": [
      "q-fin.MF",
      "math.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15177v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15177v1",
    "fetched_at": "2026-02-18T08:52:25.773646"
  },
  {
    "id": "2602.14754v2",
    "title": "Market Efficiency and the Heterogeneous Impact of Financial Liberalization: Evidence from the Shanghai-Hong Kong Stock Connect",
    "abstract": "This paper investigates the impact of the Shanghai-Hong Kong Stock Connect (SHHK Stock Connect) on the A-H share price premium and examines whether the policy effect is contingent on market efficiency. Using monthly data for 67 Shanghai-listed A-H dual-listed firms from January 2011 to May 2019, we employ a dynamic panel model estimated via two-step system generalized method of moment (GMM) to account for the persistence of the premium and potential endogeneity. Market efficiency is proxied by trading-friction measures derived from daily high-low price ranges. Our findings indicate that the implementation of SHHK Stock Connect is associated with an average 18.4% increase in the A-H premium. However, this effect is heterogeneous: the marginal impact of the policy is more pronounced for firms operating in less efficient markets and weaker for those with higher efficiency, suggesting that pre-existing trading frictions shape the policy outcome. No significant response is observed at the announcement stage. Placebo tests and alternative efficiency measures confirm the robustness of the efficiency-dependent effect. Overall, the results underscore the importance of the information environment in shaping the outcomes of financial liberalization.",
    "authors": [
      "Jiaqi Liu",
      "Chen Tang"
    ],
    "published": "2026-02-16",
    "categories": [
      "q-fin.GN"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.14754v2",
    "arxiv_url": "https://arxiv.org/abs/2602.14754v2",
    "fetched_at": "2026-02-18T08:52:25.773712"
  },
  {
    "id": "2602.15532v1",
    "title": "Quantifying construct validity in large language model evaluations",
    "abstract": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.   Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.   This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.",
    "authors": [
      "Ryan Othniel Kearns"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15532v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15532v1",
    "fetched_at": "2026-02-18T08:52:31.935783"
  },
  {
    "id": "2602.15470v1",
    "title": "The Skeletal Trap: Mapping Spatial Inequality and Ghost Stops in Ankara's Transit Network",
    "abstract": "Ankara's public transport crisis is commonly framed as a shortage of buses or operational inefficiency. This study argues that the problem is fundamentally morphological and structural. The city's leapfrog urban expansion has produced fragmented peripheral clusters disconnected from a rigid, center-oriented bus network. As a result, demand remains intensely concentrated along the Kizilay-Ulus axis and western corridors, while peripheral districts experience either chronic under-service or enforced transfer dependency. The deficiency is therefore not merely quantitative but rooted in the misalignment between urban macroform and network architecture. The empirical analysis draws on a 173-day operational dataset derived from route-level passenger and trip reports published by EGO under the former \"Transparent Ankara\" initiative. To overcome the absence of stop-level geospatial data, a Connectivity-Based Weighted Distribution Model reallocates passenger volumes to 1 km x 1 km grid cells using network centrality. The findings reveal persistent center-periphery asymmetries, structural bottlenecks, and spatially embedded accessibility inequalities.",
    "authors": [
      "Elifnaz Kancan"
    ],
    "published": "2026-02-17",
    "categories": [
      "physics.soc-ph",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15470v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15470v1",
    "fetched_at": "2026-02-18T08:52:38.096077"
  },
  {
    "id": "2602.15457v1",
    "title": "Benchmarking IoT Time-Series AD with Event-Level Augmentations",
    "abstract": "Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.",
    "authors": [
      "Dmitry Zhevnenko",
      "Ilya Makarov",
      "Aleksandr Kovalenko",
      "Fedor Meshchaninov",
      "Anton Kozhukhov",
      "Vladislav Travnikov",
      "Makar Ippolitov",
      "Kirill Yashunin",
      "Iurii Katser"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15457v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15457v1",
    "fetched_at": "2026-02-18T08:52:38.096121"
  },
  {
    "id": "2602.15325v1",
    "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents",
    "abstract": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual \"what-if\" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.",
    "authors": [
      "Zhixing Zhang",
      "Jesen Zhang",
      "Hao Liu",
      "Qinhan Lv",
      "Jing Yang",
      "Kaitong Cai",
      "Keze Wang"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15325v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15325v1",
    "fetched_at": "2026-02-18T08:52:38.096152"
  },
  {
    "id": "2602.15315v1",
    "title": "Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models",
    "abstract": "Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.",
    "authors": [
      "Tai Le-Gia",
      "Jaehyun Ahn"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15315v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15315v1",
    "fetched_at": "2026-02-18T08:52:38.096172"
  },
  {
    "id": "2602.15089v1",
    "title": "Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction",
    "abstract": "In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\\% or less and a detection rate of 88--94\\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.",
    "authors": [
      "Takato Yasuno"
    ],
    "published": "2026-02-16",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15089v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15089v1",
    "fetched_at": "2026-02-18T08:52:38.096215"
  },
  {
    "id": "2602.15828v1",
    "title": "Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation",
    "abstract": "Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.",
    "authors": [
      "Yuxuan Kuang",
      "Sungjae Park",
      "Katerina Fragkiadaki",
      "Shubham Tulsiani"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15828v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15828v1",
    "fetched_at": "2026-02-18T08:53:05.924689"
  },
  {
    "id": "2602.15827v1",
    "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching",
    "abstract": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.",
    "authors": [
      "Zhen Wu",
      "Xiaoyu Huang",
      "Lujie Yang",
      "Yuanhang Zhang",
      "Koushil Sreenath",
      "Xi Chen",
      "Pieter Abbeel",
      "Rocky Duan",
      "Angjoo Kanazawa",
      "Carmelo Sferrazza",
      "Guanya Shi",
      "C. Karen Liu"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15827v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15827v1",
    "fetched_at": "2026-02-18T08:53:05.924748"
  },
  {
    "id": "2602.15758v1",
    "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
    "abstract": "While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.",
    "authors": [
      "Manav Nitin Kapadnis",
      "Lawanya Baghel",
      "Atharva Naik",
      "Carolyn Rosé"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15758v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15758v1",
    "fetched_at": "2026-02-18T08:53:05.924773"
  },
  {
    "id": "2602.15721v1",
    "title": "Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems",
    "abstract": "We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.",
    "authors": [
      "Jingtian Yan",
      "Yulun Zhang",
      "Zhenting Liu",
      "Han Zhang",
      "He Jiang",
      "Jingkai Chen",
      "Stephen F. Smith",
      "Jiaoyang Li"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15721v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15721v1",
    "fetched_at": "2026-02-18T08:53:05.924803"
  },
  {
    "id": "2602.15559v1",
    "title": "Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities",
    "abstract": "Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails.",
    "authors": [
      "Gabriel Saco"
    ],
    "published": "2026-02-17",
    "categories": [
      "stat.ME",
      "econ.EM",
      "math.ST",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15559v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15559v1",
    "fetched_at": "2026-02-18T08:53:05.924821"
  },
  {
    "id": "2602.15549v1",
    "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing",
    "abstract": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.",
    "authors": [
      "Guoqin Tang",
      "Qingxuan Jia",
      "Gang Chen",
      "Tong Li",
      "Zeyuan Huang",
      "Zihang Lv",
      "Ning Ji"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15549v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15549v1",
    "fetched_at": "2026-02-18T08:53:05.924850"
  },
  {
    "id": "2602.15485v1",
    "title": "SecCodeBench-V2 Technical Report",
    "abstract": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
    "authors": [
      "Longfei Chen",
      "Ji Zhao",
      "Lanxiao Cui",
      "Tong Su",
      "Xingbo Pan",
      "Ziyang Li",
      "Yongxing Wu",
      "Qijiang Cao",
      "Qiyao Cai",
      "Jing Zhang",
      "Yuandong Ni",
      "Junyao He",
      "Zeyu Zhang",
      "Chao Ge",
      "Xuhuai Lu",
      "Zeyu Gao",
      "Yuxin Cui",
      "Weisen Chen",
      "Yuxuan Peng",
      "Shengping Wang",
      "Qi Li",
      "Yukai Huang",
      "Yukun Liu",
      "Tuo Zhou",
      "Terry Yue Zhuo",
      "Junyang Lin",
      "Chao Zhang"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15485v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15485v1",
    "fetched_at": "2026-02-18T08:53:05.924918"
  },
  {
    "id": "2602.15384v1",
    "title": "World-Model-Augmented Web Agents with Action Correction",
    "abstract": "Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.",
    "authors": [
      "Zhouzhou Shen",
      "Xueyu Hu",
      "Xiyun Li",
      "Tianqing Fang",
      "Juncheng Li",
      "Shengyu Zhang"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15384v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15384v1",
    "fetched_at": "2026-02-18T08:53:05.924947"
  },
  {
    "id": "2602.15379v1",
    "title": "FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations",
    "abstract": "The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs.",
    "authors": [
      "Zhihao Shu",
      "Md Musfiqur Rahman Sanim",
      "Hangyu Zheng",
      "Kunxiong Zhu",
      "Miao Yin",
      "Gagan Agrawal",
      "Wei Niu"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15379v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15379v1",
    "fetched_at": "2026-02-18T08:53:05.924976"
  },
  {
    "id": "2602.15337v1",
    "title": "FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning",
    "abstract": "Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\\% improvement over baseline methods and 1.93\\% over the current state-of-the-art method.",
    "authors": [
      "Chaoyi Lu"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15337v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15337v1",
    "fetched_at": "2026-02-18T08:53:05.924993"
  },
  {
    "id": "2602.15286v1",
    "title": "AI-Paging: Lease-Based Execution Anchoring for Network-Exposed AI-as-a-Service",
    "abstract": "With AI-as-a-Service (AIaaS) now deployed across multiple providers and model tiers, selecting the appropriate model instance at run time is increasingly outside the end user's knowledge and operational control. Accordingly, the 6G service providers are envisioned to play a crucial role in exposing AIaaS in a setting where users submit only an intent while the network helps in the intent-to-model matching (resolution) and execution placement under policy, trust, and Quality of Service (QoS) constraints. The network role becomes to discover candidate execution endpoints and selects a suitable model/anchor under policy and QoS constraints in a process referred here to as AI-paging (by analogy to cellular call paging). In the proposed architecture, AI-paging is a control-plane transaction that resolves an intent into an AI service identity (AISI), a scoped session token (AIST), and an expiring admission lease (COMMIT) that authorizes user-plane steering to a selected AI execution anchor (AEXF) under a QoS binding. AI-Paging enforces two invariants: (i) lease-gated steering (without COMMIT, no steering state is installed) and (ii) make-before-break anchoring to support continuity and reliability of AIaaS services under dynamic network conditions. We prototype AI-Paging using existing control- and user-plane mechanisms (service-based control, QoS flows, and policy-based steering) with no new packet headers, ensuring compatibility with existing 3GPP-based exposure and management architectures, and evaluate transaction latency, relocation interruption, enforcement correctness under lease expiry, and audit-evidence overhead under mobility and failures.",
    "authors": [
      "Merve Saimler",
      "Mohaned Chraiti"
    ],
    "published": "2026-02-17",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15286v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15286v1",
    "fetched_at": "2026-02-18T08:53:05.925040"
  },
  {
    "id": "2602.15197v1",
    "title": "OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction",
    "abstract": "Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general \"search\" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.",
    "authors": [
      "Skyler Hallinan",
      "Thejas Venkatesh",
      "Xiang Ren",
      "Sai Praneeth Karimireddy",
      "Ashwin Paranjape",
      "Yuhao Zhang",
      "Jack Hessel"
    ],
    "published": "2026-02-16",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15197v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15197v1",
    "fetched_at": "2026-02-18T08:53:05.925066"
  },
  {
    "id": "2602.15146v1",
    "title": "Beyond Reinforcement Learning: Fast and Scalable Quantum Circuit Synthesis",
    "abstract": "Quantum unitary synthesis addresses the problem of translating abstract quantum algorithms into sequences of hardware-executable quantum gates. Solving this task exactly is infeasible in general due to the exponential growth of the underlying combinatorial search space. Existing approaches suffer from misaligned optimization objectives, substantial training costs and limited generalization across different qubit counts. We mitigate these limitations by using supervised learning to approximate the minimum description length of residual unitaries and combining this estimate with stochastic beam search to identify near optimal gate sequences. Our method relies on a lightweight model with zero-shot generalization, substantially reducing training overhead compared to prior baselines. Across multiple benchmarks, we achieve faster wall-clock synthesis times while exceeding state-of-the-art methods in terms of success rate for complex circuits.",
    "authors": [
      "Lukas Theissinger",
      "Thore Gerlach",
      "David Berghaus",
      "Christian Bauckhage"
    ],
    "published": "2026-02-16",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15146v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15146v1",
    "fetched_at": "2026-02-18T08:53:05.925089"
  },
  {
    "id": "2602.15112v1",
    "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
    "abstract": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
    "authors": [
      "Aniketh Garikaparthi",
      "Manasi Patwardhan",
      "Arman Cohan"
    ],
    "published": "2026-02-16",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2602.15112v1",
    "arxiv_url": "https://arxiv.org/abs/2602.15112v1",
    "fetched_at": "2026-02-18T08:53:05.925109"
  }
]