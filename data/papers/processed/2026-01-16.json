[
  {
    "id": "2601.10591v1",
    "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
    "abstract": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",
    "authors": [
      "Arundeep Chinta",
      "Lucas Vinh Tran",
      "Jay Katukuri"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.RM",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10591v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10591v1",
    "fetched_at": "2026-01-16T08:35:37.817931",
    "chinese_title": "ProbFM：带不确定性分解的概率时间序列基础模型",
    "chinese_summary": "现有时间序列基础模型在金融预测中存在不确定性量化缺陷（假设受限、混淆来源、缺乏校准）；本文提出基于Transformer的ProbFM框架，结合深度证据回归（DER）实现认知-偶然不确定性的显式分解，无需预指定分布或采样且保持单遍计算效率；同时通过一致LSTM架构对比五种概率方法验证DER的不确定性量化能力。",
    "tags": [
      "Time Series",
      "Transformer",
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "首次提出基于Transformer的概率时间序列基础模型ProbFM，结合深度证据回归实现认知与偶然不确定性的显式分解，无需预指定分布或采样且保持高效计算",
      "开展基于一致LSTM架构的五种概率方法（DER、高斯NLL等）的受控对比研究，独立验证DER的不确定性量化能力"
    ],
    "processed_at": "2026-01-16T08:38:48.346469"
  },
  {
    "id": "2601.10517v1",
    "title": "From rough to multifractal multidimensional volatility: A multidimensional Log S-fBM model",
    "abstract": "We introduce the multivariate Log S-fBM model (mLog S-fBM), extending the univariate framework proposed by Wu \\textit{et al.} to the multidimensional setting. We define the multidimensional Stationary fractional Brownian motion (mS-fBM), characterized by marginals following S-fBM dynamics and a specific cross-covariance structure. It is parametrized by a correlation scale $T$, marginal-specific intermittency parameters and Hurst exponents, as well as their multidimensional counterparts: the co-intermittency matrix and the co-Hurst matrix. The mLog S-fBM is constructed by modeling volatility components as exponentials of the mS-fBM, preserving the dependence structure of the Gaussian core. We demonstrate that the model is well-defined for any co-Hurst matrix with entries in $[0, \\frac{1}{2}[$, supporting vanishing co-Hurst parameters to bridge rough volatility and multifractal regimes. We generalize the small intermittency approximation technique to the multivariate setting to develop an efficient Generalized Method of Moments calibration procedure, estimating cross-covariance parameters for pairs of marginals. We validate it on synthetic data and apply it to S\\&P 500 market data, modeling stock return fluctuations. Diagonal estimates of the stock Hurst matrix, corresponding to single-stock log-volatility Hurst exponents, are close to 0, indicating multifractal behavior, while co-Hurst off-diagonal entries are close to the Hurst exponent of the S\\&P 500 index ($H \\approx 0.12$), and co-intermittency off-diagonal entries align with univariate intermittency estimates.",
    "authors": [
      "Othmane Zarhali",
      "Emmanuel Bacry",
      "Jean-François Muzy"
    ],
    "published": "2026-01-15",
    "categories": [
      "q-fin.ST"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10517v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10517v1",
    "fetched_at": "2026-01-16T08:35:37.817970",
    "chinese_title": "从粗糙到多重分形多维波动率：一种多维Log S-fBM模型",
    "chinese_summary": "论文提出多维Log S-fBM模型（mLog S-fBM），扩展单变量框架至多维，定义含特定协方差结构的多维平稳分数布朗运动（mS-fBM），参数含相关尺度、边际及多维间歇性/Hurst指数相关矩阵；推广小间歇性近似至多维，发展高效GMM校准方法，用合成数据及标普500数据验证，发现单股票对数波动率Hurst接近0（多重分形），协Hurst非对角元接近标普500指数H≈0.12。",
    "tags": [
      "Volatility",
      "Time Series",
      "High Frequency",
      "Asset Pricing"
    ],
    "key_contributions": [
      "提出多维Log S-fBM模型，定义mS-fBM并明确其参数化结构；",
      "推广小间歇性近似至多维，构建GMM校准方法，应用于标普500数据验证并揭示市场波动率特征。"
    ],
    "processed_at": "2026-01-16T08:39:22.089566"
  },
  {
    "id": "2601.10375v1",
    "title": "Dynamic reinsurance via martingale transport",
    "abstract": "We formulate a dynamic reinsurance problem in which the insurer seeks to control the terminal distribution of its surplus while minimizing the L2-norm of the ceded risk. Using techniques from martingale optimal transport, we show that, under suitable assumptions, the problem admits a tractable solution analogous to the Bass martingale. We first consider the case where the insurer wants to match a given terminal distribution of the surplus process, and then relax this condition by only requiring certain moment or risk-based constraints.",
    "authors": [
      "Beatrice Acciaio",
      "Brandon Garcia Flores",
      "Antonio Marini",
      "Gudmund Pammer"
    ],
    "published": "2026-01-15",
    "categories": [
      "q-fin.RM",
      "math.OC",
      "math.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10375v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10375v1",
    "fetched_at": "2026-01-16T08:35:37.817997",
    "chinese_title": "基于鞅最优传输的动态再保险",
    "chinese_summary": "本文将动态再保险问题建模为控制盈余终端分布并最小化分出风险的L2范数，利用鞅最优传输技术在合适假设下得到类似Bass鞅的易处理解；进一步将约束从匹配给定终端分布放松至仅需满足矩或风险约束，提升了问题实用性。",
    "tags": [
      "Risk Management",
      "Portfolio Optimization"
    ],
    "key_contributions": [
      "结合鞅最优传输技术求解动态再保险问题，得到易处理的类Bass鞅解",
      "将终端分布约束放松为矩或风险约束，扩展问题适用场景"
    ],
    "processed_at": "2026-01-16T08:40:00.459577"
  },
  {
    "id": "2601.10143v1",
    "title": "History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis",
    "abstract": "In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra \"History Is Not Enough\" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.",
    "authors": [
      "Haochong Xia",
      "Yao Long Teng",
      "Regan Tan",
      "Molei Qin",
      "Xinrun Wang",
      "Bo An"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10143v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10143v1",
    "fetched_at": "2026-01-16T08:35:37.818025",
    "chinese_title": "历史不足：面向金融时间序列合成的自适应数据流系统",
    "chinese_summary": "针对量化金融中概念漂移与分布非平稳性导致的模型泛化差问题，论文提出漂移感知的自适应数据流系统，整合参数化数据操作模块与基于梯度双层优化的自适应规划调度器，统一数据增强、课程学习等流程，在预测和强化学习交易任务中提升模型鲁棒性与风险调整收益。",
    "tags": [
      "Time Series",
      "Reinforcement Learning",
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "提出漂移感知的自适应数据流系统，整合参数化数据操作与自适应规划调度器，统一数据增强、课程学习等流程，应对金融市场概念漂移与分布非平稳性",
      "实验验证该系统在金融预测与强化学习交易任务中提升模型鲁棒性及风险调整收益，提供通用自适应数据管理方法"
    ],
    "processed_at": "2026-01-16T08:40:28.848193"
  },
  {
    "id": "2601.10043v1",
    "title": "Instruction Finetuning LLaMA-3-8B Model Using LoRA for Financial Named Entity Recognition",
    "abstract": "Particularly, financial named-entity recognition (NER) is one of the many important approaches to translate unformatted reports and news into structured knowledge graphs. However, free, easy-to-use large language models (LLMs) often fail to differentiate organisations as people, or disregard an actual monetary amount entirely. This paper takes Meta's Llama 3 8B and applies it to financial NER by combining instruction fine-tuning and Low-Rank Adaptation (LoRA). Each annotated sentence is converted into an instruction-input-output triple, enabling the model to learn task descriptions while fine-tuning with small low-rank matrices instead of updating all weights. Using a corpus of 1,693 sentences, our method obtains a micro-F1 score of 0.894 compared with Qwen3-8B, Baichuan2-7B, T5, and BERT-Base. We present dataset statistics, describe training hyperparameters, and perform visualizations of entity density, learning curves, and evaluation metrics. Our results show that instruction tuning combined with parameter-efficient fine-tuning enables state-of-the-art performance on domain-sensitive NER.",
    "authors": [
      "Zhiming Lian"
    ],
    "published": "2026-01-15",
    "categories": [
      "q-fin.CP",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10043v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10043v1",
    "fetched_at": "2026-01-16T08:35:37.818067",
    "chinese_title": "",
    "chinese_summary": "",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [],
    "processed_at": "2026-01-16T08:40:43.917465"
  },
  {
    "id": "2601.09927v1",
    "title": "Efficiency versus Robustness under Tail Misspecification: Importance Sampling and Moment-Based VaR Bracketing",
    "abstract": "Value-at-Risk (VaR) estimation at high confidence levels is inherently a rare-event problem and is particularly sensitive to tail behavior and model misspecification. This paper studies the performance of two simulation-based VaR estimation approaches, importance sampling and discrete moment matching, under controlled tail misspecification. The analysis separates the nominal model used for estimator construction from the true data-generating process used for evaluation, allowing the effects of heavy-tailed returns to be examined in a transparent and reproducible setting. Daily returns of a broad equity market proxy are used to calibrate a nominal Gaussian model, while true returns are generated from Student-t distributions with varying degrees of freedom to represent increasingly heavy tails. Importance sampling is implemented via exponential tilting of the Gaussian model, and VaR is estimated through likelihood-weighted root-finding. Discrete moment matching constructs deterministic lower and upper VaR bounds by enforcing a finite number of moment constraints on a discretized loss distribution. The results demonstrate a clear trade-off between efficiency and robustness. Importance sampling produces low-variance VaR estimates under the nominal model but systematically underestimates the true VaR under heavy-tailed returns, with bias increasing at higher confidence levels and for thicker tails. In contrast, discrete moment matching yields conservative VaR bracketing that remains robust under tail misspecification. These findings highlight that variance reduction alone is insufficient for reliable tail risk estimation when model uncertainty is significant.",
    "authors": [
      " Aditri"
    ],
    "published": "2026-01-14",
    "categories": [
      "q-fin.RM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.09927v1",
    "arxiv_url": "https://arxiv.org/abs/2601.09927v1",
    "fetched_at": "2026-01-16T08:35:37.818086",
    "chinese_title": "尾部误设下的效率与稳健性：重要性采样与基于矩的VaR区间估计",
    "chinese_summary": "本文研究高置信水平VaR估计在尾部误设下的效率与稳健性权衡，对比重要性采样（指数倾斜高斯名义模型）和离散矩匹配（有限矩约束构造VaR上下界）两种方法；以日收益率校准高斯模型为名义模型，不同自由度t分布为真实过程，发现重要性采样在名义模型下方差低但尾部误设下低估VaR，离散矩匹配则提供保守稳健的VaR区间。",
    "tags": [
      "Risk Management",
      "Time Series"
    ],
    "key_contributions": [
      "分离名义模型与真实数据生成过程，透明验证尾部误设下VaR估计的效率-稳健性权衡关系",
      "提出离散矩匹配方法构造保守VaR上下界，在重尾分布下仍稳健，弥补重要性采样的误设偏差"
    ],
    "processed_at": "2026-01-16T08:41:16.173982"
  },
  {
    "id": "2601.09872v1",
    "title": "A continuous-time Kyle model with price-responsive traders",
    "abstract": "Classical Kyle-type models of informed trading typically treat noise trader demand as purely exogenous. In reality, many market participants react to price movements and news, generating feedback effects that can significantly alter market dynamics. This paper develops a continuous-time Kyle framework in which two types of price-responsive traders (momentum and contrarian traders) adjust their demand in response to price signals. This extension yields a finite-dimensional Kalman filter for price discovery and leads to a forward-backward Riccati system characterizing equilibrium. We show that when feedback is weak, equilibrium exists and is unique as a smooth perturbation of the classical Kyle solution, allowing us to derive explicit comparative statics for insider profits and price informativeness. For stronger feedback, the model generates rich dynamics, including potential multiplicity of equilibria and amplification effects. Our framework thus bridges the gap between purely exogenous noise and more realistic, behaviorally motivated trading.",
    "authors": [
      "Eunjung Noh"
    ],
    "published": "2026-01-14",
    "categories": [
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.09872v1",
    "arxiv_url": "https://arxiv.org/abs/2601.09872v1",
    "fetched_at": "2026-01-16T08:35:37.818104",
    "chinese_title": "含价格响应型交易者的连续时间Kyle模型",
    "chinese_summary": "本文突破经典Kyle模型外生噪声假设，引入动量与反向两类价格响应交易者构建连续时间框架；通过有限维Kalman滤波实现价格发现，以前向后向Riccati系统刻画均衡，弱反馈下均衡唯一且为经典解的光滑扰动，强反馈下呈现多重均衡等丰富动态，填补外生噪声与行为驱动交易的空白。",
    "tags": [
      "Market Microstructure",
      "Behavioral Finance",
      "Asset Pricing"
    ],
    "key_contributions": [
      "构建含价格响应交易者的连续时间Kyle模型，引入反馈效应扩展经典框架",
      "推导有限维Kalman滤波与Riccati系统刻画均衡，揭示弱/强反馈下的不同动态特征"
    ],
    "processed_at": "2026-01-16T08:41:43.217887"
  },
  {
    "id": "2601.09949v1",
    "title": "Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series",
    "abstract": "Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.",
    "authors": [
      "Griffin Kearney"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.09949v1",
    "arxiv_url": "https://arxiv.org/abs/2601.09949v1",
    "fetched_at": "2026-01-16T08:35:40.927573",
    "chinese_title": "运动学标记化：面向噪声时间序列中可学习决策策略的基于优化的连续时间标记",
    "chinese_summary": "Transformer针对离散标记设计，但现实信号多为带噪声采样的连续过程，离散标记化在低信噪比下脆弱。论文提出运动学标记化，通过优化从噪声测量重建显式样条并标记局部系数（位置、速度等）；应用于金融时间序列时，该方法在风险规避非对称目标下比离散基线更能维持稳定策略和校准动作分布，提升噪声时间序列决策策略的可学习性。",
    "tags": [
      "Transformer",
      "Time Series",
      "Asset Pricing",
      "Algorithmic Trading"
    ],
    "key_contributions": [
      "验证该方法在噪声金融时间序列的风险规避非对称目标下，能有效避免基线策略的崩溃，维持稳定且校准的决策策略"
    ],
    "processed_at": "2026-01-16T08:41:59.642346"
  },
  {
    "id": "2601.10148v1",
    "title": "DecisionLLM: Large Language Models for Long Sequence Decision Exploration",
    "abstract": "Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.",
    "authors": [
      "Xiaowei Lv",
      "Zhilin Zhang",
      "Yijun Li",
      "Yusen Huo",
      "Siyuan Ju",
      "Xuyan Li",
      "Chunxiang Hong",
      "Tianyu Wang",
      "Yongcai Wang",
      "Peng Sun",
      "Chuan Yu",
      "Jian Xu",
      "Bo Zheng"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10148v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10148v1",
    "fetched_at": "2026-01-16T08:35:44.039627",
    "chinese_title": "DecisionLLM：用于长序列决策探索的大语言模型",
    "chinese_summary": "本文针对长序列决策问题，提出DecisionLLM模型，将决策轨迹视为独特模态并对齐自然语言任务描述，解决大语言模型（LLM）对连续值的解释难题；建立了模型规模、数据量与质量影响性能的缩放定律，在离线实验和实时竞价场景中验证了其有效性。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Transformer",
      "Algorithmic Trading"
    ],
    "key_contributions": [
      "提出DecisionLLM框架，通过轨迹-自然语言对齐解决LLM解释连续值的核心挑战",
      "揭示长序列决策性能依赖模型规模、数据量与质量的缩放规律，在离线基准和竞价场景验证有效"
    ],
    "processed_at": "2026-01-16T08:42:16.955244"
  },
  {
    "id": "2601.10494v1",
    "title": "CROCS: A Two-Stage Clustering Framework for Behaviour-Centric Consumer Segmentation with Smart Meter Data",
    "abstract": "With grid operators confronting rising uncertainty from renewable integration and a broader push toward electrification, Demand-Side Management (DSM) -- particularly Demand Response (DR) -- has attracted significant attention as a cost-effective mechanism for balancing modern electricity systems. Unprecedented volumes of consumption data from a continuing global deployment of smart meters enable consumer segmentation based on real usage behaviours, promising to inform the design of more effective DSM and DR programs. However, existing clustering-based segmentation methods insufficiently reflect the behavioural diversity of consumers, often relying on rigid temporal alignment, and faltering in the presence of anomalies, missing data, or large-scale deployments.   To address these challenges, we propose a novel two-stage clustering framework -- Clustered Representations Optimising Consumer Segmentation (CROCS). In the first stage, each consumer's daily load profiles are clustered independently to form a Representative Load Set (RLS), providing a compact summary of their typical diurnal consumption behaviours. In the second stage, consumers are clustered using the Weighted Sum of Minimum Distances (WSMD), a novel set-to-set measure that compares RLSs by accounting for both the prevalence and similarity of those behaviours. Finally, community detection on the WSMD-induced graph reveals higher-order prototypes that embody the shared diurnal behaviours defining consumer groups, enhancing the interpretability of the resulting clusters.   Extensive experiments on both synthetic and real Australian smart meter datasets demonstrate that CROCS captures intra-consumer variability, uncovers both synchronous and asynchronous behavioural similarities, and remains robust to anomalies and missing data, while scaling efficiently through natural parallelisation. These results...",
    "authors": [
      "Luke W. Yerbury",
      "Ricardo J. G. B. Campello",
      "G. C. Livingston",
      "Mark Goldsworthy",
      "Lachlan O'Neil"
    ],
    "published": "2026-01-15",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10494v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10494v1",
    "fetched_at": "2026-01-16T08:35:50.238281",
    "chinese_title": "CROCS：基于智能电表数据的行为中心消费者细分两阶段聚类框架",
    "chinese_summary": "针对现有消费者细分方法在智能电表数据处理中存在的刚性时间对齐、异常/缺失数据及大规模部署适应性差等问题，本文提出两阶段聚类框架CROCS：第一阶段独立聚类每个消费者的日负荷曲线形成代表负荷集（RLS），第二阶段采用加权最小距离和（WSMD）集合间度量聚类消费者，并通过WSMD图的社区检测揭示共享行为的高阶原型，以支持更有效的需求侧管理（DSM）和需求响应（DR）程序设计。",
    "tags": [
      "Time Series",
      "Anomaly",
      "Behavioral Finance",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "提出两阶段聚类框架CROCS，通过独立生成消费者代表负荷集（RLS）并采用WSMD集合间度量，解决现有方法刚性时间对齐、异常/缺失数据及大规模部署适应性不足的问题",
      "利用WSMD诱导图的社区检测，揭示体现消费者群体共享日行为的高阶原型，提升行为中心消费者细分的有效性"
    ],
    "processed_at": "2026-01-16T08:42:50.998719"
  },
  {
    "id": "2601.10193v1",
    "title": "GFM4GA: Graph Foundation Model for Group Anomaly Detection",
    "abstract": "Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.",
    "authors": [
      "Jiujiu Chen",
      "Weijun Zeng",
      "Shaofeng Hu",
      "Sihong Xie",
      "Hui Xiong"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10193v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10193v1",
    "fetched_at": "2026-01-16T08:35:50.238317",
    "chinese_title": "GFM4GA：面向群体异常检测的图基础模型",
    "chinese_summary": "群体异常检测因模式多样面临挑战，现有图基础模型（GFM）仅能处理个体异常无法泛化到群体（群体需整体检测且个体可能正常）。论文提出GFM4GA，通过特征估计与群体提取的双层次对比学习预训练捕捉群体异常结构，下游在参数约束的少样本设置下微调，利用标注异常邻居的群体上下文扩展自适应能力，实验显示其AUROC和AUPRC平均提升2.85%和2.55%。",
    "tags": [
      "Anomaly",
      "Graph Neural Network",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出面向群体异常检测的图基础模型GFM4GA，解决现有GFM无法处理群体异常的问题",
      "设计双层次对比学习预训练与少样本微调策略，显著提升群体异常检测的性能指标"
    ],
    "processed_at": "2026-01-16T08:43:07.289828"
  },
  {
    "id": "2601.10130v1",
    "title": "Redundancy-Driven Top-$k$ Functional Dependency Discovery",
    "abstract": "Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.",
    "authors": [
      "Xiaolong Wan",
      "Xixian Han"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10130v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10130v1",
    "fetched_at": "2026-01-16T08:35:50.238338",
    "chinese_title": "冗余驱动的Top-k函数依赖发现",
    "chinese_summary": "针对传统函数依赖（FD）发现算法计算代价高、结果集庞大的问题，提出SDP方法按冗余计数排名发现Top-k FD（冗余计数关联存储开销与更新异常），通过单调性上界剪枝搜索空间，结合属性排序、分区基数矩阵、全局调度器等优化提升效率，实验验证其高效性。",
    "tags": [
      "Anomaly",
      "Factor Mining"
    ],
    "key_contributions": [
      "提出SDP方法，按冗余计数排名发现Top-k函数依赖，解决传统算法计算代价高、结果集庞大的问题",
      "证明冗余上界的单调性并设计剪枝策略，结合多项优化提升效率，实验验证其优于 exhaustive方法"
    ],
    "processed_at": "2026-01-16T08:43:33.632215"
  },
  {
    "id": "2601.09902v1",
    "title": "A Novel Contrastive Loss for Zero-Day Network Intrusion Detection",
    "abstract": "Machine learning has achieved state-of-the-art results in network intrusion detection; however, its performance significantly degrades when confronted by a new attack class -- a zero-day attack. In simple terms, classical machine learning-based approaches are adept at identifying attack classes on which they have been previously trained, but struggle with those not included in their training data. One approach to addressing this shortcoming is to utilise anomaly detectors which train exclusively on benign data with the goal of generalising to all attack classes -- both known and zero-day. However, this comes at the expense of a prohibitively high false positive rate. This work proposes a novel contrastive loss function which is able to maintain the advantages of other contrastive learning-based approaches (robustness to imbalanced data) but can also generalise to zero-day attacks. Unlike anomaly detectors, this model learns the distributions of benign traffic using both benign and known malign samples, i.e. other well-known attack classes (not including the zero-day class), and consequently, achieves significant performance improvements. The proposed approach is experimentally verified on the Lycos2017 dataset where it achieves an AUROC improvement of .000065 and .060883 over previous models in known and zero-day attack detection, respectively. Finally, the proposed method is extended to open-set recognition achieving OpenAUC improvements of .170883 over existing approaches.",
    "authors": [
      "Jack Wilkie",
      "Hanan Hindy",
      "Craig Michie",
      "Christos Tachtatzis",
      "James Irvine",
      "Robert Atkinson"
    ],
    "published": "2026-01-14",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.09902v1",
    "arxiv_url": "https://arxiv.org/abs/2601.09902v1",
    "fetched_at": "2026-01-16T08:35:50.238366",
    "chinese_title": "一种用于零日网络入侵检测的新型对比损失函数",
    "chinese_summary": "该文针对传统机器学习在零日网络入侵检测中泛化能力弱、异常检测器假阳性高的问题，提出一种新型对比损失函数，利用良性与已知恶意样本学习良性流量分布，既保持对比学习对不平衡数据的鲁棒性，又能有效泛化到零日攻击；实验在Lycos2017数据集上显著提升已知和零日攻击检测的AUROC，且扩展到开集识别也有性能提升。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出新型对比损失函数，兼顾对比学习对不平衡数据的鲁棒性与零日攻击泛化能力，通过良性和已知恶意样本学习良性分布"
    ],
    "processed_at": "2026-01-16T08:43:58.187128"
  },
  {
    "id": "2601.09768v1",
    "title": "CLiMB: A Domain-Informed Novelty Detection Clustering Framework for Scientific Discovery",
    "abstract": "In data-driven scientific discovery, a challenge lies in classifying well-characterized phenomena while identifying novel anomalies. Current semi-supervised clustering algorithms do not always fully address this duality, often assuming that supervisory signals are globally representative. Consequently, methods often enforce rigid constraints that suppress unanticipated patterns or require a pre-specified number of clusters, rendering them ineffective for genuine novelty detection. To bridge this gap, we introduce CLiMB (CLustering in Multiphase Boundaries), a domain-informed framework decoupling the exploitation of prior knowledge from the exploration of unknown structures. Using a sequential two-phase approach, CLiMB first anchors known clusters using constrained partitioning, and subsequently applies density-based clustering to residual data to reveal arbitrary topologies. We demonstrate this framework on RR Lyrae stars data from the Gaia Data Release 3. CLiMB attains an Adjusted Rand Index of 0.829 with 90% seed coverage in recovering known Milky Way substructures, drastically outperforming heuristic and constraint-based baselines, which stagnate below 0.20. Furthermore, sensitivity analysis confirms CLiMB's superior data efficiency, showing monotonic improvement as knowledge increases. Finally, the framework successfully isolates three dynamical features (Shiva, Shakti, and the Galactic Disk) in the unlabelled field, validating its potential for scientific discovery.",
    "authors": [
      "Lorenzo Monti",
      "Tatiana Muraveva",
      "Brian Sheridan",
      "Davide Massari",
      "Alessia Garofalo",
      "Gisella Clementini",
      "Umberto Michelucci"
    ],
    "published": "2026-01-14",
    "categories": [
      "astro-ph.IM",
      "astro-ph.GA",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.09768v1",
    "arxiv_url": "https://arxiv.org/abs/2601.09768v1",
    "fetched_at": "2026-01-16T08:35:50.238394",
    "chinese_title": "CLiMB：面向科学发现的领域知情型异常检测聚类框架",
    "chinese_summary": "该文提出CLiMB领域知情型异常检测聚类框架，针对半监督聚类难以同时分类已知现象与识别新异常的问题，采用“约束划分锚定已知簇+密度聚类探索未知结构”的两阶段方法；在Gaia数据发布3的RR Lyrae星数据上验证，其Adjusted Rand Index达0.829，远超基线模型（<0.20），还成功分离出3个新动力学特征，具备科学发现潜力。",
    "tags": [
      "Anomaly",
      "Benchmark"
    ],
    "key_contributions": [
      "提出CLiMB两阶段领域知情聚类框架，解耦已知知识利用与未知结构探索，解决半监督聚类无法兼顾已知现象分类和新异常识别的问题",
      "在Gaia DR3 RR Lyrae星数据上验证，效果显著优于基线，且成功发现3个新动力学特征，证明其科学发现价值"
    ],
    "processed_at": "2026-01-16T08:44:28.726855"
  },
  {
    "id": "2601.10560v1",
    "title": "Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems",
    "abstract": "Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS",
    "authors": [
      "Xi Shi",
      "Mengxin Zheng",
      "Qian Lou"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10560v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10560v1",
    "fetched_at": "2026-01-16T08:36:18.175740",
    "chinese_title": "面向并行多智能体系统的延迟感知编排学习",
    "chinese_summary": "多智能体系统（MAS）因多步执行和重复模型调用导致高推理延迟，现有方法多优化任务性能且假设串行执行，不适用于并行场景的延迟控制。本文提出延迟感知多智能体系统（LAMaS）框架，显式监督并行执行延迟并优化关键路径，构建低延迟执行拓扑；实验表明其相比SOTA方法减少关键路径长度38-46%，同时保持或提升任务性能。",
    "tags": [
      "Financial Agent",
      "Deep Learning",
      "Execution"
    ],
    "key_contributions": [
      "提出延迟感知多智能体系统（LAMaS）框架，显式优化并行执行下的关键路径，解决现有串行假设方法的延迟优化不足问题",
      "实验验证LAMaS在多基准下显著降低延迟（关键路径减38-46%）且保持任务性能，凸显并行执行显式延迟优化的重要性"
    ],
    "processed_at": "2026-01-16T08:44:53.681627"
  },
  {
    "id": "2601.10485v1",
    "title": "Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge",
    "abstract": "Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.",
    "authors": [
      "Runhao Zhao",
      "Weixin Zeng",
      "Wentao Zhang",
      "Chong Chen",
      "Zhengpin Li",
      "Xiang Zhao",
      "Lei Chen"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10485v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10485v1",
    "fetched_at": "2026-01-16T08:36:18.175780",
    "chinese_title": "淘金：利用通用知识扩展领域特定知识图谱",
    "chinese_summary": "该论文针对领域特定知识图谱（DKG）覆盖不足的问题，提出领域特定知识图谱融合（DKGF）任务，旨在整合通用知识图谱（GKG）中的相关事实以丰富DKG；提出ExeFuse方法，采用Fact-as-Program范式，通过目标DKG上的程序可执行性验证领域相关性，联合解决相关性模糊与知识粒度不对齐两大挑战；构建两个基准数据集并开展实验，验证方法有效性与任务重要性，提供首个DKGF标准化测试平台。",
    "tags": [
      "Graph Neural Network",
      "Benchmark",
      "NLP"
    ],
    "key_contributions": [
      "提出领域特定知识图谱融合（DKGF）任务，填补领域特定知识图谱覆盖不足的研究空白",
      "提出ExeFuse方法，通过Fact-as-Program范式联合解决领域相关性模糊与知识粒度不对齐问题，并构建两个基准数据集及首个DKGF标准化测试平台"
    ],
    "processed_at": "2026-01-16T08:45:21.965794"
  },
  {
    "id": "2601.10440v1",
    "title": "AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior",
    "abstract": "Artificial intelligence (AI) agents are increasingly used in a variety of domains to automate tasks, interact with users, and make decisions based on data inputs. Ensuring that AI agents perform only authorized actions and handle inputs appropriately is essential for maintaining system integrity and preventing misuse. In this study, we introduce the AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. During a controlled staging phase, the framework monitors execution traces to learn legitimate agent behaviors and input patterns. From this phase, it derives adaptive policies that regulate tool calls made by the agent, guided by both real-time input context and the control flow dependencies of multi-step agent actions. Evaluation across two real-world AI agent applications demonstrates that AgentGuardian effectively detects malicious or misleading inputs while preserving normal agent functionality. Moreover, its control-flow-based governance mechanism mitigates hallucination-driven errors and other orchestration-level malfunctions.",
    "authors": [
      "Nadya Abaev",
      "Denis Klimov",
      "Gerard Levinov",
      "David Mimran",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10440v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10440v1",
    "fetched_at": "2026-01-16T08:36:18.175808",
    "chinese_title": "AgentGuardian：学习访问控制策略以治理AI智能体行为",
    "chinese_summary": "本文引入AgentGuardian安全框架，通过受控阶段监控执行轨迹学习AI智能体的合法行为与输入模式，推导结合实时输入上下文和多步动作控制流依赖的自适应策略以监管工具调用；评估表明该框架可有效检测恶意/误导性输入并保留正常功能，同时缓解幻觉驱动错误等编排级问题。",
    "tags": [
      "LLM",
      "Anomaly",
      "Risk Management"
    ],
    "key_contributions": [
      "提出上下文感知的AgentGuardian安全框架，通过学习合法行为推导自适应访问控制策略监管AI智能体工具调用",
      "验证该框架可有效检测恶意输入、保留正常功能，且缓解幻觉驱动错误等编排级问题"
    ],
    "processed_at": "2026-01-16T08:45:41.421110"
  },
  {
    "id": "2601.10402v1",
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "abstract": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
    "authors": [
      "Xinyu Zhu",
      "Yuzhu Cai",
      "Zexi Liu",
      "Bingyang Zheng",
      "Cheng Wang",
      "Rui Ye",
      "Jiaao Chen",
      "Hanrui Wang",
      "Wei-Chen Wang",
      "Yuzhi Zhang",
      "Linfeng Zhang",
      "Weinan E",
      "Di Jin",
      "Siheng Chen"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10402v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10402v1",
    "fetched_at": "2026-01-16T08:36:18.175849",
    "chinese_title": "迈向超长期智能体科学：机器学习工程的认知积累",
    "chinese_summary": "本文针对AI超长期智能体科学的瓶颈（实验周期长的战略连贯性不足），提出ML-Master 2.0自主智能体，采用分层认知缓存（HCC）架构动态蒸馏执行轨迹为稳定知识，解耦即时执行与长期策略；在MLE-Bench评估中以24小时预算实现56.44%的SOTA奖牌率，为超长期自主探索提供可扩展蓝图。",
    "tags": [
      "LLM",
      "Financial Agent",
      "Benchmark"
    ],
    "key_contributions": [
      "提出基于分层认知缓存（HCC）的ML-Master 2.0自主智能体，解决超长期机器学习工程中LLM战略连贯性不足问题",
      "在MLE-Bench评估中以24小时预算实现56.44%的SOTA奖牌率，验证超长期自主探索的可扩展性"
    ],
    "processed_at": "2026-01-16T08:46:03.173553"
  },
  {
    "id": "2601.10398v1",
    "title": "LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries",
    "abstract": "In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.",
    "authors": [
      "Xuancheng Ren",
      "Shijing Hu",
      "Zhihui Lu",
      "Jiangqi Huang",
      "Qiang Duan"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10398v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10398v1",
    "fetched_at": "2026-01-16T08:36:18.175877",
    "chinese_title": "LatentRefusal：面向不可回答文本到SQL查询的潜在信号拒绝机制",
    "chinese_summary": "针对基于大语言模型（LLM）的文本到SQL系统中不可回答/欠指定查询易产生错误的问题，论文提出LatentRefusal潜在信号拒绝机制，通过Tri-Residual Gated Encoder从LLM中间隐藏激活预测查询可回答性，实验验证其高效安全且可附加。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出LatentRefusal潜在信号拒绝机制，从LLM中间隐藏激活预测查询可回答性，解决现有方法脆弱或复杂的问题",
      "引入Tri-Residual Gated Encoder，抑制模式噪声并放大不匹配线索，且实验证明该方法高效（仅增加约2ms开销）且可附加于文本到SQL系统"
    ],
    "processed_at": "2026-01-16T08:46:27.612249"
  },
  {
    "id": "2601.10338v1",
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "abstract": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.",
    "authors": [
      "Yi Liu",
      "Weizhe Wang",
      "Ruitao Feng",
      "Yao Zhang",
      "Guangquan Xu",
      "Gelei Deng",
      "Yuekang Li",
      "Leo Zhang"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10338v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10338v1",
    "fetched_at": "2026-01-16T08:36:18.175910",
    "chinese_title": "野生环境中的Agent技能：大规模安全漏洞的实证研究",
    "chinese_summary": "该研究首次大规模实证分析AI Agent技能生态的安全风险，收集两大市场42447个技能并通过整合静态分析与LLM的SkillScan框架分析31132个，发现26.1%的技能存在至少一种漏洞（分四类14种模式）；提出基于漏洞技能的分类法，开发高精度检测方法并发布开放数据集与工具。",
    "tags": [
      "LLM",
      "Financial Agent",
      "Risk Management"
    ],
    "key_contributions": [
      "首次大规模实证揭示AI Agent技能生态的安全漏洞分布，提出四类14种漏洞模式的分类法",
      "开发整合静态分析与LLM的SkillScan检测框架，验证其精度86.7%、召回率82.5%，并发布开放数据集与工具"
    ],
    "processed_at": "2026-01-16T08:46:41.332398"
  },
  {
    "id": "2601.10318v1",
    "title": "Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis",
    "abstract": "In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.",
    "authors": [
      "Songsong Tian",
      "Kongsheng Zhuo",
      "Zhendong Wang",
      "Rong Shen",
      "Shengtao Zhang",
      "Yong Wu"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10318v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10318v1",
    "fetched_at": "2026-01-16T08:36:18.175936",
    "chinese_title": "边界感知NL2SQL：通过混合奖励与数据合成实现可靠性集成",
    "chinese_summary": "本文提出边界感知可靠NL2SQL框架BAR-SQL，通过Seed Mutation数据合成构建含多步分析查询与边界案例的企业语料，结合知识接地推理合成生成带推理轨迹的SQL；采用SFT+GRPO两阶段训练，设计任务条件混合奖励优化SQL执行精度与弃权语义精度，并构建Ent-SQL-Bench基准，实验显示其在SQL生成质量和边界感知弃权能力上优于领先专有模型。",
    "tags": [
      "LLM",
      "NLP",
      "Reinforcement Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "提出BAR-SQL统一训练框架，集成边界感知与可靠性，通过混合奖励机制和数据合成方法优化SQL生成精度与边界感知弃权能力",
      "构建Ent-SQL-Bench基准，首次联合评估SQL生成精度与边界感知弃权能力，实验验证其性能优于Claude 4.5 Sonnet、GPT-5等领先专有模型"
    ],
    "processed_at": "2026-01-16T08:47:06.905519"
  },
  {
    "id": "2601.10156v1",
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "abstract": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.",
    "authors": [
      "Yutao Mou",
      "Zhangchi Xue",
      "Lijun Li",
      "Peiyang Liu",
      "Shikun Zhang",
      "Wei Ye",
      "Jing Shao"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10156v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10156v1",
    "fetched_at": "2026-01-16T08:36:18.175963",
    "chinese_title": "ToolSafe：通过主动步骤级护栏与反馈增强基于LLM的智能体工具调用安全性",
    "chinese_summary": "该论文构建TS-Bench基准用于LLM智能体步骤级工具调用安全检测；开发基于多任务强化学习的TS-Guard护栏模型，可主动检测不安全工具调用并给出可解释反馈；提出TS-Flow框架，在提示注入攻击下平均减少有害工具调用65%，提升良性任务完成率约10%。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "构建TS-Bench基准填补LLM智能体步骤级工具调用安全检测的空白",
      "提出TS-Guard护栏模型与TS-Flow框架，有效降低有害工具调用并提升任务完成率"
    ],
    "processed_at": "2026-01-16T08:47:38.396071"
  },
  {
    "id": "2601.10154v1",
    "title": "MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging",
    "abstract": "Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.",
    "authors": [
      "Leonard Nürnberg",
      "Dennis Bontempi",
      "Suraj Pai",
      "Curtis Lisle",
      "Steve Pieper",
      "Ron Kikinis",
      "Sil van de Leemput",
      "Rahul Soni",
      "Gowtham Murugesan",
      "Cosmin Ciausu",
      "Miriam Groeneveld",
      "Felix J. Dorfner",
      "Jue Jiang",
      "Aneesh Rangnekar",
      "Harini Veeraraghavan",
      "Joeran S. Bosma",
      "Keno Bressem",
      "Raymond Mak",
      "Andrey Fedorov",
      "Hugo JWL Aerts"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG",
      "cs.SE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10154v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10154v1",
    "fetched_at": "2026-01-16T08:36:18.176014",
    "chinese_title": "MHub.ai：医疗影像AI模型的简单、标准化且可复现平台",
    "chinese_summary": "论文针对医疗影像AI研究与临床应用中模型实现多样、文档不一致及可复现性差的问题，提出开源容器化平台MHub.ai，将同行评审模型打包为标准化容器（支持DICOM等格式、统一接口、嵌入元数据），附带参考数据，含多模态前沿模型，还通过肺部分割模型对比验证其效用，增强透明度与可复现性。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出开源容器化平台MHub.ai，标准化医疗影像AI模型的访问与使用，解决可复现性、实现多样性等问题",
      "平台附带公开参考数据、交互式dashboard，支持社区贡献与多模型对比，提升透明度与实用性"
    ],
    "processed_at": "2026-01-16T08:48:00.429534"
  },
  {
    "id": "2601.10120v1",
    "title": "TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems",
    "abstract": "Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/",
    "authors": [
      "Rui Sun",
      "Jie Ding",
      "Chenghua Gong",
      "Tianjun Gu",
      "Yihang Jiang",
      "Juyuan Zhang",
      "Liming Pan",
      "Linyuan Lü"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10120v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10120v1",
    "fetched_at": "2026-01-16T08:36:18.176060",
    "chinese_title": "TopoDIM：多智能体系统中多样交互模式的一次性拓扑生成",
    "chinese_summary": "针对LLM-based多智能体系统中现有时空交互范式高延迟、高计算成本的问题，本文受评估与辩论机制启发，提出TopoDIM框架，支持智能体去中心化自主构建异构通信拓扑，无需迭代协调；实验表明该框架可减少46.41%的token消耗，平均性能提升1.50%，且适配异构智能体通信。",
    "tags": [
      "LLM",
      "Graph Neural Network",
      "Financial Agent"
    ],
    "key_contributions": [
      "提出TopoDIM框架，实现多智能体系统多样交互模式的一次性拓扑生成，支持去中心化自主构建异构通信拓扑，无需迭代协调",
      "实验验证该框架显著降低token消耗（46.41%）并提升任务性能（1.50%），且具备异构智能体通信的强适应性"
    ],
    "processed_at": "2026-01-16T08:48:32.441941"
  },
  {
    "id": "2601.10101v1",
    "title": "MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning",
    "abstract": "As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.",
    "authors": [
      "Ke Chen",
      "Jiandian Zeng",
      "Zihao Peng",
      "Guo Li",
      "Guangxue Zhang",
      "Tian Wang"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10101v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10101v1",
    "fetched_at": "2026-01-16T08:36:18.176087",
    "chinese_title": "矩阵即计划：带反馈驱动重规划的结构化逻辑推理",
    "chinese_summary": "针对现有Chain-of-Thought（CoT）在依赖符号表达式和严格演绎规则的逻辑推理任务上的不足，论文提出MatrixCoT框架：通过归一化类型化自然语言表达式、附加引用字段、矩阵规划保留步骤全局关系，使计划可验证；还加入反馈驱动重规划机制，识别缺陷并优化依赖矩阵，提升推理稳定性与可信度，实验在5个逻辑推理基准和5个LLM上验证了效果。",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark"
    ],
    "key_contributions": [
      "提出MatrixCoT结构化CoT框架，以矩阵规划保留步骤全局关系并使计划可验证，增强LLM逻辑推理的稳定性",
      "引入反馈驱动重规划机制，在语义等价约束下识别缺陷并优化依赖矩阵，提升推理结果可信度"
    ],
    "processed_at": "2026-01-16T08:48:45.869145"
  },
  {
    "id": "2601.10080v1",
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "abstract": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.",
    "authors": [
      "Letian Peng",
      "Kun Zhou",
      "Longfei Yun",
      "Yupeng Hou",
      "Jingbo Shang"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10080v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10080v1",
    "fetched_at": "2026-01-16T08:36:18.176111",
    "chinese_title": "从故事情节中推导角色逻辑：编码决策树方法",
    "chinese_summary": "针对现有角色行为profile非结构化、不可执行、验证弱导致智能体行为脆弱的问题，论文提出编码决策树（CDT）框架，从大规模叙事数据中诱导可执行且可解释的角色行为决策结构（条件规则树），支持透明检查与更新；实验在多基准上表明CDT显著优于人工编写的profile及现有方法，提升了智能体行为的可靠性。",
    "tags": [
      "NLP",
      "Financial Agent",
      "Benchmark"
    ],
    "key_contributions": [
      "提出可执行可解释的编码决策树（CDT）框架，从大规模叙事数据中诱导角色行为决策结构，解决现有profile的非结构化等问题",
      "实验验证CDT在多基准上优于人工及传统方法，提升智能体行为可靠性"
    ],
    "processed_at": "2026-01-16T08:49:18.398255"
  },
  {
    "id": "2601.10011v1",
    "title": "Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL",
    "abstract": "Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.",
    "authors": [
      "Zerui Yang",
      "Weichuan Wang",
      "Yanwei Xu",
      "Linqi Song",
      "Yudai Matsuda",
      "Wei Han",
      "Bo Bai"
    ],
    "published": "2026-01-15",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.10011v1",
    "arxiv_url": "https://arxiv.org/abs/2601.10011v1",
    "fetched_at": "2026-01-16T08:36:18.176138",
    "chinese_title": "Memo-SQL：用于无训练自然语言到SQL的结构化分解与经验驱动自修正",
    "chinese_summary": "现有NL2SQL系统存在依赖仅正确示例的上下文学习、分解随意及准确率-效率权衡问题；Memo-SQL通过实体级、层次化、原子顺序三种结构化分解策略促进多样推理，构建含成功查询与历史错误修正对的动态记忆，用检索增强提示实现经验驱动自修正（无需微调/外部API），在BIRD上达68.5%执行准确率，为开源无微调方法设新SOTA且资源消耗降低超10倍。",
    "tags": [
      "LLM",
      "NLP",
      "Execution"
    ],
    "key_contributions": [
      "提出无训练NL2SQL框架Memo-SQL，采用三种结构化分解策略（实体级、层次化、原子顺序）解决现有方法分解随意问题并提升推理多样性",
      "构建动态记忆（含成功查询与历史错误修正对），通过检索增强提示实现经验驱动自修正，无需微调/外部API，在BIRD上创开源无微调方法执行准确率新SOTA且资源效率提升超10倍"
    ],
    "processed_at": "2026-01-16T08:49:44.680786"
  },
  {
    "id": "2601.09923v1",
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "abstract": "AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.",
    "authors": [
      "Hanna Foerster",
      "Robert Mullins",
      "Tom Blanchard",
      "Nicolas Papernot",
      "Kristina Nikolić",
      "Florian Tramèr",
      "Ilia Shumailov",
      "Cheng Zhang",
      "Yiren Zhao"
    ],
    "published": "2026-01-14",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.09923v1",
    "arxiv_url": "https://arxiv.org/abs/2601.09923v1",
    "fetched_at": "2026-01-16T08:36:18.176169",
    "chinese_title": "CaMeLs也能使用计算机：计算机使用代理的系统级安全",
    "chinese_summary": "针对计算机使用代理（CUA）易受prompt注入攻击的问题，论文提出Single-Shot Planning方法，通过可信规划器在接触潜在恶意内容前生成含条件分支的完整执行图，保证控制流完整性；同时指出需额外措施防御Branch Steering攻击，并在OSWorld上验证安全与效用可共存（前沿模型保留57%性能，小开源模型提升19%）。",
    "tags": [
      "LLM",
      "Financial Agent",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出Single-Shot Planning方法，解决CUA架构隔离与连续UI观察的冲突，提供可证明的控制流完整性防御prompt注入攻击",
      "指出需额外措施防御Branch Steering攻击，并验证安全与效用可共存（前沿模型保留57%性能，小开源模型提升19%）"
    ],
    "processed_at": "2026-01-16T08:50:08.640932"
  },
  {
    "id": "2601.09876v1",
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "abstract": "Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.",
    "authors": [
      "Yifei Shen",
      "Yilun Zhao",
      "Justice Ou",
      "Tinglin Huang",
      "Arman Cohan"
    ],
    "published": "2026-01-14",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.09876v1",
    "arxiv_url": "https://arxiv.org/abs/2601.09876v1",
    "fetched_at": "2026-01-16T08:36:18.176193",
    "chinese_title": "临床文本到SQL中的患者相似队列推理",
    "chinese_summary": "本文引入CLINSQL基准（含633个MIMIC-IV专家标注任务，需多表连接、临床过滤及可执行SQL），评估22个闭源/开源模型并指出其性能距临床可靠仍有差距，推动临床可靠文本到SQL的发展。",
    "tags": [
      "NLP",
      "LLM",
      "Benchmark"
    ],
    "key_contributions": [
      "构建CLINSQL基准，覆盖临床文本到SQL的多表连接、临床过滤等核心需求",
      "评估22个模型揭示性能差距，为临床文本到SQL研究提供参考"
    ],
    "processed_at": "2026-01-16T08:50:25.943957"
  }
]