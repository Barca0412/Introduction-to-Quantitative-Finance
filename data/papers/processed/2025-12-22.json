[
  {
    "id": "2512.17895v1",
    "title": "Visualization of The Content of Surah al Fiil using Marker-Based Augmented Reality",
    "abstract": "This study presents the development of a marker-based augmented reality (AR) application designed to visualize the content of Surah al-Fil as an interactive and context-rich medium for Islamic education. Using a research and development approach, the system was developed through structured stages including data collection, user requirement analysis, interface design, 3D asset creation using Blender, and integration of Unity 3D with the Vuforia SDK. The application features key visual elements such as the elephant army, the Kaaba, and the Ababil birds, which were modeled in detail and linked to high-contrast image markers to ensure accurate and stable AR tracking. Functional testing demonstrated strong technical performance, achieving a 95 percent marker detection accuracy at an optimal distance of 30-40 cm with consistent real-time rendering across multiple Android devices. User evaluations involving students and Islamic education teachers indicated high acceptance, with an overall satisfaction score of 4.7 out of 5 in terms of usability, visual appeal, interactivity, and learning effectiveness. These findings indicate that AR-based learning media can enhance learner engagement, deepen understanding of Quranic narratives, and provide immersive insights into historical and spiritual contexts. Overall, this study demonstrates that marker-based AR technology has significant potential to support innovation in digital Islamic education by enriching traditional learning with interactive and visually intuitive experiences.",
    "authors": [
      "Ahmad Badru Al Husaeni",
      "Dzakwanfaiq Nauval",
      "Farid Muhtar Fathir",
      "Mahesa Adlan Falah",
      "Muhammad Miftahur Rizki Awalin"
    ],
    "published": "2025-12-19",
    "categories": [
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17895v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17895v1",
    "fetched_at": "2025-12-22T08:35:20.276371",
    "chinese_title": "基于标记的增强现实可视化《象章》内容",
    "chinese_summary": "该研究开发了用于伊斯兰教育的标记式AR应用以可视化《象章》内容，采用研发方法整合Blender 3D资产、Unity+Vuforia实现AR功能；测试显示95%标记检测准确率，用户满意度达4.7/5，验证了AR在伊斯兰教育中的创新潜力。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "开发了整合3D资产与标记式AR的《象章》可视化应用，实现互动式伊斯兰教育",
      "技术测试与用户评估验证了AR在伊斯兰教育中的有效性与技术稳定性"
    ],
    "processed_at": "2025-12-22T08:38:37.463411"
  },
  {
    "id": "2512.17791v1",
    "title": "Near-Maturity Asymptotics of Critical Prices of American Put Options under Exponential Lévy Models",
    "abstract": "In the present paper, we study the near-maturity ($t\\rightarrow T^{-}$) convergence rate of the optimal early-exercise price $b(t)$ of an American put under an exponential Lévy model with a {\\it nonzero} Brownian component. Two important settings, not previous covered in the literature, are considered. In the case that the optimal exercise price converges to the strike price ($b(T^{-})=K$), we contemplate models with negative jumps of unbounded variation (i.e., processes that exhibit high activity of negative jumps or sudden falls in asset prices). In the second case, when the optimal exercise price tend to a value lower than $K$, we consider infinite activity jumps (though still of bounded variations), extending existing results for models with finite jump activity (finitely many jumps in any finite interval). In both cases, we show that $b(T^{-})-b(t)$ is of order $\\sqrt{T-t}$ with explicit constants proportionality. Furthermore, we also derive the second-order near-maturity expansion of the American put price around the critical price along a certain parabolic branch.",
    "authors": [
      "José E. Figueroa-López",
      "Ruoting Gong"
    ],
    "published": "2025-12-19",
    "categories": [
      "q-fin.MF",
      "math.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17791v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17791v1",
    "fetched_at": "2025-12-22T08:35:20.276407",
    "chinese_title": "指数 Lévy 模型下美式看跌期权临界价格的近到期渐近性",
    "chinese_summary": "本文研究带非零布朗分量的指数Lévy模型下美式看跌期权最优提前行权价b(t)的近到期（t→T⁻）收敛速率，覆盖文献未涉及的两种重要情形：最优行权价收敛到执行价K时含无界变差负跳的模型，以及行权价趋于低于K值时含无限活动但有界变差跳的模型；证明两种情形下b(T⁻)-b(t)为√(T-t)阶且有显式比例常数，还推导了美式看跌期权价格沿某抛物分支围绕临界价格的二阶近到期展开。",
    "tags": [
      "Options",
      "Asset Pricing",
      "Volatility"
    ],
    "key_contributions": [
      "首次覆盖带非零布朗分量的指数Lévy模型下美式看跌期权最优提前行权价近到期收敛的两种文献空白情形（含无界变差负跳、含无限活动但有界变差跳）",
      "证明两种情形下最优行权价差为√(T-t)阶且有显式比例常数，推导美式看跌期权价格沿抛物分支的二阶近到期展开"
    ],
    "processed_at": "2025-12-22T08:38:59.623068"
  },
  {
    "id": "2512.17702v1",
    "title": "Relative arbitrage problem under eigenvalue lower bounds",
    "abstract": "We give a new formulation of the relative arbitrage problem from stochastic portfolio theory that asks for a time horizon beyond which arbitrage relative to the market exists in all ``sufficiently volatile'' markets. In our formulation, ``sufficiently volatile'' is interpreted as a lower bound on an ordered eigenvalue of the instantaneous covariation matrix, a quantity that has been studied extensively in the empirical finance literature. Upon framing the problem in the language of stochastic optimal control, we characterize the time horizon in question through the unique upper semicontinuous viscosity solution of a fully nonlinear elliptic partial differential equation (PDE). In a special case, this PDE amounts to the arrival time formulation of the Ambrosio-Soner co-dimension mean curvature flow. Beyond the setting of stochastic portfolio theory, the stochastic optimal control problem is analyzed for arbitrary compact, possibly non-convex, domains, thanks to a boundedness assumption on the instantaneous covariation matrix.",
    "authors": [
      "Jou-Hua Lai",
      "Mykhaylo Shkolnikov",
      "H. Mete Soner"
    ],
    "published": "2025-12-19",
    "categories": [
      "q-fin.MF",
      "math.AP",
      "math.DG",
      "math.OC",
      "math.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17702v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17702v1",
    "fetched_at": "2025-12-22T08:35:20.276434",
    "chinese_title": "特征值下界下的相对套利问题",
    "chinese_summary": "本文重新表述随机组合理论中的相对套利问题，以瞬时协变矩阵有序特征值下界定义“足够波动”市场，将问题转化为随机最优控制，通过全非线性椭圆PDE的唯一上半连续粘性解刻画存在相对市场套利的时间范围；特殊情况对应Ambrosio-Soner余维平均曲率流的到达时间表述，且因协变矩阵有界假设可扩展到任意紧（含非凸）域。",
    "tags": [
      "Portfolio Optimization",
      "Volatility",
      "Benchmark"
    ],
    "key_contributions": [
      "以瞬时协变矩阵有序特征值下界定义“足够波动”市场，将相对套利问题转化为随机最优控制，通过全非线性椭圆PDE粘性解刻画套利时间范围",
      "特殊情况对应Ambrosio-Soner余维平均曲率流到达时间表述，且扩展到任意紧（含非凸）域"
    ],
    "processed_at": "2025-12-22T08:39:19.814837"
  },
  {
    "id": "2512.17354v1",
    "title": "Implementation of Augmented Reality as an Educational Tool for Practice in Early Childhood",
    "abstract": "Learning Wudhu for young children requires engaging and interactive media to foster a deep understanding of the worship procedures. This study aims to develop a Wudhu learning application based on Augmented Reality (AR) as an interactive and fun educational medium. The development method used includes the stages of needs analysis, system design, implementation, and testing using Black Box Testing. The system utilizes marker-based tracking to display 3D animations of Wudhu movements in real-time when the camera detects a marker on the printed media. The test results indicate that all main functions run well, and a limited trial on children aged 5-7 years showed an increase in learning interest and a better understanding of the Wudhu sequence. Thus, the application of AR technology is proven effective in improving the quality of basic worship instruction for young children.",
    "authors": [
      "Wisnu Uriawan",
      "Muhammad Aditya Hafizh Zahran",
      "Inayah Ayu Deswita",
      "Muhammad Ahsani Taqwim",
      "Ismail Muhammad Ahmadi",
      "Marvi Yoga Pratama"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.HC",
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17354v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17354v1",
    "fetched_at": "2025-12-22T08:35:20.276463",
    "chinese_title": "增强现实作为幼儿实践教育工具的实现",
    "chinese_summary": "本研究针对幼儿学习Wudhu的需求，采用需求分析、系统设计、实现及黑盒测试方法开发基于AR的学习应用，通过标记跟踪实时显示3D动作；测试表明应用功能良好，能提升幼儿学习兴趣与对Wudhu流程的理解，证明AR有效改进基础礼拜教学质量。",
    "tags": [
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "开发了基于AR的幼儿Wudhu学习应用，利用标记跟踪实现实时3D动作展示",
      "验证了AR技术在提升幼儿学习兴趣和基础礼拜教学质量方面的有效性"
    ],
    "processed_at": "2025-12-22T08:39:44.022496"
  },
  {
    "id": "2512.17225v1",
    "title": "Modelling financial time series with $φ^{4}$ quantum field theory",
    "abstract": "We use a $φ^{4}$ quantum field theory with inhomogeneous couplings and explicit symmetry-breaking to model an ensemble of financial time series from the S$\\&$P 500 index. The continuum nature of the $φ^4$ theory avoids the inaccuracies that occur in Ising-based models which require a discretization of the time series. We demonstrate this using the example of the 2008 global financial crisis. The $φ^{4}$ quantum field theory is expressive enough to reproduce the higher-order statistics such as the market kurtosis, which can serve as an indicator of possible market shocks. Accurate reproduction of high kurtosis is absent in binarized models. Therefore Ising models, despite being widely employed in econophysics, are incapable of fully representing empirical financial data, a limitation not present in the generalization of the $φ^{4}$ scalar field theory. We then investigate the scaling properties of the $φ^{4}$ machine learning algorithm and extract exponents which govern the behavior of the learned couplings (or weights and biases in ML language) in relation to the number of stocks in the model. Finally, we use our model to forecast the price changes of the AAPL, MSFT, and NVDA stocks. We conclude by discussing how the $φ^{4}$ scalar field theory could be used to build investment strategies and the possible intuitions that the QFT operations of dimensional compactification and renormalization can provide for financial modelling.",
    "authors": [
      "Dimitrios Bachtis",
      "David S. Berman",
      "Arabella Schelpe"
    ],
    "published": "2025-12-19",
    "categories": [
      "q-fin.ST",
      "cond-mat.dis-nn",
      "cs.CE",
      "hep-th"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17225v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17225v1",
    "fetched_at": "2025-12-22T08:35:20.276487",
    "chinese_title": "用φ⁴量子场论建模金融时间序列",
    "chinese_summary": "本文提出带非均匀耦合和显式对称破缺的φ⁴量子场论模型，用于标普500金融时间序列集合建模，避免Ising模型离散化缺陷并准确重现市场峰度等高阶统计；还分析模型缩放性质、预测AAPL等股票价格变化，探讨量子场论操作对金融建模的启发。",
    "tags": [
      "Time Series",
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "提出φ⁴量子场论模型，解决Ising类模型无法准确重现金融时间序列高阶统计的问题",
      "分析模型缩放性质并验证股票价格预测能力，拓展量子场论在金融建模中的应用"
    ],
    "processed_at": "2025-12-22T08:40:00.539751"
  },
  {
    "id": "2512.17185v1",
    "title": "Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning",
    "abstract": "Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.   We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.   This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.",
    "authors": [
      "Sandeep Neela"
    ],
    "published": "2025-12-19",
    "categories": [
      "q-fin.RM",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17185v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17185v1",
    "fetched_at": "2025-12-22T08:35:20.276508",
    "chinese_title": "系统性风险雷达：用于市场崩盘早期预警的多层图框架",
    "chinese_summary": "本文提出系统性风险雷达（SRR）框架，将金融市场建模为多层图以检测系统性脆弱性及崩盘状态转换的早期信号；在互联网泡沫、全球金融危机和新冠冲击三次危机中验证，结果表明图结构信息比仅基于特征的模型更具预警价值；研究还指出可扩展更多图层（如行业/因子暴露、情绪）及更具表达力的时间架构以适配不同危机类型。",
    "tags": [
      "Graph Neural Network",
      "Risk Management",
      "Anomaly",
      "Time Series"
    ],
    "key_contributions": [
      "提出系统性风险雷达（SRR）多层图框架，突破孤立价格变动局限，通过捕捉市场参与者交互检测崩盘早期信号",
      "实证验证图结构信息的预警价值，为扩展多层图（含情绪、因子暴露）及时序架构提供方向"
    ],
    "processed_at": "2025-12-22T08:40:11.871477"
  },
  {
    "id": "2512.17594v1",
    "title": "MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification",
    "abstract": "Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.",
    "authors": [
      "Tosin Ige",
      "Christopher Kiekintveld",
      "Aritran Piplai",
      "Asif Rahman",
      "Olukunle Kolade",
      "Sasidhar Kunapuli"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17594v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17594v1",
    "fetched_at": "2025-12-22T08:35:33.570341",
    "chinese_title": "MAD-OOD：一种基于深度学习聚类驱动的离群分布恶意软件检测与分类框架",
    "chinese_summary": "论文提出两阶段聚类驱动的MAD-OOD框架，第一阶段利用基于高斯判别分析（GDA）的类条件球形决策边界建模恶意软件家族嵌入，无需训练时的离群分布（OOD）数据即可区分分布内与OOD样本；第二阶段融合聚类预测、精炼嵌入及监督分类器输出，提升未知恶意软件家族的检测与分类精度，实验在基准数据集上显著优于现有方法。",
    "tags": [
      "Deep Learning",
      "Anomaly",
      "Benchmark"
    ],
    "key_contributions": [
      "提出两阶段聚类驱动的MAD-OOD框架，第一阶段基于GDA的类条件球形决策边界建模家族嵌入，无需训练OOD数据即可区分分布内与OOD样本",
      "第二阶段融合多源信息提升未知恶意软件家族的检测分类精度，基准数据集实验显示性能优于现有方法"
    ],
    "processed_at": "2025-12-22T08:40:27.810666"
  },
  {
    "id": "2512.17048v1",
    "title": "Another Fit Bites the Dust: Conformal Prediction as a Calibration Standard for Machine Learning in High-Energy Physics",
    "abstract": "Machine-learning techniques are essential in modern collider research, yet their probabilistic outputs often lack calibrated uncertainty estimates and finite-sample guarantees, limiting their direct use in statistical inference and decision-making. Conformal prediction (CP) provides a simple, distribution-free framework for calibrating arbitrary predictive models without retraining, yielding rigorous uncertainty quantification with finite-sample coverage guarantees under minimal exchangeability assumptions, without reliance on asymptotics, limit theorems, or Gaussian approximations. In this work, we investigate CP as a unifying calibration layer for machine-learning applications in high-energy physics. Using publicly available collider datasets and a diverse set of models, we show that a single conformal formalism can be applied across regression, binary and multi-class classification, anomaly detection, and generative modelling, converting raw model outputs into statistically valid prediction sets, typicality regions, and p-values with controlled false-positive rates. While conformal prediction does not improve raw model performance, it enforces honest uncertainty quantification and transparent error control. We argue that conformal calibration should be adopted as a standard component of machine-learning pipelines in collider physics, enabling reliable interpretation, robust comparisons, and principled statistical decisions in experimental and phenomenological analyses.",
    "authors": [
      "Jack Y. Araz",
      "Michael Spannowsky"
    ],
    "published": "2025-12-18",
    "categories": [
      "hep-ph",
      "cs.AI",
      "hep-ex"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17048v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17048v1",
    "fetched_at": "2025-12-22T08:35:33.570372",
    "chinese_title": "又一次拟合失败：共形预测作为高能物理中机器学习的校准标准",
    "chinese_summary": "论文指出高能物理中机器学习概率输出缺乏校准的不确定性估计与有限样本保证，共形预测（CP）是无需重训练的分布无关框架，可提供严格覆盖保证；通过公开对撞机数据集验证，CP能统一应用于回归、分类等多任务，转化为统计有效预测集等，虽不提升原始性能但强制诚实不确定性量化；主张将CP作为对撞机物理机器学习 pipeline 的标准组件。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Risk Management",
      "Benchmark"
    ],
    "key_contributions": [
      "证明共形预测可统一适配高能物理机器学习多任务（回归、分类等），无需重训练且提供严格有限样本覆盖保证",
      "主张将共形预测作为对撞机物理机器学习 pipeline 的标准组件，支撑可靠解释与原则性统计决策"
    ],
    "processed_at": "2025-12-22T08:40:38.987833"
  },
  {
    "id": "2512.17671v1",
    "title": "Polyharmonic Cascade",
    "abstract": "This paper presents a deep machine learning architecture, the \"polyharmonic cascade\" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed \"constellations\" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.",
    "authors": [
      "Yuriy N. Bakhvalov"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.LG",
      "math.NA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17671v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17671v1",
    "fetched_at": "2025-12-22T08:36:03.611701",
    "chinese_title": "多调和级联",
    "chinese_summary": "本文提出多调和级联深度学习架构，基于随机函数理论与无差别原则设计（每层为多调和样条包），可近似任意复杂度非线性函数且保留全局光滑性与概率解释；同时提出替代梯度下降的训练方法，通过批次内解全局线性系统同步更新所有层，计算高效（GPU上2D矩阵操作），并在MNIST上验证了快速学习无过拟合的效果。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出多调和级联深度学习架构，基于随机函数理论与无差别原则，可近似任意复杂度非线性函数且保留全局光滑性与概率解释",
      "提出替代梯度下降的训练方法，通过批次内解全局线性系统同步更新所有层，计算高效且保持理论一致性"
    ],
    "processed_at": "2025-12-22T08:40:55.533629"
  },
  {
    "id": "2512.17574v1",
    "title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing",
    "abstract": "Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.   To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.",
    "authors": [
      "Lingxiao Zhao",
      "Haoran Zhou",
      "Yuezhi Che",
      "Dazhao Cheng"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17574v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17574v1",
    "fetched_at": "2025-12-22T08:36:03.611740",
    "chinese_title": "通过GPU内部调度与资源共享实现解聚式多阶段多模态大语言模型推理",
    "chinese_summary": "该论文针对多模态大语言模型（MLLM）推理中多模态预处理延迟高、视觉编码器与LLM推理跨阶段阻塞及资源利用率低等问题，提出FlashCodec和UnifiedServe两种互补设计：FlashCodec通过协作多GPU视频解码加速预处理，降低延迟并保持高吞吐量；UnifiedServe逻辑解耦视觉到文本与推理阶段以消除跨阶段阻塞，同时物理共享GPU资源提升利用率。",
    "tags": [
      "LLM",
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "提出FlashCodec，通过协作多GPU视频解码加速MLLM的多模态预处理阶段，降低解码延迟并保持高吞吐量",
      "提出UnifiedServe，逻辑解耦视觉编码器与LLM推理阶段以消除跨阶段阻塞，同时物理共享GPU资源最大化利用率，优化端到端MLLM推理流程"
    ],
    "processed_at": "2025-12-22T08:41:12.655991"
  },
  {
    "id": "2512.17570v1",
    "title": "GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping",
    "abstract": "SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake",
    "authors": [
      "Yikang Yue",
      "Yishu Yin",
      "Xuehai Qian"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17570v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17570v1",
    "fetched_at": "2025-12-22T08:36:03.611763",
    "chinese_title": "GreedySnake：通过高效调度与优化器步骤重叠加速SSD卸载的大语言模型训练",
    "chinese_summary": "该论文针对SSD卸载的大语言模型（LLM）训练，提出GreedySnake系统，采用垂直调度（完成一层所有微批次后再处理下一层）替代现有水平调度，同时重叠部分优化器步骤与下一轮前向传播以缓解I/O瓶颈；实验在A100 GPU上验证，相比ZeRO-Infinity，GPT-65B在1/4GPU上吞吐量提升1.96x/1.93x，GPT-175B在1GPU上提升2.53x。",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出GreedySnake系统，采用垂直调度优化SSD卸载的LLM训练，提升小批量下的训练吞吐量，更接近 roofline 模型理想场景",
      "通过重叠优化器步骤与下一轮前向传播缓解I/O瓶颈，实验验证吞吐量显著优于ZeRO-Infinity"
    ],
    "processed_at": "2025-12-22T08:41:24.634396"
  },
  {
    "id": "2512.17419v1",
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "abstract": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
    "authors": [
      "Lilin Wang",
      "Lucas Ramalho",
      "Alan Celestino",
      "Phuc Anthony Pham",
      "Yu Liu",
      "Umang Kumar Sinha",
      "Andres Portillo",
      "Onassis Osunwa",
      "Gabriel Maduekwe"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17419v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17419v1",
    "fetched_at": "2025-12-22T08:36:03.611809",
    "chinese_title": "SWE-Bench++：从开源仓库可扩展生成软件工程基准的框架",
    "chinese_summary": "现有SWE-bench等基准存在手动整理、静态数据集、仅聚焦Python bug修复的局限，论文提出自动化框架SWE-Bench++，从GitHub拉取请求（PR）生成覆盖11种语言、包含bug修复和功能请求的可复现仓库级编码任务，实验显示基于该基准微调可提升多语言SWE-bench性能，当前最强LLM在其任务上表现有限。",
    "tags": [
      "LLM",
      "Benchmark",
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "提出自动化框架SWE-Bench++，从GitHub PR生成覆盖11种语言、含bug修复和功能请求的可复现仓库级编码任务，解决现有基准的手动化、静态化、语言单一问题",
      "构建11133个实例的多语言基准，实验验证微调该数据集可提升多语言SWE-bench性能，揭示当前最强LLM在该任务上的表现差距"
    ],
    "processed_at": "2025-12-22T08:41:44.400833"
  },
  {
    "id": "2512.17387v1",
    "title": "CIFE: Code Instruction-Following Evaluation",
    "abstract": "Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.",
    "authors": [
      "Sravani Gunnu",
      "Shanmukha Guttula",
      "Hima Patel"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.SE",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17387v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17387v1",
    "fetched_at": "2025-12-22T08:36:03.611850",
    "chinese_title": "CIFE：代码指令遵循评估",
    "chinese_summary": "该论文针对现有代码生成基准仅聚焦功能正确性的局限，提出包含1000个Python任务（每个含平均7个开发者指定约束）的CIFE基准，通过四阶段人-LLM流程构建原子、相关且客观的约束，引入C2A分数综合衡量正确性与约束遵循度；评估14个开源/闭源模型发现，强模型部分约束遵循超90%但严格遵循仅39%-66%，凸显代码生成需兼顾正确性与开发者意图遵循。",
    "tags": [
      "LLM",
      "Benchmark",
      "NLP"
    ],
    "key_contributions": [
      "构建包含多类开发者约束的CIFE代码生成评估基准，弥补现有基准仅关注功能正确性的不足",
      "提出C2A分数综合衡量代码生成的正确性与约束遵循度，揭示模型严格遵循与部分遵循的显著差距"
    ],
    "processed_at": "2025-12-22T08:41:59.617005"
  },
  {
    "id": "2512.17370v1",
    "title": "TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data",
    "abstract": "Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.",
    "authors": [
      "Deqing Liu",
      "Yinfeng Gao",
      "Deheng Qian",
      "Qichao Zhang",
      "Xiaoqing Ye",
      "Junyu Han",
      "Yupeng Zheng",
      "Xueyi Liu",
      "Zhongpu Xia",
      "Dawei Ding",
      "Yifeng Pan",
      "Dongbin Zhao"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17370v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17370v1",
    "fetched_at": "2025-12-22T08:36:03.611890",
    "chinese_title": "TakeAD：基于偏好的端到端自动驾驶后优化方法（结合专家接管数据）",
    "chinese_summary": "现有端到端自动驾驶模仿学习存在开环训练与闭环部署的错位问题，易引发专家接管；本文提出TakeAD框架，通过高效收集专家接管数据，结合迭代DAgger模仿学习与DPO偏好对齐的后优化方法，逐步学习接管场景的恢复策略，提升闭环驾驶性能。",
    "tags": [
      "Deep Learning",
      "Reinforcement Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "设计受真实自动驾驶接管机制启发的高效专家接管数据收集 pipeline",
      "提出结合迭代DAgger与DPO的后优化框架，缓解开环-闭环错位问题，提升闭环驾驶性能"
    ],
    "processed_at": "2025-12-22T08:42:11.890844"
  },
  {
    "id": "2512.17259v1",
    "title": "Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems",
    "abstract": "As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.",
    "authors": [
      "Abhivansh Gupta"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17259v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17259v1",
    "fetched_at": "2025-12-22T08:36:03.611913",
    "chinese_title": "优先可验证性代理：用于控制自主LLM系统的可证明可观测性与轻量级审计代理",
    "chinese_summary": "论文提出优先可验证性架构，整合加密与符号方法实现运行时行为证明、嵌入轻量级审计代理持续验证意图与行为，还引入OPERA基准套件评估对齐检测、隐蔽策略下的检测时间及机制抗对抗性注入能力；方法将评估重点从“对齐可能性”转向“检测修复的速度与可靠性”。",
    "tags": [
      "LLM",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "提出优先可验证性架构，包含运行时行为证明、轻量级审计代理及高风险操作的挑战-响应证明协议",
      "引入OPERA基准套件，聚焦评估LLM代理的对齐检测能力、隐蔽策略下的检测时间及可验证机制的抗对抗性注入鲁棒性"
    ],
    "processed_at": "2025-12-22T08:42:27.183616"
  },
  {
    "id": "2512.17250v1",
    "title": "Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction",
    "abstract": "Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.",
    "authors": [
      "Ziyang Lin",
      "Zixuan Sun",
      "Sanhorn Chen",
      "Xiaoyang Chen",
      "Roy Zhao"
    ],
    "published": "2025-12-19",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17250v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17250v1",
    "fetched_at": "2025-12-22T08:36:03.611938",
    "chinese_title": "通过输入预测与误匹配修正加速多模态大语言模型游戏性能",
    "chinese_summary": "论文针对实时序列控制代理的推理延迟瓶颈，提出基于预测-验证思想的推测执行框架，结合预训练世界模型生成短视距动作队列，通过轻量学习修正器处理潜态误匹配（大误匹配则fallback重规划）；实验在DMC人形行走任务中显著减少规划推理次数、降低端到端延迟，且仅轻微损失控制性能。",
    "tags": [
      "Reinforcement Learning",
      "Transformer",
      "Deep Learning",
      "LLM"
    ],
    "key_contributions": [
      "提出适配TD-MPC2模型控制的推测执行框架，结合输入预测与误匹配修正提升实时控制效率",
      "实验验证该框架在保持强控制性能的前提下，显著降低规划推理次数与端到端延迟"
    ],
    "processed_at": "2025-12-22T08:42:41.956444"
  },
  {
    "id": "2512.17108v1",
    "title": "Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse",
    "abstract": "Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\\leq$ 2.3 Recall@1 in retrieval, $\\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.",
    "authors": [
      "Kunjal Panchal",
      "Saayan Mitra",
      "Somdeb Sarkhel",
      "Haoliang Wang",
      "Ishita Dasgupta",
      "Gang Wu",
      "Hui Guan"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.LG",
      "cs.MM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17108v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17108v1",
    "fetched_at": "2025-12-22T08:36:03.611967",
    "chinese_title": "Atom：通过模块化复用实现高效的端侧视频-语言流水线",
    "chinese_summary": "针对视频-语言模型在移动设备上执行效率低的问题，论文提出Atom系统，将十亿参数模型分解为可复用模块（如视觉编码器、语言解码器），跨子任务复用以消除重复加载并支持并行执行，在商品智能手机上比非复用基线快27%-33%且性能损失极小。",
    "tags": [
      "Deep Learning",
      "Transformer",
      "Execution"
    ],
    "key_contributions": [
      "提出面向端侧的视频-语言流水线重构系统Atom，基于模块化复用设计消除重复模型加载",
      "实现跨子任务的模块复用与并行执行，在移动设备上显著提升执行效率且性能损失可忽略"
    ],
    "processed_at": "2025-12-22T08:42:52.645636"
  },
  {
    "id": "2512.17053v1",
    "title": "Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL",
    "abstract": "Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.",
    "authors": [
      "Khushboo Thaker",
      "Yony Bresler"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17053v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17053v1",
    "fetched_at": "2025-12-22T08:36:03.611986",
    "chinese_title": "面向Text-to-SQL的结构化思维链知识蒸馏方法",
    "chinese_summary": "企业部署Text-to-SQL系统面临成本、安全与性能的三难困境，现有方案需在昂贵大模型与低性能小模型间取舍；论文提出Struct-SQL框架，以查询执行计划为结构化推理蓝图实现大模型知识蒸馏；实验显示该方法比非结构化思维链蒸馏基线绝对提升8.1%，语法错误显著减少，验证了结构化推理对小模型可靠SQL生成的价值。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出Struct-SQL知识蒸馏框架，以查询执行计划为结构化推理蓝图，解决Text-to-SQL系统的三难困境",
      "实验证明结构化CoT蒸馏比非结构化基线提升8.1%，语法错误显著减少，验证结构化推理对小模型可靠SQL生成的作用"
    ],
    "processed_at": "2025-12-22T08:43:03.696240"
  },
  {
    "id": "2512.17052v1",
    "title": "Dynamic Tool Dependency Retrieval for Efficient Function Calling",
    "abstract": "Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\\%$ and $104\\%$ compared to state-of-the-art static retrievers.",
    "authors": [
      "Bhrij Patel",
      "Davide Belli",
      "Amir Jalalirad",
      "Maximilian Arnold",
      "Aleksandr Ermovol",
      "Bence Major"
    ],
    "published": "2025-12-18",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.17052v1",
    "arxiv_url": "https://arxiv.org/abs/2512.17052v1",
    "fetched_at": "2025-12-22T08:36:03.612013",
    "chinese_title": "面向高效函数调用的动态工具依赖检索",
    "chinese_summary": "针对现有工具检索方法依赖静态有限输入、无法捕捉多步工具依赖及动态任务上下文的问题，论文提出轻量级动态工具依赖检索（DTDR）方法，该方法结合初始查询与演化执行上下文，从函数调用演示中建模工具依赖以实现自适应检索；实验表明，相比最先进静态检索器，动态工具检索使函数调用成功率提升23%至104%。",
    "tags": [
      "LLM",
      "NLP",
      "Financial Agent",
      "Benchmark"
    ],
    "key_contributions": [
      "提出轻量级动态工具依赖检索（DTDR）方法，结合初始查询与演化执行上下文建模工具依赖，实现自适应工具检索",
      "实验验证DTDR相比最先进静态检索器显著提升函数调用成功率（23%~104%）"
    ],
    "processed_at": "2025-12-22T08:43:18.515481"
  }
]