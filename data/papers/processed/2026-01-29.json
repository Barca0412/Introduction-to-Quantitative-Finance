[
  {
    "id": "2601.20643v1",
    "title": "Shrinkage Estimators for Mean and Covariance: Evidence on Portfolio Efficiency Across Market Dimensions",
    "abstract": "The mean-variance model remains the most prevalent investment framework, built on diversification principles. However, it consistently struggles with estimation errors in expected returns and the covariance matrix, its core parameters. To address this concern, this research evaluates the performance of mean variance (MV) and global minimum-variance (GMV) models across various shrinkage estimators designed to improve these parameters. Specifically, we examine five shrinkage estimators for expected returns and eleven for the covariance matrix. To compare multiple portfolios, we employ a super efficient data envelopment analysis model to rank the portfolios according to investors risk-return preferences. Our comprehensive empirical investigation utilizes six real world datasets with different dimensional characteristics, applying a rolling window methodology across three out of sample testing periods. Following the ranking process, we examine the chosen shrinkage based MV or GMV portfolios against five traditional portfolio optimization techniques classical MV and GMV for sample estimates, MiniMax, conditional value at risk, and semi mean absolute deviation risk measures. Our empirical findings reveal that, in most scenarios, the GMV model combined with the Ledoit Wolf two parameter shrinkage covariance estimator (COV2) represents the optimal selection for a broad spectrum of investors. Meanwhile, the MV model utilizing COV2 alongside the sample mean (SM) proves more suitable for return oriented investors. These two identified models demonstrate superior performance compared to traditional benchmark approaches. Overall, this study lays the groundwork for a more comprehensive understanding of how specific shrinkage models perform across diverse investor profiles and market setups.",
    "authors": [
      "Rupendra Yadav",
      "Amita Sharma",
      "Aparna Mehra"
    ],
    "published": "2026-01-28",
    "categories": [
      "q-fin.PM",
      "stat.AP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20643v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20643v1",
    "fetched_at": "2026-01-29T08:46:41.168509",
    "chinese_title": "均值和协方差的收缩估计器：不同市场维度下的投资组合效率证据",
    "chinese_summary": "本文针对均值-方差模型的参数估计误差问题，评估5种均值收缩估计器与11种协方差收缩估计器在MV及GMV模型中的表现，通过超效率DEA结合滚动窗口在6个真实数据集上验证；发现GMV结合Ledoit-Wolf两参数协方差收缩估计器（COV2）多场景最优，MV结合COV2与样本均值适配特定场景。",
    "tags": [
      "Portfolio Optimization",
      "Risk Management",
      "Benchmark"
    ],
    "key_contributions": [
      "系统评估多类收缩估计器在均值-方差及全局最小方差模型中的表现，采用超效率DEA与滚动窗口在多维度真实数据集上验证投资组合效率",
      "揭示GMV结合Ledoit-Wolf两参数协方差收缩估计器（COV2）为多数投资者的最优选择，MV结合该协方差估计器与样本均值适配特定场景"
    ],
    "processed_at": "2026-01-29T08:49:49.828203"
  },
  {
    "id": "2601.20533v1",
    "title": "Incorporating data drift to perform survival analysis on credit risk",
    "abstract": "Survival analysis has become a standard approach for modelling time to default by time-varying covariates in credit risk. Unlike most existing methods that implicitly assume a stationary data-generating process, in practise, mortgage portfolios are exposed to various forms of data drift caused by changing borrower behaviour, macroeconomic conditions, policy regimes and so on. This study investigates the impact of data drift on survival-based credit risk models and proposes a dynamic joint modelling framework to improve robustness under non-stationary environments. The proposed model integrates a longitudinal behavioural marker derived from balance dynamics with a discrete-time hazard formulation, combined with landmark one-hot encoding and isotonic calibration. Three types of data drift (sudden, incremental and recurring) are simulated and analysed on mortgage loan datasets from Freddie Mac. Experiments and corresponding evidence show that the proposed landmark-based joint model consistently outperforms classical survival models, tree-based drift-adaptive learners and gradient boosting methods in terms of discrimination and calibration across all drift scenarios, which confirms the superiority of our model design.",
    "authors": [
      "Jianwei Peng",
      "Stefan Lessmann"
    ],
    "published": "2026-01-28",
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.RM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20533v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20533v1",
    "fetched_at": "2026-01-29T08:46:41.168549",
    "chinese_title": "融入数据漂移以进行信用风险生存分析",
    "chinese_summary": "现有信用风险生存分析多假设数据生成过程平稳，但实际抵押贷款组合存在由借款人行为、宏观经济等导致的数据漂移；本文提出动态联合建模框架，整合纵向行为标记、离散时间风险率公式、地标独热编码与等渗校准以提升非平稳环境下的鲁棒性；在Freddie Mac数据集上模拟三种数据漂移并实验，结果显示该模型在所有漂移场景下的判别和校准性能优于经典生存模型及其他对比方法。",
    "tags": [
      "Risk Management",
      "Time Series",
      "Behavioral Finance"
    ],
    "key_contributions": [
      "提出融入数据漂移的动态联合建模框架，整合纵向行为标记、离散风险率等方法以应对信用风险中的非平稳性",
      "在抵押贷款数据集上验证，该模型在三类数据漂移场景下的判别与校准性能优于经典及主流对比方法"
    ],
    "processed_at": "2026-01-29T08:50:14.164624"
  },
  {
    "id": "2601.20452v1",
    "title": "Manipulation in Prediction Markets: An Agent-based Modeling Experiment",
    "abstract": "Prediction markets mobilize financial incentives to forecast binary event outcomes through the aggregation of dispersed beliefs and heterogeneous information. Their growing popularity and demonstrated predictive accuracy in political elections have raised speculation and concern regarding their susceptibility to manipulation and the potential consequences for democratic processes. Using agent-based simulations combined with an analytic characterization of price dynamics, we study how high-budget agents can introduce price distortions in prediction markets. We explore the persistence and stability of these distortions in the presence of herding or stubborn agents, and analyze how agent expertise affects market-price variance. Firstly we propose an agent-based model of a prediction market in which bettors with heterogeneous expertise, noisy private information, variable learning rates and budgets observe the evolution of public opinion on a binary election outcome to inform their betting strategies in the market. The model exhibits stability across a broad parameter space, with complex agent behaviors and price interactions producing self-regulatory price discovery. Second, using this simulation framework, we investigate the conditions under which a highly resourced minority, or ''whale'' agent, with a biased valuation can distort the market price, and for how long. We find that biased whales can temporarily shift prices, with the magnitude and duration of distortion increasing when non-whale bettors exhibit herding behavior and slow learning. Our theoretical analysis corroborates these results, showing that whales can shift prices proportionally to their share of market capital, with distortion duration depending on non-whale learning rates and herding intensity.",
    "authors": [
      "Bridget Smart",
      "Ebba Mark",
      "Anne Bastian",
      "Josefina Waugh"
    ],
    "published": "2026-01-28",
    "categories": [
      "econ.GN",
      "physics.soc-ph",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20452v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20452v1",
    "fetched_at": "2026-01-29T08:46:41.168578",
    "chinese_title": "预测市场中的操纵：基于主体的建模实验",
    "chinese_summary": "本文采用基于主体的模拟结合价格动态分析方法，研究预测市场中的操纵行为；提出包含异质专业知识、噪声私有信息等要素的预测市场主体模型，该模型在宽参数空间稳定且能自调节价格发现，并探究资源充足的有偏估值“鲸鱼”主体扭曲价格的条件、时长及从众/固执主体、专业知识对价格的影响。",
    "tags": [
      "Financial Agent",
      "Behavioral Finance",
      "Asset Pricing",
      "Market Microstructure"
    ],
    "key_contributions": [
      "提出包含异质专业知识、噪声私有信息等要素的预测市场主体模型，该模型在宽参数空间稳定且能通过主体行为互动实现自调节价格发现",
      "探究资源充足的有偏估值“鲸鱼”主体扭曲市场价格的条件、时长，及从众/固执主体、主体专业知识对价格扭曲和方差的影响"
    ],
    "processed_at": "2026-01-29T08:50:42.385885"
  },
  {
    "id": "2601.20336v1",
    "title": "Do Whitepaper Claims Predict Market Behavior? Evidence from Cryptocurrency Factor Analysis",
    "abstract": "Cryptocurrency projects articulate value propositions through whitepapers, making claims about functionality and technical capabilities. This study investigates whether these narratives align with observed market behavior. We construct a pipeline combining zero-shot NLP classification (BART-MNLI) with CP tensor decomposition to compare three spaces: (1) a claims matrix from 24 whitepapers across 10 semantic categories, (2) market statistics for 49 assets over two years of hourly data, and (3) latent factors from tensor decomposition (rank 2, 92.45% variance explained). Using Procrustes rotation and Tucker's congruence coefficient, we test alignment across 23 common entities.   Results show weak alignment: claims-statistics (phi=0.341, p=0.332), claims-factors (phi=0.077, p=0.747), and statistics-factors (phi=0.197, p<0.001). The statistics-factors significance validates our methodology, confirming the pipeline detects relationships when present. Inter-model validation with DeBERTa-v3 yields 32% exact agreement but 67% top-3 agreement. Cross-sectional analysis reveals heterogeneous contributions: NEAR, MKR, ATOM show positive alignment while ENS, UNI, Bitcoin diverge most. Excluding Bitcoin confirms results are not driven by market dominance.   We interpret findings as weak alignment between whitepaper narratives and market factor structure. Limited power (n=23) precludes distinguishing weak from no alignment, but strong alignment (phi>=0.70) can be confidently rejected. Implications for narrative economics and investment analysis are discussed.",
    "authors": [
      "Murad Farzulla"
    ],
    "published": "2026-01-28",
    "categories": [
      "q-fin.CP",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20336v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20336v1",
    "fetched_at": "2026-01-29T08:46:41.168601",
    "chinese_title": "白皮书声称是否预测市场行为？来自加密货币因子分析的证据",
    "chinese_summary": "该研究构建结合零样本NLP分类（BART-MNLI）与CP张量分解的 pipeline，比较加密货币白皮书声称矩阵、市场统计及潜在因子三个空间；发现白皮书叙事与市场因子结构弱对齐，市场统计与因子对齐显著验证方法有效性，且不同加密货币（如NEAR等正对齐、ENS等偏离大）存在异质性，结果不受比特币主导性驱动。",
    "tags": [
      "LLM",
      "NLP",
      "Factor Model",
      "Factor Mining"
    ],
    "key_contributions": [
      "提出结合零样本NLP（BART-MNLI）与CP张量分解的分析框架，通过市场统计与潜在因子的显著对齐验证方法有效性",
      "揭示加密货币白皮书声称与市场行为/因子结构弱对齐，且不同资产存在异质性（如NEAR等正对齐、ENS等偏离大），结果不受比特币主导性影响"
    ],
    "processed_at": "2026-01-29T08:51:05.688035"
  },
  {
    "id": "2601.20251v1",
    "title": "Efficient Evaluation of LLM Performance with Statistical Guarantees",
    "abstract": "Exhaustively evaluating many large language models (LLMs) on a large suite of benchmarks is expensive. We cast benchmarking as finite-population inference and, under a fixed query budget, seek tight confidence intervals (CIs) for model accuracy with valid frequentist coverage. We propose Factorized Active Querying (FAQ), which (a) leverages historical information through a Bayesian factor model; (b) adaptively selects questions using a hybrid variance-reduction/active-learning sampling policy; and (c) maintains validity through Proactive Active Inference -- a finite-population extension of active inference (Zrnic & Candes, 2024) that enables direct question selection while preserving coverage. With negligible overhead cost, FAQ delivers up to $5\\times$ effective sample size gains over strong baselines on two benchmark suites, across varying historical-data missingness levels: this means that it matches the CI width of uniform sampling while using up to $5\\times$ fewer queries. We release our source code and our curated datasets to support reproducible evaluation and future research.",
    "authors": [
      "Skyler Wu",
      "Yash Nair",
      "Emmanuel J. Candés"
    ],
    "published": "2026-01-28",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20251v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20251v1",
    "fetched_at": "2026-01-29T08:46:47.373172",
    "chinese_title": "带统计保证的大语言模型性能高效评估",
    "chinese_summary": "针对多LLM在大量基准测试上评估成本高昂的问题，本文提出Factorized Active Querying（FAQ）方法，通过贝叶斯因子模型利用历史信息、混合方差减少/主动学习采样策略自适应选问题，并用Proactive Active Inference保证有限总体推断的有效性；实验表明FAQ比强基线有效样本量提升最多5倍，可减少查询数量同时匹配均匀采样的置信区间宽度。",
    "tags": [
      "LLM",
      "Benchmark",
      "Factor Model"
    ],
    "key_contributions": [
      "提出整合贝叶斯因子模型、混合采样策略与Proactive Active Inference的FAQ方法，实现高效且带统计保证的LLM性能评估",
      "实验验证FAQ有效样本量比强基线提升最多5倍，减少查询成本，并发布代码与数据集支持可复现研究"
    ],
    "processed_at": "2026-01-29T08:51:20.728896"
  },
  {
    "id": "2601.20830v1",
    "title": "VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring",
    "abstract": "Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings. VSCOUT combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) architecture with ensemble-based latent outlier filtering and changepoint detection. The ARD prior isolates the most informative latent dimensions, while the ensemble and changepoint filters identify pointwise and structural contamination within the determined latent space. A second-stage retraining step removes flagged observations and re-estimates the latent structure using only the retained inliers, mitigating masking and stabilizing the IC latent manifold. This two-stage refinement produces a clean and reliable IC baseline suitable for subsequent Phase II deployment. Extensive experiments across benchmark datasets demonstrate that VSCOUT achieves superior sensitivity to special-cause structure while maintaining controlled false alarms, outperforming classical SPC procedures, robust estimators, and modern machine-learning baselines. Its scalability, distributional flexibility, and resilience to complex contamination patterns position VSCOUT as a practical and effective method for retrospective modeling and anomaly detection in AI-enabled environments.",
    "authors": [
      "Waldyn G. Martinez"
    ],
    "published": "2026-01-28",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20830v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20830v1",
    "fetched_at": "2026-01-29T08:46:53.583327",
    "chinese_title": "VSCOUT：一种用于高维回顾性监测异常检测的混合变分自编码器方法",
    "chinese_summary": "针对高维非高斯易污染数据对经典统计过程控制（SPC）假设的挑战，提出VSCOUT分布无关框架，结合自动相关性确定变分自编码器（ARD-VAE）、集成潜在异常过滤与变点检测，通过两阶段优化（移除异常后重训练）构建可靠受控基线，实验显示其性能优于经典SPC方法、鲁棒估计器及现代方法。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Time Series",
      "Benchmark"
    ],
    "key_contributions": [
      "提出适配高维非高斯易污染数据回顾性监测的VSCOUT分布无关框架，整合ARD-VAE、集成过滤与变点检测",
      "设计两阶段优化策略（去异常后重训练），稳定受控潜在流形，提升异常检测敏感性与假阳性控制能力"
    ],
    "processed_at": "2026-01-29T08:51:44.701576"
  },
  {
    "id": "2601.20626v1",
    "title": "Trigger Optimization and Event Classification for Dark Matter Searches in the CYGNO Experiment Using Machine Learning",
    "abstract": "The CYGNO experiment employs an optical-readout Time Projection Chamber (TPC) to search for rare low-energy interactions using finely resolved scintillation images. While the optical readout provides rich topological information, it produces large, sparse megapixel images that challenge real-time triggering, data reduction, and background discrimination.   We summarize two complementary machine-learning approaches developed within CYGNO. First, we present a fast and fully unsupervised strategy for online data reduction based on reconstruction-based anomaly detection. A convolutional autoencoder trained exclusively on pedestal images (i.e. frames acquired with GEM amplification disabled) learns the detector noise morphology and highlights particle-induced structures through localized reconstruction residuals, from which compact Regions of Interest (ROIs) are extracted. On real prototype data, the selected configuration retains (93.0 +/- 0.2)% of reconstructed signal intensity while discarding (97.8 +/- 0.1)% of the image area, with ~25 ms per-frame inference time on a consumer GPU.   Second, we report a weakly supervised application of the Classification Without Labels (CWoLa) framework to data acquired with an Americium--Beryllium neutron source. Using only mixed AmBe and standard datasets (no event-level labels), a convolutional classifier learns to identify nuclear-recoil-like topologies. The achieved performance approaches the theoretical limit imposed by the mixture composition and isolates a high-score population with compact, approximately circular morphologies consistent with nuclear recoils.",
    "authors": [
      "F. D. Amaro",
      "R. Antonietti",
      "E. Baracchini",
      "L. Benussi",
      "C. Capoccia",
      "M. Caponero",
      "L. G. M. de Carvalho",
      "G. Cavoto",
      "I. A. Costa",
      "A. Croce",
      "M. D'Astolfo",
      "G. D'Imperio",
      "G. Dho",
      "E. Di Marco",
      "J. M. F. dos Santos",
      "D. Fiorina",
      "F. Iacoangeli",
      "Z. Islam",
      "E. Kemp",
      "H. P. Lima",
      "G. Maccarrone",
      "R. D. P. Mano",
      "D. J. G. Marques",
      "G. Mazzitelli",
      "P. Meloni",
      "A. Messina",
      "C. M. B. Monteiro",
      "R. A. Nobrega",
      "G. M. Oppedisano",
      "I. F. Pains",
      "E. Paoletti",
      "F. Petrucci",
      "S. Piacentini",
      "D. Pierluigi",
      "D. Pinci",
      "F. Renga",
      "A. Russo",
      "G. Saviano",
      "P. A. O. C. Silva",
      "N. J. Spooner",
      "R. Tesauro",
      "S. Tomassini",
      "D. Tozzi"
    ],
    "published": "2026-01-28",
    "categories": [
      "physics.ins-det",
      "cs.LG",
      "physics.data-an"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20626v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20626v1",
    "fetched_at": "2026-01-29T08:46:53.583427",
    "chinese_title": "CYGNO实验中暗物质搜索的触发优化与事件分类：机器学习方法",
    "chinese_summary": "论文针对CYGNO实验光学读出TPC产生的大尺寸稀疏图像挑战，提出两种互补机器学习方法：一是基于卷积自动编码器的无监督在线数据降维策略，通过学习探测器噪声形态提取感兴趣区域，高效保留信号并压缩数据；二是将无标签分类（CWoLa）框架弱监督应用于中子源数据，在无事件级标签下识别核反冲类拓扑结构。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出无监督卷积自动编码器方法，实现高效在线数据降维，保留93.0±0.2%信号同时丢弃97.8±0.1%图像区域，推理时间约25ms/帧"
    ],
    "processed_at": "2026-01-29T08:51:59.540046"
  },
  {
    "id": "2601.20367v1",
    "title": "Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models",
    "abstract": "Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.",
    "authors": [
      "Qing Lyu",
      "Zhe Fu",
      "Alexandre Bayen"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20367v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20367v1",
    "fetched_at": "2026-01-29T08:46:53.583449",
    "chinese_title": "基于Transformer的多智能体轨迹预测无监督异常检测",
    "chinese_summary": "针对自动驾驶安全关键场景稀缺导致监督标注不可行的问题，本文提出基于多智能体Transformer的无监督异常检测框架，通过建模正常驾驶行为并分析预测残差识别异常；设计包含稳定性（Kendall秩相关、Jaccard指数）和物理对齐（与替代安全指标相关性）的双评估方案，在NGSIM数据集上验证有效，发现传统方法遗漏的388个异常并聚类出4种可解释风险类型。",
    "tags": [
      "Anomaly",
      "Transformer",
      "Risk Management",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出基于多智能体Transformer的无监督异常检测框架，解决自动驾驶安全场景标注难问题",
      "设计双评估方案（稳定性+物理对齐），在NGSIM数据集上发现传统方法遗漏的异常并聚类出可解释风险类型"
    ],
    "processed_at": "2026-01-29T08:52:22.604261"
  },
  {
    "id": "2601.20333v1",
    "title": "Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining",
    "abstract": "Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.",
    "authors": [
      "Ali Zia",
      "Usman Ali",
      "Umer Ramzan",
      "Abdul Rehman",
      "Abdelwahed Khamis",
      "Wei Xiang"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20333v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20333v1",
    "fetched_at": "2026-01-29T08:46:53.583475",
    "chinese_title": "基于拓扑感知最优传输链的异常分割测试时自适应方法",
    "chinese_summary": "本文提出TopoOT拓扑感知最优传输框架，整合多过滤持久化图与测试时自适应，核心创新为最优传输链（顺序对齐跨阈值/过滤的持久化图以获得测地线稳定性分数），用稳定性伪标签监督轻量头部在线训练，在2D/3D异常分割基准上实现SOTA性能（2D F1提升最多24.1%，3D最多10.2%）。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "基于稳定性伪标签监督轻量头部在线训练，在2D和3D异常分割基准上显著提升性能至SOTA水平"
    ],
    "processed_at": "2026-01-29T08:52:41.058256"
  },
  {
    "id": "2601.20148v1",
    "title": "LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis",
    "abstract": "Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.   We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.   Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.",
    "authors": [
      "Marcus Emmanuel Barnes",
      "Taher A. Ghaleb",
      "Safwat Hassan"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20148v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20148v1",
    "fetched_at": "2026-01-29T08:46:53.583496",
    "chinese_title": "LogSieve：面向可持续LLM分析的任务感知CI日志精简方法",
    "chinese_summary": "针对CI日志体积大导致分析成本高的问题，论文提出LogSieve——一种轻量、根因感知且语义保留的日志精简技术，过滤低信息行并保留下游推理相关内容；在开源Android项目CI日志上评估，该方法平均减少42%行数和40% tokens，语义损失极小，比基线方法保留更高语义/分类保真度，嵌入分类器实现近97%准确率的相关性检测以支持可扩展集成。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning",
      "Anomaly"
    ],
    "key_contributions": [
      "提出任务感知、语义保留的CI日志精简技术LogSieve，针对非结构化CI日志过滤低信息行并保留推理相关内容（区别于现有结构化日志方法）",
      "实验验证其高效性：平均减少42%行数/40% tokens，语义/分类保真度显著高于基线，嵌入分类器近97%准确率，降低计算与能源成本支持可持续LLM分析"
    ],
    "processed_at": "2026-01-29T08:53:02.213214"
  },
  {
    "id": "2601.20103v1",
    "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
    "abstract": "Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.",
    "authors": [
      "Darshan Deshpande",
      "Anand Kannappan",
      "Rebecca Qian"
    ],
    "published": "2026-01-27",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20103v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20103v1",
    "fetched_at": "2026-01-29T08:46:53.583518",
    "chinese_title": "基于对比分析的代码环境中奖励黑客检测基准研究",
    "chinese_summary": "本文提出覆盖54类的奖励漏洞分类法，构建了包含517条人类验证测试轨迹的TRecE基准；对比孤立分类与对比异常检测设置，发现对比设置下模型检测效果更优（如GPT-5.2达63%，孤立仅45%），且模型对语义上下文化奖励黑客检测更困难，还分析了良性与黑客轨迹比例等影响因素并发布基准工具。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Anomaly",
      "Benchmark"
    ],
    "key_contributions": [
      "提出覆盖54类的奖励漏洞分类法，构建了人类验证的TRecE基准（含517条测试轨迹）",
      "揭示对比异常检测设置下模型奖励黑客检测效果更优，分析语义上下文化漏洞的检测难点并发布基准工具"
    ],
    "processed_at": "2026-01-29T08:53:19.051178"
  },
  {
    "id": "2601.19992v1",
    "title": "BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection",
    "abstract": "Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.",
    "authors": [
      "Soham Sarkar",
      "Tanmay Sen",
      "Sayantan Banerjee"
    ],
    "published": "2026-01-27",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.19992v1",
    "arxiv_url": "https://arxiv.org/abs/2601.19992v1",
    "fetched_at": "2026-01-29T08:46:53.583538",
    "chinese_title": "BayPrAnoMeta：面向少样本工业图像异常检测的贝叶斯原型MAML",
    "chinese_summary": "针对少样本工业图像异常检测中类不平衡与缺陷样本稀缺的挑战，提出BayPrAnoMeta——Proto-MAML的贝叶斯泛化方法，以任务特定概率正态模型替代确定性原型，结合Student-t预测分布实现不确定性感知的重尾异常评分；进一步扩展为联邦元学习框架并证明收敛性，实验在MVTec AD基准上显示其AUROC显著优于现有方法。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "提出BayPrAnoMeta，将Proto-MAML贝叶斯化，引入概率正态模型与Student-t预测分布，实现不确定性感知的少样本工业图像异常检测",
      "扩展为联邦元学习框架并证明收敛性，实验验证其在MVTec AD基准上的性能优势"
    ],
    "processed_at": "2026-01-29T08:53:39.254918"
  },
  {
    "id": "2601.20680v1",
    "title": "Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin",
    "abstract": "Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.",
    "authors": [
      "Ostap Vykhopen",
      "Viktoria Skorik",
      "Maxim Tereschenko",
      "Veronika Solopova"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20680v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20680v1",
    "fetched_at": "2026-01-29T08:47:12.154203",
    "chinese_title": "用于实时叙事演化监测的在线密度聚类方法",
    "chinese_summary": "本文针对社交媒体监测中传统批量聚类（如HDBSCAN）的可扩展性不足问题，提出用在线聚类方法替代以实现实时叙事演化监测；构建三阶段架构处理每日数千多语言文档，评估多种在线聚类算法的聚类质量、效率等指标，填补了批量与实时处理的研究空白。",
    "tags": [
      "NLP",
      "Time Series",
      "Behavioral Finance"
    ],
    "key_contributions": [
      "提出在线密度聚类方法替代传统批量聚类（HDBSCAN），解决实时叙事演化监测的可扩展性与实时性问题",
      "构建三阶段架构并设计兼顾传统聚类指标与叙事特性的评估体系，通过历史数据仿真验证算法 trade-offs"
    ],
    "processed_at": "2026-01-29T08:54:05.766481"
  },
  {
    "id": "2601.20852v1",
    "title": "C3Box: A CLIP-based Class-Incremental Learning Toolbox",
    "abstract": "Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.",
    "authors": [
      "Hao Sun",
      "Da-Wei Zhou"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20852v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20852v1",
    "fetched_at": "2026-01-29T08:47:21.468087",
    "chinese_title": "C3Box：基于CLIP的类增量学习工具箱",
    "chinese_summary": "针对现有基于CLIP的类增量学习方法分散、配置不一致导致难以公平对比和复现的问题，论文提出模块化综合Python工具箱C3Box；该工具箱整合传统、ViT-based及SOTA CLIP-based类增量学习方法，基于PyCIL提供JSON配置与标准化流程，支持可复现实验与主流系统。",
    "tags": [
      "Transformer",
      "Benchmark",
      "Deep Learning"
    ],
    "key_contributions": [
      "设计JSON配置与标准化流程，降低工程 overhead，成为类增量学习可靠基准平台"
    ],
    "processed_at": "2026-01-29T08:54:22.745968"
  },
  {
    "id": "2601.20784v1",
    "title": "REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence",
    "abstract": "Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.   This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.",
    "authors": [
      "Zishen Wan",
      "Che-Kai Liu",
      "Jiayi Qian",
      "Hanchen Yang",
      "Arijit Raychowdhury",
      "Tushar Krishna"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20784v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20784v1",
    "fetched_at": "2026-01-29T08:47:21.468123",
    "chinese_title": "REASON：加速可扩展神经符号智能的概率逻辑推理",
    "chinese_summary": "本文针对神经符号AI中概率逻辑推理的低效瓶颈（CPU/GPU利用率低、算术强度不足等），提出REASON加速框架——通过统一有向无环图（DAG）表示结合自适应剪枝与正则化，架构上采用可重构树状处理结构，系统上与GPU流式多处理器（SM）集成多级流水线；在6类神经符号工作负载上实现12-50倍加速及310-681倍能效提升。",
    "tags": [
      "Deep Learning",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "系统分析神经符号工作负载，识别概率逻辑推理为低效瓶颈（控制流不规则、算术强度低等，CPU/GPU利用率差）",
      "提出REASON集成加速框架，含统一DAG表示+自适应剪枝正则化、可重构树状处理架构、GPU SM集成多级流水线，实现显著性能与能效提升"
    ],
    "processed_at": "2026-01-29T08:54:52.450900"
  },
  {
    "id": "2601.20782v1",
    "title": "Neural Quantum States in Mixed Precision",
    "abstract": "Scientific computing has long relied on double precision (64-bit floating point) arithmetic to guarantee accuracy in simulations of real-world phenomena. However, the growing availability of hardware accelerators such as Graphics Processing Units (GPUs) has made low-precision formats attractive due to their superior performance, reduced memory footprint, and improved energy efficiency. In this work, we investigate the role of mixed-precision arithmetic in neural-network based Variational Monte Carlo (VMC), a widely used method for solving computationally otherwise intractable quantum many-body systems. We first derive general analytical bounds on the error introduced by reduced precision on Metropolis-Hastings MCMC, and then empirically validate these bounds on the use-case of VMC. We demonstrate that significant portions of the algorithm, in particular, sampling the quantum state, can be executed in half precision without loss of accuracy. More broadly, this work provides a theoretical framework to assess the applicability of mixed-precision arithmetic in machine-learning approaches that rely on MCMC sampling. In the context of VMC, we additionally demonstrate the practical effectiveness of mixed-precision strategies, enabling more scalable and energy-efficient simulations of quantum many-body systems.",
    "authors": [
      "Massimo Solinas",
      "Agnes Valenti",
      "Nawaf Bou-Rabee",
      "Roeland Wiersema"
    ],
    "published": "2026-01-28",
    "categories": [
      "quant-ph",
      "cond-mat.str-el",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20782v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20782v1",
    "fetched_at": "2026-01-29T08:47:21.468148",
    "chinese_title": "",
    "chinese_summary": "",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [],
    "processed_at": "2026-01-29T09:01:47.258807"
  },
  {
    "id": "2601.20613v1",
    "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
    "abstract": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.",
    "authors": [
      "Kaiyuan Chen",
      "Qimin Wu",
      "Taiyu Hou",
      "Tianhao Tang",
      "Xueyu Hu",
      "Yuchen Hou",
      "Bikun Li",
      "Chengming Qian",
      "Guoyin Wang",
      "Haolin Chen",
      "Haotong Tian",
      "Haoye Zhang",
      "Haoyu Bian",
      "Hongbing Pan",
      "Hongkang Zhang",
      "Hongyi Zhou",
      "Jiaqi Cai",
      "Jiewu Rao",
      "Jiyuan Ren",
      "Keduan Huang",
      "Lucia Zhu Huang",
      "Mingyu Yuan",
      "Naixu Guo",
      "Qicheng Tang",
      "Qinyan Zhang",
      "Shuai Chen",
      "Siheng Chen",
      "Ting Ting Li",
      "Xiaoxing Guo",
      "Yaocheng Zuo",
      "Yaoqi Guo",
      "Yinan Wang",
      "Yinzhou Yu",
      "Yize Wang",
      "Yuan Jiang",
      "Yuan Tian",
      "Yuanshuo Zhang",
      "Yuxuan Liu",
      "Yvette Yan Zeng",
      "Zenyu Shan",
      "Zihan Yin",
      "Xiaobo Hu",
      "Yang Liu",
      "Yixin Ren",
      "Yuan Gong"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20613v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20613v1",
    "fetched_at": "2026-01-29T08:47:21.468244",
    "chinese_title": "AgentIF-OneDay：面向日常场景通用AI智能体的任务级指令遵循基准",
    "chinese_summary": "针对现有AI智能体评估未充分覆盖日常任务多样性的问题，论文提出AgentIF-OneDay基准，涵盖开放工作流执行、隐式指令推理、迭代优化三类日常任务；构建实例级评分标准结合Gemini-3-Pro的LLM验证 pipeline，实现与人类判断80.1%的一致性，包含104个任务及767个评分点。",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark"
    ],
    "key_contributions": [
      "提出AgentIF-OneDay基准，弥补现有评估日常任务多样性不足的缺陷，覆盖三类用户中心日常任务",
      "构建实例级评分标准与LLM验证结合的评估 pipeline，实现较高的人类判断一致性"
    ],
    "processed_at": "2026-01-29T09:02:03.449380"
  },
  {
    "id": "2601.20480v1",
    "title": "An explainable framework for the relationship between dementia and glucose metabolism patterns",
    "abstract": "High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.",
    "authors": [
      "C. Vázquez-García",
      "F. J. Martínez-Murcia",
      "F. Segovia Román",
      "A. Forte",
      "J. Ramírez",
      "I. Illán",
      "A. Hernández-Segura",
      "C. Jiménez-Mesa",
      "Juan M. Górriz"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20480v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20480v1",
    "fetched_at": "2026-01-29T08:47:21.468276",
    "chinese_title": "痴呆与葡萄糖代谢模式关系的可解释框架",
    "chinese_summary": "本文提出含灵活相似性正则项的半监督变分自动编码器（VAE）框架，可对齐潜在变量与痴呆进展的临床/生物标志物；利用阿尔茨海默病神经影像倡议（ADNI）的PET扫描数据验证，发现关键脑区及静息态网络代谢降低，有效提取与阿尔茨海默病生物标志物一致的疾病相关模式。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出灵活可适配的半监督VAE框架，实现潜在变量与痴呆进展标志物的对齐",
      "从PET扫描数据中提取与阿尔茨海默病生物标志物一致的疾病相关模式，提供可解释的研究工具"
    ],
    "processed_at": "2026-01-29T09:02:22.446914"
  },
  {
    "id": "2601.20439v1",
    "title": "PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use",
    "abstract": "Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \\textbf{56.5\\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.",
    "authors": [
      "Qihao Wang",
      "Mingzhe Lu",
      "Jiayue Wu",
      "Yue Hu",
      "Yanbing Liu"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20439v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20439v1",
    "fetched_at": "2026-01-29T08:47:21.468304",
    "chinese_title": "PEARL：用于多跳工具使用的规划探索与自适应强化学习",
    "chinese_summary": "针对大语言模型（LLM）在复杂多轮工具调用中规划能力弱、工具幻觉、参数错误等问题，提出PEARL框架，包含离线探索工具使用模式与失败条件、在线采用带区分规划质量奖励的群体相对策略优化（GRPO）训练Planner两个阶段；在ToolHop和T-Eval基准上显著优于现有方法，ToolHop成功率达56.5%（新SOTA）且保持低调用错误率。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Benchmark",
      "NLP"
    ],
    "key_contributions": [
      "提出两阶段PEARL框架，离线探索工具模式与失败条件，在线用带规划质量信号的GRPO训练Planner，解决LLM多轮工具调用核心问题",
      "在ToolHop和T-Eval基准上刷新性能，ToolHop成功率达56.5%且低调用错误率，提升工具使用可靠性"
    ],
    "processed_at": "2026-01-29T09:02:40.184150"
  },
  {
    "id": "2601.20408v1",
    "title": "Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT",
    "abstract": "Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.   We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.   In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.",
    "authors": [
      "Nicholas Santavas",
      "Kareem Eissa",
      "Patrycja Cieplicka",
      "Piotr Florek",
      "Matteo Nulli",
      "Stefan Vasilev",
      "Seyyed Hadi Hashemi",
      "Antonios Gasteratos",
      "Shahram Khadivi"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20408v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20408v1",
    "fetched_at": "2026-01-29T08:47:21.468336",
    "chinese_title": "满足服务等级目标、减少工时：基于OptiKIT的企业LLM自动化优化",
    "chinese_summary": "针对企业LLM部署面临的可扩展性挑战（需专业优化但相关 expertise 稀缺），论文提出分布式优化框架OptiKIT，通过自动化复杂工作流（含动态资源分配、分阶段流水线执行等），使非专家团队能高效优化模型，生产中GPU吞吐量提升超2倍且系统已开源。",
    "tags": [
      "LLM",
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "提出分布式LLM优化框架OptiKIT，自动化复杂优化工作流， democratize 模型压缩与调优给非专家团队",
      "生产中实现超2倍GPU吞吐量提升，开源系统促进外部贡献与可复现"
    ],
    "processed_at": "2026-01-29T09:02:57.325492"
  },
  {
    "id": "2601.20404v1",
    "title": "On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents",
    "abstract": "AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS.md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS.md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS.md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.",
    "authors": [
      "Jai Lal Lulla",
      "Seyedmoein Mohsenimofidi",
      "Matthias Galster",
      "Jie M. Zhang",
      "Sebastian Baltes",
      "Christoph Treude"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20404v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20404v1",
    "fetched_at": "2026-01-29T08:47:21.468363",
    "chinese_title": "AGENTS.md文件对AI编码代理效率的影响研究",
    "chinese_summary": "本文以Codex、Claude Code等AI编码代理为对象，分析10个GitHub仓库的124个拉取请求，对比有无AGENTS.md时代理的运行时间与token消耗，发现AGENTS.md可降低中位数运行时间28.64%、输出token消耗16.58%且任务完成行为相当；同时讨论了该结果对AI编码代理配置部署的实践启示及相关研究方向。",
    "tags": [
      "LLM",
      "Deep Learning"
    ],
    "key_contributions": [
      "填补仓库级配置 artifacts（AGENTS.md）对AI编码代理效率影响的研究空白，量化证明其可显著降低运行时间与token消耗且不影响任务完成质量",
      "提出AI编码代理配置部署的实践建议，并 outline 仓库级指令对代理行为、效率及集成影响的 broader 研究议程"
    ],
    "processed_at": "2026-01-29T09:03:22.483800"
  },
  {
    "id": "2601.20380v1",
    "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
    "abstract": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.",
    "authors": [
      "Le Zhang",
      "Yixiong Xiao",
      "Xinjiang Lu",
      "Jingjia Cao",
      "Yusai Zhao",
      "Jingbo Zhou",
      "Lang An",
      "Zikan Feng",
      "Wanxiang Sha",
      "Yu Shi",
      "Congxi Xiao",
      "Jian Xiong",
      "Yankai Zhang",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20380v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20380v1",
    "fetched_at": "2026-01-29T08:47:21.468403",
    "chinese_title": "OmegaUse：构建用于自主任务执行的通用GUI代理",
    "chinese_summary": "本文提出通用GUI代理OmegaUse，支持移动与桌面平台自主任务执行；方法上设计含开源数据与自动化合成框架的数据构建流程，采用SFT+GRPO两阶段训练并基于MoE backbone平衡效率与推理能力，同时引入OS-Nav基准评估跨终端性能。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "提出跨移动/桌面平台的通用GUI代理OmegaUse，实现自主任务执行",
      "构建含自动化合成框架的数据构建流程与两阶段训练范式，提升代理交互与规划能力"
    ],
    "processed_at": "2026-01-29T09:03:39.319699"
  },
  {
    "id": "2601.20379v1",
    "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution",
    "abstract": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of \"conjectures and refutations,\" we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.",
    "authors": [
      "Zhengbo Jiao",
      "Hongyu Xian",
      "Qinglong Wang",
      "Yunpu Ma",
      "Zhebo Wang",
      "Zifan Zhang",
      "Dezhang Kong",
      "Meng Han"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20379v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20379v1",
    "fetched_at": "2026-01-29T08:47:21.468432",
    "chinese_title": "",
    "chinese_summary": "",
    "tags": [
      "LLM"
    ],
    "key_contributions": [],
    "processed_at": "2026-01-29T09:03:57.895858"
  },
  {
    "id": "2601.20335v1",
    "title": "MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment",
    "abstract": "Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.",
    "authors": [
      "Qinzhuo Wu",
      "Zhizhuo Yang",
      "Hanhao Li",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20335v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20335v1",
    "fetched_at": "2026-01-29T08:47:21.468457",
    "chinese_title": "MobileBench-OL：面向真实环境中移动GUI智能体的中文综合基准",
    "chinese_summary": "论文针对现有移动GUI智能体在线基准忽略推理探索能力、未考虑真实环境随机噪声的问题，提出MobileBench-OL在线基准，包含80个中文应用的1080个任务及5个子集，覆盖任务执行、复杂推理和噪声鲁棒性等维度；同时提供带重置机制的自动评估框架，通过对12个领先智能体的评估验证了该基准的可靠性。",
    "tags": [
      "Benchmark"
    ],
    "key_contributions": [
      "提出MobileBench-OL在线基准，包含多维度任务以覆盖真实移动环境中GUI智能体的核心能力需求",
      "提供带重置机制的自动评估框架，实现稳定可重复的真实环境基准测试"
    ],
    "processed_at": "2026-01-29T09:04:08.843475"
  },
  {
    "id": "2601.20162v1",
    "title": "Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction",
    "abstract": "Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.",
    "authors": [
      "Shuoxin Wang",
      "Chang Liu",
      "Gowen Loo",
      "Lifan Zheng",
      "Kaiwen Wei",
      "Xinyi Zeng",
      "Jingyuan Zhang",
      "Yu Tian"
    ],
    "published": "2026-01-28",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20162v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20162v1",
    "fetched_at": "2026-01-29T08:47:21.468485",
    "chinese_title": "Me-Agent：一种具备两级用户习惯学习以增强交互的个性化移动Agent",
    "chinese_summary": "针对现有LLM移动Agent忽略个性化需求（如无法解释模糊指令、缺乏历史学习等）的问题，提出Me-Agent，采用两级用户习惯学习（提示层结合个人奖励模型的偏好学习、记忆层分层存储长期及应用特定记忆）；引入新基准User FingerTip验证个性化能力，实验表明其个性化与指令执行性能均达SOTA。",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "提出具备两级用户习惯学习的Me-Agent，解决LLM移动Agent个性化不足问题",
      "引入含大量日常模糊指令的新基准User FingerTip，验证个性化能力并证明Me-Agent性能优势"
    ],
    "processed_at": "2026-01-29T09:04:27.985663"
  },
  {
    "id": "2601.20090v1",
    "title": "Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control",
    "abstract": "Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.",
    "authors": [
      "Amirmohammad Farzaneh",
      "Salvatore D'Oro",
      "Osvaldo Simeone"
    ],
    "published": "2026-01-27",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20090v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20090v1",
    "fetched_at": "2026-01-29T08:47:21.468505",
    "chinese_title": "我是否应该换一种意图表达？基于大语言模型的自主控制的反事实生成",
    "chinese_summary": "本文针对用户想了解换意图表达后的反事实结果问题，提出保形反事实生成（CCG）框架，用结构因果模型建模用户、LLM代理与环境的闭环交互，通过概率溯因和离线校准生成高概率包含真实反事实结果的候选集，在无线网络控制场景中优于naive重执行基线。",
    "tags": [
      "LLM",
      "Transformer",
      "NLP",
      "Financial Agent"
    ],
    "key_contributions": [
      "提出保形反事实生成（CCG）框架，为LLM驱动的自主控制场景提供形式化可靠性保证的反事实推理能力",
      "基于结构因果模型（SCM）建模闭环交互，通过概率溯因与离线校准生成高概率包含真实反事实结果的候选集，在无线控制用例中优于基线方法"
    ],
    "processed_at": "2026-01-29T09:04:51.623127"
  },
  {
    "id": "2601.20048v1",
    "title": "Insight Agents: An LLM-Based Multi-Agent System for Data Insights",
    "abstract": "Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.",
    "authors": [
      "Jincheng Bai",
      "Zhenyu Zhang",
      "Jennifer Zhang",
      "Zhihuai Zhu"
    ],
    "published": "2026-01-27",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20048v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20048v1",
    "fetched_at": "2026-01-29T08:47:21.468527",
    "chinese_title": "Insight Agents：基于大语言模型的多智能体数据洞察系统",
    "chinese_summary": "针对电商卖家工具利用与数据洞察的挑战，论文提出基于大语言模型的多智能体系统Insight Agents（IA），采用计划-执行范式，包含经理智能体及数据呈现、洞察生成两个工作智能体；经理智能体结合轻量编码器-解码器的域外检测与BERT路由优化准确率与延迟，工作智能体通过分解API查询并动态注入领域知识提升响应准确性，助力卖家高效决策。",
    "tags": [
      "LLM",
      "NLP",
      "Financial Agent",
      "Transformer"
    ],
    "key_contributions": [
      "提出基于大语言模型的多智能体系统Insight Agents，采用计划-执行范式为电商卖家提供个性化数据与业务洞察",
      "设计经理智能体的高效模块（轻量编码器-解码器的域外检测+BERT路由）及工作智能体的战略规划与领域知识注入，优化系统准确率、延迟与响应准确性"
    ],
    "processed_at": "2026-01-29T09:05:14.397477"
  },
  {
    "id": "2601.20021v1",
    "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
    "abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.",
    "authors": [
      "Shuhui Qu"
    ],
    "published": "2026-01-27",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.20021v1",
    "arxiv_url": "https://arxiv.org/abs/2601.20021v1",
    "fetched_at": "2026-01-29T08:47:21.468544",
    "chinese_title": "模糊范畴规划：带分级语义约束的自主目标满足",
    "chinese_summary": "现有范畴规划器因将动作适用度视为非模糊（crisp），无法处理自然语言规划中模糊谓词的分级满足问题，且强制阈值会丢失关键区分、无法跟踪多步计划质量退化；本文提出模糊范畴规划（FCP），用[0,1]标注动作、通过Lukasiewicz t-范数合成计划质量，结合LLM与k样本中位数聚合处理语言中的分级适用度，支持基于剩余的双向搜索；在PDDL3基准和RecipeNLG-Subs（缺失替代食谱规划）上评估，FCP比LLM-only和ReAct基线提升成功率、减少硬约束违反，与经典PDDL3相当。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "提出模糊范畴规划（FCP），首次将分级语义约束引入范畴规划框架，解决模糊谓词分级满足问题",
      "设计LLM结合k样本中位数聚合的语言分级适用度处理方法，支持双向搜索，在食谱替代规划基准上优于传统LLM基线"
    ],
    "processed_at": "2026-01-29T09:05:44.556075"
  }
]