[
  {
    "id": "2511.21556v1",
    "title": "Informative Risk Measuresin the Banking Industry: A Proposal based on the Magnitude-Propensity Approach",
    "abstract": "Despite decades of research in risk management, most of the literature has focused on scalar risk measures (like e.g. Value-at-Risk and Expected Shortfall). While such scalar measures provide compact and tractable summaries, they provide a poor informative value as they miss the intrinsic multivariate nature of risk.To contribute to a paradigmatic enhancement, and building on recent theoretical work by Faugeras and Pagés (2024), we propose a novel multivariate representation of risk that better reflects the structure of potential portfolio losses, while maintaining desirable properties of interpretability and analytical coherence. The proposed framework extends the classical frequency-severity approach and provides a more comprehensive characterization of extreme events. Several empirical applications based on real-world data demonstrate the feasibility, robustness and practical relevance of the methodology, suggesting its potential for both regulatory and managerial applications.",
    "authors": [
      "Michele Bonollo",
      "Martino Grasselli",
      "Gianmarco Mori",
      "Havva Nilsu Oz"
    ],
    "published": "2025-11-26",
    "categories": [
      "q-fin.RM",
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21556v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21556v1",
    "fetched_at": "2025-11-28T11:05:14.085577",
    "chinese_title": "银行业中的信息性风险度量：基于量级-倾向方法的提案",
    "chinese_summary": "现有标量风险度量（如VaR、ES）因忽略风险多变量本质而信息价值不足，论文基于Faugeras和Pagés(2024)的理论工作，提出新的多变量风险表示，扩展经典频率-严重度方法以更好反映投资组合损失结构，保持可解释性与分析一致性，实证验证其在银行业监管与管理中的可行性、稳健性及实践价值。",
    "tags": [
      "Risk Management"
    ],
    "key_contributions": [
      "扩展频率-严重度方法刻画极端事件结构，实证验证银行业应用的可行性与价值"
    ],
    "processed_at": "2025-11-28T11:33:45.224924"
  },
  {
    "id": "2511.21515v1",
    "title": "The Quantum Network of Assets: A Non-Classical Framework for Market Correlation and Structural Risk",
    "abstract": "Classical correlation matrices capture only linear and pairwise co-movements, leaving higher-order, nonlinear, and state-dependent interactions of financial markets unrepresented. This paper introduces the Quantum Network of Assets (QNA), a density-matrix based framework that embeds cross-asset dependencies into a quantum-information representation. The approach does not assume physical quantum effects but uses the mathematical structure of density operators, entropy, and mutual information to describe market organisation at a structural level.   Within this framework we define two structural measures: the Entanglement Risk Index (ERI), which summarises global non-separability and the compression of effective market degrees of freedom, and the Quantum Early-Warning Signal (QEWS), which tracks changes in entropy to detect latent information build-up. These measures reveal dependency geometry that classical covariance-based tools cannot capture.   Using NASDAQ-100 data from 2024-2025, we show that quantum entropy displays smoother evolution and clearer regime distinctions than classical entropy, and that ERI rises during periods of structural tightening even when volatility remains low. Around the 2025 US tariff announcement, QEWS shows a marked pre-event increase in structural tension followed by a sharp collapse after the announcement, indicating that structural transitions can precede price movements without implying predictive modelling.   QNA therefore provides a structural diagnostic of market fragility, regime shifts, and latent information flow. The framework suggests new directions for systemic risk research by linking empirical asset networks with tools from quantum information theory.",
    "authors": [
      "Hui Gong",
      "Akash Sharma",
      "Francesca Medda"
    ],
    "published": "2025-11-26",
    "categories": [
      "q-fin.RM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21515v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21515v1",
    "fetched_at": "2025-11-28T11:05:14.085598",
    "chinese_title": "资产量子网络：市场相关性与结构风险的非经典框架",
    "chinese_summary": "该论文针对经典相关矩阵无法捕捉高阶非线性及状态依赖交互的问题，引入基于密度矩阵的资产量子网络（QNA）框架，利用量子信息数学结构描述市场组织；定义纠缠风险指数（ERI）和量子预警信号（QEWS）两类结构指标，通过NASDAQ-100数据验证其在市场结构分析与风险预警中的优势，如ERI在结构收紧时上升、QEWS可提前显示事件前的结构张力变化。",
    "tags": [
      "Risk Management",
      "Anomaly",
      "Volatility"
    ],
    "key_contributions": [
      "引入资产量子网络（QNA）框架，基于密度矩阵的量子信息数学结构，突破经典相关矩阵局限，刻画高阶非线性及状态依赖的市场依赖关系",
      "定义纠缠风险指数（ERI）和量子预警信号（QEWS）两类结构指标，实证验证其在市场结构分析与风险预警中的独特价值"
    ],
    "processed_at": "2025-11-28T11:34:01.730257"
  },
  {
    "id": "2511.21287v1",
    "title": "Dynamic characterization of barycentric optimal transport problems and their martingale relaxation",
    "abstract": "We extend the Benamou-Brenier formula from classical optimal transport to weak optimal transport and show that the barycentric optimal transport problem studied by Gozlan and Juillet has a dynamic analogue. We also investigate a martingale relaxation of this problem, and relate it to the martingale Benamou-Brenier formula of Backhoff-Veraguas, Beiglböck, Huesmann and Källblad.",
    "authors": [
      "Ivan Guo",
      "Severin Nilsson",
      "Johannes Wiesel"
    ],
    "published": "2025-11-26",
    "categories": [
      "math.PR",
      "math.OC",
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21287v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21287v1",
    "fetched_at": "2025-11-28T11:05:14.085609",
    "chinese_title": "重心最优传输问题的动态刻画及其鞅松弛",
    "chinese_summary": "本文将经典最优传输的Benamou-Brenier公式扩展至弱最优传输，得到重心最优传输问题的动态刻画；同时研究该问题的鞅松弛，建立其与鞅Benamou-Brenier公式的关联。",
    "tags": [
      "Asset Pricing",
      "Risk Management"
    ],
    "key_contributions": [
      "将经典最优传输的Benamou-Brenier公式扩展至弱最优传输，获得重心最优传输问题的动态类似物",
      "研究重心最优传输问题的鞅松弛，关联其与鞅Benamou-Brenier公式"
    ],
    "processed_at": "2025-11-28T11:34:21.220723"
  },
  {
    "id": "2511.21221v1",
    "title": "Portfolio Optimization via Transfer Learning",
    "abstract": "Recognizing that asset markets generally exhibit shared informational characteristics, we develop a portfolio strategy based on transfer learning that leverages cross-market information to enhance the investment performance in the market of interest by forward validation. Our strategy asymptotically identifies and utilizes the informative datasets, selectively incorporating valid information while discarding the misleading information. This enables our strategy to achieve the maximum Sharpe ratio asymptotically. The promising performance is demonstrated by numerical studies and case studies of two portfolios: one consisting of stocks dual-listed in A-shares and H-shares, and another comprising equities from various industries of the United States.",
    "authors": [
      "Kexin Wang",
      "Xiaomeng Zhang",
      "Xinyu Zhang"
    ],
    "published": "2025-11-26",
    "categories": [
      "q-fin.PM",
      "stat.AP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21221v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21221v1",
    "fetched_at": "2025-11-28T11:05:14.085619",
    "chinese_title": "基于迁移学习的投资组合优化",
    "chinese_summary": "论文针对资产市场存在共享信息特征的特点，提出基于迁移学习的投资组合策略，通过前向验证利用跨市场信息提升目标市场投资表现；该策略可渐近识别有效数据集，选择性纳入有效信息并丢弃误导信息，渐近实现最大夏普比率；数值与案例研究（A股H股双重上市股票、美国各行业股票）验证了其良好性能。",
    "tags": [
      "Portfolio Optimization",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出基于迁移学习的投资组合优化策略，利用资产市场共享信息特征，通过前向验证选择性纳入跨市场有效信息以提升目标市场表现",
      "证明策略可渐近实现最大夏普比率，并通过数值实验及A股H股双重上市股票、美国各行业股票的案例验证了其性能"
    ],
    "processed_at": "2025-11-28T11:34:38.388111"
  },
  {
    "id": "2511.20837v1",
    "title": "Constrained deep learning for pricing and hedging european options in incomplete markets",
    "abstract": "In incomplete financial markets, pricing and hedging European options lack a unique no-arbitrage solution due to unhedgeable risks. This paper introduces a constrained deep learning approach to determine option prices and hedging strategies that minimize the Profit and Loss (P&L) distribution around zero. We employ a single neural network to represent the option price function, with its gradient serving as the hedging strategy, optimized via a loss function enforcing the self-financing portfolio condition. A key challenge arises from the non-smooth nature of option payoffs (e.g., vanilla calls are non-differentiable at-the-money, while digital options are discontinuous), which conflicts with the inherent smoothness of standard neural networks. To address this, we compare unconstrained networks against constrained architectures that explicitly embed the terminal payoff condition, drawing inspiration from PDE-solving techniques. Our framework assumes two tradable assets: the underlying and a liquid call option capturing volatility dynamics. Numerical experiments evaluate the method on simple options with varying non-smoothness, the exotic Equinox option, and scenarios with market jumps for robustness. Results demonstrate superior P&L distributions, highlighting the efficacy of constrained networks in handling realistic payoffs. This work advances machine learning applications in quantitative finance by integrating boundary constraints, offering a practical tool for pricing and hedging in incomplete markets.",
    "authors": [
      "Nicolas Baradel"
    ],
    "published": "2025-11-25",
    "categories": [
      "q-fin.CP",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20837v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20837v1",
    "fetched_at": "2025-11-28T11:05:14.085628",
    "chinese_title": "不完全市场中欧式期权定价与对冲的约束深度学习方法",
    "chinese_summary": "针对不完全市场中欧式期权定价与对冲无唯一无套利解的问题，论文提出约束深度学习方法，用单个神经网络表示期权价格函数（梯度为对冲策略），通过损失函数约束自融资条件；为解决期权收益非光滑与神经网络光滑性冲突，设计嵌入终端收益约束的架构，实验验证其在不同非光滑期权及跳跃场景下P&L分布更优，整合边界约束提升了机器学习在量化金融中的应用。",
    "tags": [
      "Deep Learning",
      "Options",
      "Asset Pricing",
      "Volatility"
    ],
    "key_contributions": [
      "提出约束深度学习框架，用单个神经网络同时表示期权价格与对冲策略，通过自融资条件优化解决不完全市场定价对冲问题",
      "针对期权收益非光滑性设计嵌入终端收益约束的网络架构，提升P&L分布性能，适用于多种非光滑期权及跳跃场景"
    ],
    "processed_at": "2025-11-28T11:34:50.119356"
  },
  {
    "id": "2511.20606v2",
    "title": "Limit Order Book Dynamics in Matching Markets: Microstructure, Spread, and Execution Slippage",
    "abstract": "Conventional models of matching markets assume that monetary transfers can clear markets by compensating for utility differentials. However, empirical patterns show that such transfers often fail to close structural preference gaps. This paper introduces a market microstructure framework that models matching decisions as a limit order book system with rigid bid ask spreads. Individual preferences are represented by a latent preference state matrix, where the spread between an agent's internal ask price (the unconditional maximum) and the market's best bid (the reachable maximum) creates a structural liquidity constraint. We establish a Threshold Impossibility Theorem showing that linear compensation cannot close these spreads unless it induces a categorical identity shift. A dynamic discrete choice execution model further demonstrates that matches occur only when the market to book ratio crosses a time decaying liquidity threshold, analogous to order execution under inventory pressure. Numerical experiments validate persistent slippage, regional invariance of preference orderings, and high tier zero spread executions. The model provides a unified microstructure explanation for matching failures, compensation inefficiency, and post match regret in illiquid order driven environments.",
    "authors": [
      "Yao Wu"
    ],
    "published": "2025-11-25",
    "categories": [
      "q-fin.TR",
      "cs.MA",
      "cs.SI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20606v2",
    "arxiv_url": "https://arxiv.org/abs/2511.20606v2",
    "fetched_at": "2025-11-28T11:05:14.085636",
    "chinese_title": "匹配市场中的限价订单簿动态：微观结构、价差与执行滑点",
    "chinese_summary": "本文针对传统匹配市场模型假设货币转移可弥补效用差异但实证存在结构性偏好缺口的问题，引入基于刚性买卖价差的限价订单簿系统微观结构框架，用潜在偏好状态矩阵刻画个体偏好；建立阈值不可能定理并提出动态离散选择执行模型，数值实验验证持续滑点等现象，统一解释匹配失败、补偿低效等问题。",
    "tags": [
      "Market Microstructure",
      "Execution",
      "Behavioral Finance"
    ],
    "key_contributions": [
      "构建基于刚性价差的限价订单簿微观结构框架，用潜在偏好矩阵刻画个体偏好，证明线性补偿无法填补结构性偏好缺口的阈值不可能定理",
      "提出动态离散选择执行模型，通过数值实验验证持续滑点等现象，统一解释匹配失败、补偿低效等匹配市场问题"
    ],
    "processed_at": "2025-11-28T11:35:12.277170"
  },
  {
    "id": "2511.19826v1",
    "title": "Efficient Importance Sampling under Heston Model: Short Maturity and Deep Out-of-the-Money Options",
    "abstract": "This paper investigates asymptotically optimal importance sampling (IS) schemes for pricing European call options under the Heston stochastic volatility model. We focus on two distinct rare-event regimes where standard Monte Carlo methods suffer from significant variance deterioration: the limit as maturity approaches zero and the limit as the strike price tends to infinity. Leveraging the large deviation principle (LDP), we design a state-dependent change of measure derived from the asymptotic behavior of the log-price cumulant generating functions. In the short-maturity regime, we rigorously prove that our proposed IS drift, inspired by the variational characterization of the rate function, achieves logarithmic efficiency (asymptotic optimality) by minimizing the decay rate of the second moment of the estimator. In the deep OTM regime, we introduce a novel slow mean-reversion scaling for the variance process, where the mean-reversion speed scales as the inverse square of the small-noise parameter (defined as the reciprocal of the log-moneyness). We establish that under this specific scaling, the variance process contributes non-trivially to the large deviation rate function, requiring a specialized Riccati analysis to verify optimality. Numerical experiments demonstrate that the proposed method yields substantial variance reduction--characterized by factors exceeding several orders of magnitude--compared to standard estimators in both asymptotic regimes.",
    "authors": [
      "Yun-Feng Tu",
      "Chuan-Hsiang Han"
    ],
    "published": "2025-11-25",
    "categories": [
      "q-fin.MF",
      "math.PR",
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19826v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19826v1",
    "fetched_at": "2025-11-28T11:05:14.085644",
    "chinese_title": "Heston模型下的高效重要性采样：短到期与深度虚值期权",
    "chinese_summary": "论文针对Heston模型下短到期和深度虚值这两个稀有事件场景，基于大偏差原理设计状态依赖的测度变换；短到期 regime证明所提重要性采样漂移达对数效率（渐近最优），深度虚值 regime引入慢均值回复 scaling并通过Riccati分析验证最优性，数值实验显示方差降低超几个数量级。",
    "tags": [
      "Volatility",
      "Options",
      "Asset Pricing",
      "Benchmark"
    ],
    "key_contributions": [
      "针对Heston模型短到期和深度虚值期权，基于大偏差原理设计状态依赖的重要性采样方案，短到期 regime证明渐近最优（对数效率）",
      "深度虚值 regime引入慢均值回复 scaling，通过Riccati分析验证最优性，数值实验显示方差大幅降低（超几个数量级）"
    ],
    "processed_at": "2025-11-28T11:35:27.330010"
  },
  {
    "id": "2511.19701v1",
    "title": "Optimal dividend and capital injection under self-exciting claims",
    "abstract": "In this paper, we study an optimal dividend and capital-injection problem in a Cramér--Lundberg model where claim arrivals follow a Hawkes process, capturing clustering effects often observed in insurance portfolios. We establish key analytical properties of the value function and characterise the optimal capital-injection strategy through an explicit threshold. We also show that the value function is the unique viscosity solution of the associated HJB variational inequality. For numerical purposes, we first compute a benchmark solution via a monotone finite-difference scheme with Howard's policy iteration. We then develop a reinforcement learning approach based on policy-gradient and actor-critic methods. The learned strategies closely match the PDE benchmark and remain stable across initial conditions. The results highlight the relevance of policy-gradient techniques for dividend optimisation under self-exciting claim dynamics and point toward scalable methods for higher-dimensional extensions.",
    "authors": [
      "Paulin Aubert",
      "Etienne Chevalier",
      "Vathana Ly Vath"
    ],
    "published": "2025-11-24",
    "categories": [
      "math.OC",
      "math.PR",
      "q-fin.RM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19701v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19701v1",
    "fetched_at": "2025-11-28T11:05:14.085653",
    "chinese_title": "自激发索赔下的最优分红与资本注入",
    "chinese_summary": "本文研究自激发索赔（Hawkes过程）下Cramér-Lundberg模型的最优分红与资本注入问题，建立价值函数分析性质并以显式阈值刻画最优资本注入策略，证明其为HJB变分不等式的唯一粘性解；数值上结合单调有限差分+Howard策略迭代得到基准解，提出基于策略梯度和演员-评论家的强化学习方法，学习策略匹配基准且稳定，为高维扩展提供可扩展思路。",
    "tags": [
      "Reinforcement Learning",
      "Risk Management",
      "Portfolio Optimization",
      "Benchmark"
    ],
    "key_contributions": [
      "建立自激发索赔下最优分红与资本注入问题的价值函数分析性质，以显式阈值刻画最优资本注入策略并证明其为HJB变分不等式的唯一粘性解",
      "提出基于策略梯度和演员-评论家的强化学习方法，学习策略匹配PDE基准且稳定，为高维扩展提供可扩展途径"
    ],
    "processed_at": "2025-11-28T11:35:41.913663"
  },
  {
    "id": "2511.19186v1",
    "title": "Carbon-Penalised Portfolio Insurance Strategies in a Stochastic Factor Model with Partial Information",
    "abstract": "Given the increasing importance of environmental, social and governance (ESG) factors, particularly carbon emissions, we investigate optimal proportional portfolio insurance (PPI) strategies accounting for carbon footprint reduction. PPI strategies enable investors to mitigate downside risk while retaining the potential for upside gains. This paper aims to determine the multiplier of the PPI strategy to maximise the expected utility of the terminal cushion, where the terminal cushion is penalised proportionally to the realised volatility of stocks issued by firms operating in carbon-intensive sectors. We model the risky assets' dynamics using geometric Brownian motions whose drift rates are modulated by an unobservable common stochastic factor to capture market-specific or economy-wide state variables that are typically not directly observable. Using classical stochastic filtering theory, we formulate a suitable optimization problem and solve it for CRRA utility function. We characterise optimal carbon penalised PPI strategies and optimal value functions under full and partial information and quantify the loss of utility due incomplete information. Finally, we carry a numerical analysis showing that the proposed strategy reduces carbon emission intensity without compromising financial performance.",
    "authors": [
      "Katia Colaneri",
      "Federico D'Amario",
      "Daniele Mancinelli"
    ],
    "published": "2025-11-24",
    "categories": [
      "q-fin.PM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19186v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19186v1",
    "fetched_at": "2025-11-28T11:05:14.085662",
    "chinese_title": "含部分信息的随机因子模型下碳惩罚型投资组合保险策略",
    "chinese_summary": "论文将碳惩罚融入比例投资组合保险（PPI）策略，以降低碳足迹并平衡风险收益；用含不可观测随机因子的几何布朗运动建模风险资产，结合随机滤波理论求解CRRA效用下的最优策略，量化信息不完全的效用损失，数值分析显示策略可降碳排放强度且不损害财务表现。",
    "tags": [
      "Factor Model",
      "Portfolio Optimization",
      "Risk Management",
      "Volatility"
    ],
    "key_contributions": [
      "量化部分信息导致的效用损失，数值验证策略可降低碳排放强度且不损害财务表现"
    ],
    "processed_at": "2025-11-28T11:35:50.037533"
  },
  {
    "id": "2511.18804v1",
    "title": "Sentiment Analysis of Financial Text Using Quantum Language Processing QDisCoCirc",
    "abstract": "We apply quantum distributional compositional circuit (QDisCoCirc) to 3-class sentiment analysis of financial text. In our classical simulations, we keep the Hilbert-space dimension manageable by decomposing each sentence into short contiguous chunks. Each chunk is mapped to a shallow quantum circuit, and the resulting Bloch vectors are used as a sequence of quantum tokens. Simple averaging of chunk vectors ignores word order and syntactic roles. We therefore add a small Transformer encoder over the raw Bloch-vector sequence and attach a CCG-based type embedding to each chunk. This hybrid design preserves physically interpretable semantic axes of quantum tokens while allowing the classical side to model word order and long-range dependencies. The sequence model improves test macro-F1 over the averaging baseline and chunk-level attribution further shows that evidential mass concentrates on a small number of chunks, that type embeddings are used more reliably for correctly predicted sentences. For real-world quantum language processing applications in finance, future key challenges include circuit designs that avoid chunking and the design of inter-chunk fusion layers.",
    "authors": [
      "Takayuki Sakuma"
    ],
    "published": "2025-11-24",
    "categories": [
      "q-fin.GN"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18804v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18804v1",
    "fetched_at": "2025-11-28T11:05:14.085670",
    "chinese_title": "基于量子分布组合电路（QDisCoCirc）的金融文本情感分析",
    "chinese_summary": "该论文将量子分布组合电路（QDisCoCirc）应用于金融文本三分类情感分析，通过分解句子为短连续chunk并映射到浅量子电路得到Bloch向量序列；采用量子token+小Transformer编码器+CCG类型嵌入的混合设计，既保留量子语义可解释性，又能建模语序与长依赖，提升了测试macro-F1，且chunk归因显示证据集中于少数chunk、正确预测句子更可靠使用类型嵌入。",
    "tags": [
      "Sentiment Analysis",
      "NLP",
      "Transformer",
      "Investor Sentiment"
    ],
    "key_contributions": [
      "提出量子-经典混合模型（QDisCoCirc+Transformer+CCG嵌入），平衡量子语义可解释性与经典序列建模能力，提升金融文本情感分析性能",
      "通过chunk归因分析揭示金融文本情感预测的证据分布规律，为量子语言处理在金融场景的应用提供实证支持"
    ],
    "processed_at": "2025-11-28T11:36:06.149061"
  },
  {
    "id": "2511.18614v1",
    "title": "A calibrated model of debt recycling with interest costs and tax shields: viability under different fiscal regimes and jurisdictions",
    "abstract": "Debt recycling is a leveraged equity management strategy in which homeowners use accumulated home equity to finance investments, applying the resulting returns to accelerate mortgage repayment. We propose a novel framework to model equity and mortgage dynamics in presence of mortgage interest rates, borrowing costs on equity-backed credit lines, and tax shields arising from interest deductibility. The model is calibrated on three jurisdictions -- Australia, Germany, and Switzerland -- representing diverse interest rate environments and fiscal regimes. Results demonstrate that introducing positive interest rates without tax shields contracts success regions and lengthens repayment times, while tax shields partially reverse these effects by reducing effective borrowing costs and adding equity boosts from mortgage interest deductibility. Country-specific outcomes vary systematically, and rental properties consistently outperform owner-occupied housing due to mortgage interest deductibility provisions.",
    "authors": [
      "Carlo von der Osten",
      "Sabrina Aufiero",
      "Pierpaolo Vivo",
      "Fabio Caccioli",
      "Silvia Bartolucci"
    ],
    "published": "2025-11-23",
    "categories": [
      "q-fin.RM",
      "cond-mat.stat-mech",
      "econ.GN"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18614v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18614v1",
    "fetched_at": "2025-11-28T11:05:14.085685",
    "chinese_title": "考虑利息成本与税盾的债务回收校准模型：不同财政制度及辖区下的可行性",
    "chinese_summary": "论文提出一个新框架，建模股权与抵押贷款动态并纳入抵押贷款利率、股权信贷成本及利息抵扣税盾，通过澳大利亚、德国、瑞士三个辖区校准发现：正利率无税盾会缩小成功区域、延长还款时间，税盾可部分逆转该影响；且租赁房产因税盾规定持续优于自住房产。",
    "tags": [
      "Asset Pricing",
      "Risk Management",
      "Portfolio Optimization"
    ],
    "key_contributions": [
      "提出考虑利息成本与税盾的债务回收动态建模新框架",
      "揭示不同财政制度、房产类型及利率环境下债务回收策略的可行性规律"
    ],
    "processed_at": "2025-11-28T11:36:18.540939"
  },
  {
    "id": "2511.18578v1",
    "title": "Re(Visiting) Time Series Foundation Models in Finance",
    "abstract": "Financial time series forecasting is central to trading, portfolio optimization, and risk management, yet it remains challenging due to noisy, non-stationary, and heterogeneous data. Recent advances in time series foundation models (TSFMs), inspired by large language models, offer a new paradigm for learning generalizable temporal representations from large and diverse datasets. This paper presents the first comprehensive empirical study of TSFMs in global financial markets. Using a large-scale dataset of daily excess returns across diverse markets, we evaluate zero-shot inference, fine-tuning, and pre-training from scratch against strong benchmark models. We find that off-the-shelf pre-trained TSFMs perform poorly in zero-shot and fine-tuning settings, whereas models pre-trained from scratch on financial data achieve substantial forecasting and economic improvements, underscoring the value of domain-specific adaptation. Increasing the dataset size, incorporating synthetic data augmentation, and applying hyperparameter tuning further enhance performance.",
    "authors": [
      "Eghbal Rahimikia",
      "Hao Ni",
      "Weiguan Wang"
    ],
    "published": "2025-11-23",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG",
      "q-fin.PM",
      "q-fin.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18578v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18578v1",
    "fetched_at": "2025-11-28T11:05:14.085695",
    "chinese_title": "重新审视金融领域的时间序列基础模型",
    "chinese_summary": "本文是首个全面实证研究时间序列基础模型（TSFMs）在全球金融市场表现的工作，利用大规模日度超额收益数据集对比零样本推理、微调及从头预训练与基准模型的效果；发现现成预训练TSFMs在零样本和微调场景下表现不佳，但基于金融数据从头预训练的模型在预测能力和经济收益上显著提升，且增大数据集、合成数据增强及超参调优可进一步优化性能。",
    "tags": [
      "Time Series",
      "Deep Learning",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "开展金融领域TSFMs的首个全面实证研究，对比不同训练策略与基准模型在全球金融市场的表现",
      "验证领域特定适配的关键价值，即金融数据从头预训练的TSFMs显著提升预测与经济收益，且数据集增大等方法可进一步优化"
    ],
    "processed_at": "2025-11-28T11:36:34.537227"
  },
  {
    "id": "2511.18169v1",
    "title": "Superhedging under Proportional Transaction Costs in Continuous Time",
    "abstract": "We revisit the well-studied superhedging problem under proportional transaction costs in continuous time using the recently developed tools of set-valued stochastic analysis. By relying on a simple Black-Scholes-type market model for mid-prices and using continuous trading schemes, we define a dynamic family of superhedging sets in continuous time and express them in terms of set-valued integrals. We show that these sets, defined as subsets of Lebesgue spaces at different times, form a dynamic set-valued risk measure with multi-portfolio time-consistency. Finally, we transfer the problem formulation to a path-space setting and introduce approximate versions of superhedging sets that will involve relaxing the superhedging inequality, the superhedging probability, and the solvency requirement for the superhedging strategy with a predetermined error level. In this more technical framework, we are able to relate the approximate superhedging sets at different times by means of a set-valued Bellman's principle, which we believe will pave the way for a set-valued differential structure that characterizes the superhedging sets.",
    "authors": [
      "Atiqah Almuzaini",
      "Çağın Ararat",
      "Jin Ma"
    ],
    "published": "2025-11-22",
    "categories": [
      "q-fin.RM",
      "math.OC",
      "math.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18169v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18169v1",
    "fetched_at": "2025-11-28T11:05:14.085705",
    "chinese_title": "连续时间下含比例交易成本的超对冲问题",
    "chinese_summary": "本文利用集值随机分析工具重新研究连续时间下含比例交易成本的超对冲问题，基于Black-Scholes型中间价市场模型定义动态超对冲集族并以集值积分表示，证明其为多组合时间一致的动态集值风险测度；进一步在路径空间引入带预定误差的近似超对冲集，通过集值Bellman原理关联不同时间的近似集，为刻画超对冲集的集值微分结构奠定基础。",
    "tags": [
      "Risk Management",
      "Options",
      "Portfolio Optimization"
    ],
    "key_contributions": [
      "在路径空间引入带预定误差的近似超对冲集，通过集值Bellman原理关联不同时间的近似集，为刻画超对冲集的集值微分结构铺路"
    ],
    "processed_at": "2025-11-28T11:36:46.290419"
  },
  {
    "id": "2511.18125v1",
    "title": "Random processes for long-term market simulations",
    "abstract": "For long term investments, model portfolios are defined at the level of indexes, a setup known as Strategic Asset Allocation (SAA). The possible outcomes at a scale of a few decades can be obtained by Monte Carlo simulations, resulting in a probability density for the possible portfolio values at the investment horizon. Such studies are critical for long term wealth plannings, for example in the financial component of social insurances or in accumulated capital for retirement. The quality of the results depends on two inputs: the process used for the simulations and its parameters. The base model is a constant drift, a constant covariance and normal innovations, as pioneered by Bachelier. Beyond this model, this document presents in details a multivariate process that incorporate the most recent advances in the models for financial time series. This includes the negative correlations of the returns at a scale of a few years, the heteroskedasticity (i.e. the volatility' dynamics), and the fat tails and asymmetry for the distributions of returns. For the parameters, the quantitative outcomes depend critically on the estimate for the drift, because this is a non random contribution acting at each time step. Replacing the point forecast by a probabilistic forecast allows us to analyze the impact of the drift values, and then to incorporate this uncertainty in the Monte Carlo simulations.",
    "authors": [
      "Gilles Zumbach"
    ],
    "published": "2025-11-22",
    "categories": [
      "q-fin.RM",
      "q-fin.ST"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18125v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18125v1",
    "fetched_at": "2025-11-28T11:05:14.085712",
    "chinese_title": "长期市场模拟的随机过程",
    "chinese_summary": "论文针对长期投资的战略资产配置（SAA），指出传统Bachelier模型的局限性，提出融入金融时间序列最新进展（收益负相关、异方差、厚尾及非对称分布）的多元随机过程；同时强调漂移项估计的关键影响，通过概率预测替代点预测将漂移不确定性纳入蒙特卡洛模拟，提升长期财富规划的模拟质量。",
    "tags": [
      "Time Series",
      "Portfolio Optimization",
      "Risk Management",
      "Volatility"
    ],
    "key_contributions": [
      "用漂移项的概率预测替代点预测，将漂移不确定性纳入蒙特卡洛模拟，优化长期投资结果的分析"
    ],
    "processed_at": "2025-11-28T11:36:55.699788"
  },
  {
    "id": "2511.18117v1",
    "title": "Diffusive Limit of Hawkes Driven Order Book Dynamics With Liquidity Migration",
    "abstract": "This paper develops a theoretical mesoscopic model of the limit order book driven by multivariate Hawkes processes, designed to capture temporal self-excitation and the spatial propagation of order flow across price levels. In contrast to classical zero-intelligence or Poisson based queueing models, the proposed framework introduces mathematically defined migration events between neighbouring price levels, whose intensities are themselves governed by the underlying Hawkes structure. This provides a principled stochastic mechanism for modeling interactions between order arrivals, cancellations, and liquidity movement across adjacent queues.   Starting from a microscopic specification of Hawkes driven order flow, we derive a diffusion approximation which yields a reflected mesoscopic stochastic differential equation (SDE) system for queue volumes. The limiting generator is obtained through a Taylor expansion of the microscopic generator, demonstrating how temporal excitation together with spatial migration determine the drift and diffusion structure of the limit order book in the mesoscopic regime. The resulting model extends existing diffusion limits by incorporating correlated excitations and price level to price level liquidity movement within a unified Hawkes based formulation.   By establishing this diffusive limit, the paper provides a mathematically consistent bridge between high frequency event based models and macroscopic stochastic descriptions of market microstructure. The work is entirely theoretical and lays a foundation for future analytical and numerical developments without relying on empirical calibration.",
    "authors": [
      "Levon Mahseredjian"
    ],
    "published": "2025-11-22",
    "categories": [
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18117v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18117v1",
    "fetched_at": "2025-11-28T11:05:14.085723",
    "chinese_title": "带流动性迁移的霍克斯驱动订单簿动力学的扩散极限",
    "chinese_summary": "论文构建了多元霍克斯过程驱动的限价订单簿介观模型，引入相邻价格水平间流动性迁移事件（强度由霍克斯结构控制），从微观订单流出发推导扩散近似，得到反映队列容量的介观反射随机微分方程系统，建立了高频事件模型与市场微观结构宏观随机描述的数学桥梁。",
    "tags": [
      "Market Microstructure",
      "High Frequency",
      "Algorithmic Trading"
    ],
    "key_contributions": [
      "提出多元霍克斯过程驱动的带流动性迁移的限价订单簿介观模型，刻画订单流的时间自激与空间传播特性",
      "从微观霍克斯驱动订单流推导扩散近似，得到介观反射随机微分方程系统，建立高频事件模型与市场微观结构宏观随机描述的数学桥梁"
    ],
    "processed_at": "2025-11-28T11:37:11.116529"
  },
  {
    "id": "2511.18076v1",
    "title": "Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons",
    "abstract": "This research proposes an enhancement to the innovative portfolio optimization approach using the G-Learning algorithm, combined with parametric optimization via the GIRL algorithm (G-learning approach to the setting of Inverse Reinforcement Learning) as presented by. The goal is to maximize portfolio value by a target date while minimizing the investor's periodic contributions. Our model operates in a highly volatile market with a well-diversified portfolio, ensuring a low-risk level for the investor, and leverages reinforcement learning to dynamically adjust portfolio positions over time. Results show that we improved the Sharpe Ratio from 0.42, as suggested by recent studies using the same approach, to a value of 0.483 a notable achievement in highly volatile markets with diversified portfolios. The comparison between G-Learning and GIRL reveals that while GIRL optimizes the reward function parameters (e.g., lambda = 0.0012 compared to 0.002), its impact on portfolio performance remains marginal. This suggests that reinforcement learning methods, like G-Learning, already enable robust optimization. This research contributes to the growing development of reinforcement learning applications in financial decision-making, demonstrating that probabilistic learning algorithms can effectively align portfolio management strategies with investor needs.",
    "authors": [
      "Fermat Leukam",
      "Rock Stephane Koffi",
      "Prudence Djagba"
    ],
    "published": "2025-11-22",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18076v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18076v1",
    "fetched_at": "2025-11-28T11:05:14.085732",
    "chinese_title": "带财务目标和确定时间范围的投资组合优化强化学习方法",
    "chinese_summary": "论文提出结合G-Learning算法与GIRL算法（逆强化学习的G学习方法）的投资组合优化增强方法，目标是在目标日期前最大化投资组合价值并最小化投资者定期贡献，在高波动市场中使夏普比率从0.42提升至0.483，验证了强化学习方法的稳健优化能力。",
    "tags": [
      "Reinforcement Learning",
      "Portfolio Optimization",
      "Risk Management",
      "Volatility"
    ],
    "key_contributions": [
      "提出结合G-Learning与GIRL的适配财务目标和时间范围的投资组合优化增强方法",
      "在高波动市场中显著提升夏普比率，验证强化学习的稳健优化效果"
    ],
    "processed_at": "2025-11-28T11:37:21.928554"
  },
  {
    "id": "2511.17963v1",
    "title": "Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization",
    "abstract": "This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.",
    "authors": [
      "Jun Kevin",
      "Pujianto Yugopuspito"
    ],
    "published": "2025-11-22",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "q-fin.PM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.17963v1",
    "arxiv_url": "https://arxiv.org/abs/2511.17963v1",
    "fetched_at": "2025-11-28T11:05:14.085740",
    "chinese_title": "混合LSTM与PPO网络用于动态投资组合优化",
    "chinese_summary": "论文提出融合长短期记忆网络（LSTM）预测与近端策略优化（PPO）强化学习的混合框架，利用LSTM捕捉时间依赖，PPO在连续动作空间自适应优化投资组合配置；通过2018-2024年多资产（美股、印尼股市、美债、主流加密货币）数据集评估，该框架相比等权、指数及单模型基准，在非平稳市场下收益更高、韧性更强。",
    "tags": [
      "Portfolio Optimization",
      "Reinforcement Learning",
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "提出融合LSTM预测与PPO强化学习的混合框架，兼顾时间序列预测与动态投资组合优化",
      "实证验证该框架在多资产非平稳市场下，优于传统基准及单模型（仅LSTM/仅PPO）"
    ],
    "processed_at": "2025-11-28T11:37:33.121231"
  },
  {
    "id": "2511.17954v1",
    "title": "A multi-view contrastive learning framework for spatial embeddings in risk modelling",
    "abstract": "Incorporating spatial information, particularly those influenced by climate, weather, and demographic factors, is crucial for improving underwriting precision and enhancing risk management in insurance. However, spatial data are often unstructured, high-dimensional, and difficult to integrate into predictive models. Embedding methods are needed to convert spatial data into meaningful representations for modelling tasks. We propose a novel multi-view contrastive learning framework for generating spatial embeddings that combine information from multiple spatial data sources. To train the model, we construct a spatial dataset that merges satellite imagery and OpenStreetMap features across Europe. The framework aligns these spatial views with coordinate-based encodings, producing low-dimensional embeddings that capture both spatial structure and contextual similarity. Once trained, the model generates embeddings directly from latitude-longitude pairs, enabling any dataset with coordinates to be enriched with meaningful spatial features without requiring access to the original spatial inputs. In a case study on French real estate prices, we compare models trained on raw coordinates against those using our spatial embeddings as inputs. The embeddings consistently improve predictive accuracy across generalised linear, additive, and boosting models, while providing interpretable spatial effects and demonstrating transferability to unseen regions.",
    "authors": [
      "Freek Holvoet",
      "Christopher Blier-Wong",
      "Katrien Antonio"
    ],
    "published": "2025-11-22",
    "categories": [
      "q-fin.RM",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.17954v1",
    "arxiv_url": "https://arxiv.org/abs/2511.17954v1",
    "fetched_at": "2025-11-28T11:05:14.085749",
    "chinese_title": "一种用于风险建模中空间嵌入的多视图对比学习框架",
    "chinese_summary": "论文提出多视图对比学习框架，整合卫星图像与OpenStreetMap等多源空间数据生成空间嵌入，训练后可直接从经纬度生成嵌入；在法国房价预测案例中，该嵌入显著提升各类模型精度，且具有可解释性与跨区域迁移性。",
    "tags": [
      "Risk Management",
      "Deep Learning",
      "Asset Pricing"
    ],
    "key_contributions": [
      "提出多视图对比学习框架，整合多源空间数据生成可从经纬度直接获取的空间嵌入，无需原始空间输入",
      "实证验证嵌入可提升房价预测模型精度，具备可解释性与跨区域迁移性"
    ],
    "processed_at": "2025-11-28T11:37:50.181113"
  },
  {
    "id": "2511.17892v1",
    "title": "Arbitrage-Free Bond and Yield Curve Forecasting with Neural Filters under HJM Constraints",
    "abstract": "We develop an arbitrage-free deep learning framework for yield curve and bond price forecasting based on the Heath-Jarrow-Morton (HJM) term-structure model and a dynamic Nelson-Siegel parameterization of forward rates. Our approach embeds a no-arbitrage drift restriction into a neural state-space architecture by combining Kalman, extended Kalman, and particle filters with recurrent neural networks (LSTM/CLSTM), and introduces an explicit arbitrage error regularization (AER) term during training. The model is applied to U.S. Treasury and corporate bond data, and its performance is evaluated for both yield-space and price-space predictions at 1-day and 5-day horizons. Empirically, arbitrage regularization leads to its strongest improvements at short maturities, particularly in 5-day-ahead forecasts, increasing market-consistency as measured by bid-ask hit rates and reducing dollar-denominated prediction errors.",
    "authors": [
      "Xiang Gao",
      "Cody Hyndman"
    ],
    "published": "2025-11-22",
    "categories": [
      "q-fin.MF",
      "cs.LG",
      "q-fin.CP",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.17892v1",
    "arxiv_url": "https://arxiv.org/abs/2511.17892v1",
    "fetched_at": "2025-11-28T11:05:14.085757",
    "chinese_title": "HJM约束下带神经滤波器的无套利债券及收益率曲线预测",
    "chinese_summary": "本文基于Heath-Jarrow-Morton（HJM）期限结构模型和动态Nelson-Siegel前向利率参数化，开发无套利深度学习框架；通过结合卡尔曼/扩展卡尔曼/粒子滤波器与LSTM/CLSTM嵌入无套利漂移约束，训练中引入显式套利误差正则化（AER）项；实证应用于美债和企业债数据，显示该正则化在短期限（尤5天预测）提升预测性能，增强市场一致性并降低误差。",
    "tags": [
      "Deep Learning",
      "Time Series",
      "Asset Pricing",
      "Market Microstructure"
    ],
    "key_contributions": [
      "实证验证框架在美债/企业债预测中短期限性能提升，增强市场一致性并降低误差"
    ],
    "processed_at": "2025-11-28T11:38:10.245790"
  },
  {
    "id": "2511.21378v1",
    "title": "Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data",
    "abstract": "Handling contaminated data poses a critical challenge in anomaly detection, as traditional models assume training on purely normal data. Conventional methods mitigate contamination by relying on fixed contamination ratios, but discrepancies between assumed and actual ratios can severely degrade performance, especially in noisy environments where normal and abnormal data distributions overlap. To address these limitations, we propose Adaptive and Aggressive Rejection (AAR), a novel method that dynamically excludes anomalies using a modified z-score and Gaussian mixture model-based thresholds. AAR effectively balances the trade-off between preserving normal data and excluding anomalies by integrating hard and soft rejection strategies. Extensive experiments on two image datasets and thirty tabular datasets demonstrate that AAR outperforms the state-of-the-art method by 0.041 AUROC. By providing a scalable and reliable solution, AAR enhances robustness against contaminated datasets, paving the way for broader real-world applications in domains such as security and healthcare.",
    "authors": [
      "Jungi Lee",
      "Jungkwon Kim",
      "Chi Zhang",
      "Kwangsun Yoo",
      "Seok-Joo Byun"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21378v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21378v1",
    "fetched_at": "2025-11-28T11:05:26.408256",
    "chinese_title": "面向污染训练数据的自适应激进拒绝异常检测方法",
    "chinese_summary": "传统异常检测模型假设训练数据为纯正常数据，但污染数据会严重影响性能，现有方法依赖固定污染率易因实际比例偏差失效；论文提出自适应激进拒绝（AAR）方法，通过改进z-score和高斯混合模型阈值动态排除异常，结合软硬拒绝策略平衡正常数据保留与异常排除；实验表明AAR在多数据集上优于现有SOTA方法，增强污染数据鲁棒性，支持安全、医疗等领域应用。",
    "tags": [
      "Anomaly",
      "Risk Management",
      "Benchmark"
    ],
    "key_contributions": [
      "提出自适应激进拒绝（AAR）方法，动态排除异常并平衡正常数据保留与异常排除，解决传统固定污染率导致的性能下降问题",
      "实验验证AAR在图像和表格数据集上优于现有SOTA方法，提升污染数据鲁棒性，拓展实际应用场景"
    ],
    "processed_at": "2025-11-28T11:38:37.384848"
  },
  {
    "id": "2511.20044v1",
    "title": "RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction",
    "abstract": "The proactive prediction of anomalies (AP) in multivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction-Elimination based Dual-stream Contrastive Forecasting framework, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future context to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.",
    "authors": [
      "PengYu Chen",
      "Xiaohou Shi",
      "Yuan Chang",
      "Yan Sun",
      "Sajal K. Das"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20044v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20044v1",
    "fetched_at": "2025-11-28T11:05:26.408437",
    "chinese_title": "RED-F：基于重构-消除的双流对比预测框架用于多元时间序列异常预测",
    "chinese_summary": "针对多元时间序列异常预测中弱异常前兆易被正常模式淹没的问题，提出RED-F框架：含重构-消除模型（REM）通过混合时频机制生成纯净正常基线，双流对比预测模型（DFM）以基线和原始序列为并行输入，通过对比预测放大前兆信号，且DFM采用多序列预测目标利用远期上下文增强预测能力。",
    "tags": [
      "Anomaly",
      "Time Series",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出RED-F框架，通过重构-消除生成正常基线、双流对比预测放大弱异常前兆，解决现有方法难检测弱前兆的问题",
      "DFM采用多序列预测（MSP）目标，利用远期上下文提升异常预测的准确性"
    ],
    "processed_at": "2025-11-28T11:38:47.522861"
  },
  {
    "id": "2511.19090v1",
    "title": "Optimization of Deep Learning Models for Dynamic Market Behavior Prediction",
    "abstract": "The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.",
    "authors": [
      "Shenghan Zhao",
      "Yuzhen Lin",
      "Ximeng Yang",
      "Qiaochu Lu",
      "Haozhong Xue",
      "Gaozhe Jiang"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19090v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19090v1",
    "fetched_at": "2025-11-28T11:05:35.665840",
    "chinese_title": "动态市场行为预测的深度学习模型优化",
    "chinese_summary": "论文聚焦电商零售SKU级多horizon（H=1、7、14）日度需求/收入预测，提出融合多尺度时序卷积、门控循环模块及时序感知自注意力的混合序列模型；经严格时间分割训练评估，模型在准确率及峰值/假期稳健性上优于传统及前沿基准模型，且通过消融实验与统计显著性检验验证可靠性并开源实现。",
    "tags": [
      "Deep Learning",
      "Time Series",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "提出融合多尺度时序卷积、门控循环及时间感知自注意力的混合序列模型，优化电商零售多horizon需求预测",
      "采用严格时间分割避免数据泄露，通过消融实验与统计显著性检验验证模型可靠性，且开源实现便于复现"
    ],
    "processed_at": "2025-11-28T11:39:02.576616"
  },
  {
    "id": "2511.21537v1",
    "title": "Context-Specific Causal Graph Discovery with Unobserved Contexts: Non-Stationarity, Regimes and Spatio-Temporal Patterns",
    "abstract": "Real-world data, for example in climate applications, often consists of spatially gridded time series data or data with comparable structure. While the underlying system is often believed to behave similar at different points in space and time, those variations that do exist are twofold relevant: They often encode important information in and of themselves. And they may negatively affect the stability / convergence and reliability\\Slash{}validity of results of algorithms assuming stationarity or space-translation invariance. We study the information encoded in changes of the causal graph, with stability in mind. An analysis of this general task identifies two core challenges. We develop guiding principles to overcome these challenges, and provide a framework realizing these principles by modifying constraint-based causal discovery approaches on the level of independence testing. This leads to an extremely modular, easily extensible and widely applicable framework. It can leverage existing constraint-based causal discovery methods (demonstrated on IID-algorithms PC, PC-stable, FCI and time series algorithms PCMCI, PCMCI+, LPCMCI) with little to no modification. The built-in modularity allows to systematically understand and improve upon an entire array of subproblems. By design, it can be extended by leveraging insights from change-point-detection, clustering, independence-testing and other well-studied related problems. The division into more accessible sub-problems also simplifies the understanding of fundamental limitations, hyperparameters controlling trade-offs and the statistical interpretation of results. An open-source implementation will be available soon.",
    "authors": [
      "Martin Rabel",
      "Jakob Runge"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "math.ST"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21537v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21537v1",
    "fetched_at": "2025-11-28T11:05:45.458613",
    "chinese_title": "含未观测上下文的特定上下文因果图发现：非平稳性、机制与时空模式",
    "chinese_summary": "针对含未观测上下文的非平稳时空数据（如气候网格时间序列），论文提出模块化因果图发现框架，通过修改约束类因果算法的独立性测试层适配数据变化，可复用PC、PCMCI等现有算法并支持结合变化点检测等方法扩展。",
    "tags": [
      "Time Series",
      "Anomaly"
    ],
    "key_contributions": [
      "提出模块化因果图发现框架，通过修改约束类算法的独立性测试层，适配含未观测上下文的非平稳时空数据，提升算法稳定性与可靠性",
      "框架可复用现有PC、PCMCI等因果发现算法，支持结合变化点检测、聚类等方法扩展，模块化设计便于子问题分析与优化"
    ],
    "processed_at": "2025-11-28T11:39:26.612267"
  },
  {
    "id": "2511.18615v1",
    "title": "Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors",
    "abstract": "Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\\boldsymbolα$ and class priors $\\boldsymbolπ$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.",
    "authors": [
      "Jiawei Hu",
      "Javier A. Barria"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18615v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18615v1",
    "fetched_at": "2025-11-28T11:05:45.459062",
    "chinese_title": "基于贝叶斯的带动态狄利克雷先验的在线标签偏移估计",
    "chinese_summary": "针对监督学习中标签偏移（训练与测试类先验分布差异）导致分类器性能下降的问题，论文提出FMAPLS（全最大后验标签偏移）及其在线版本online-FMAPLS，通过批量/在线EM算法联合动态优化狄利克雷超参数和类先验，克服现有MAPLS的刚性约束；引入线性替代函数（LSF）得到闭式解降低计算复杂度，在线版本用随机近似实现流数据实时适应，理论分析了收敛率与精度的权衡，实验验证方法性能更优。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出FMAPLS及其在线版本，利用批量/在线EM算法联合动态优化狄利克雷超参数和类先验，解决现有MAPLS的刚性约束问题",
      "引入线性替代函数（LSF）得到闭式解降低计算复杂度，在线版本实现流数据实时适应，理论分析收敛率与精度的权衡，实验验证方法优越性"
    ],
    "processed_at": "2025-11-28T11:40:01.496048"
  },
  {
    "id": "2511.18850v1",
    "title": "Cognitive Alpha Mining via LLM-Driven Code-Based Evolution",
    "abstract": "Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.",
    "authors": [
      "Fengyuan Liu",
      "Huang Yi",
      "Sichun Luo",
      "Yuqi Wang",
      "Yazheng Yang",
      "Xinye Li",
      "Zefa Hu",
      "Junlan Feng",
      "Qi Liu"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18850v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18850v1",
    "fetched_at": "2025-11-28T11:06:32.634207",
    "chinese_title": "基于LLM驱动的代码进化的认知阿尔法挖掘",
    "chinese_summary": "针对现有阿尔法挖掘方法搜索空间狭窄、模型不透明或经济依据不足等问题，本文提出CogAlpha框架，融合代码级阿尔法表示、LLM驱动推理与进化搜索，通过多阶段提示和金融反馈迭代优化候选阿尔法；在A股市场的实验表明，该框架挖掘的阿尔法在预测精度、鲁棒性和泛化性上均优于现有方法。",
    "tags": [
      "LLM",
      "Factor Mining",
      "Asset Pricing",
      "Algorithmic Trading"
    ],
    "key_contributions": [
      "提出CogAlpha认知阿尔法挖掘框架，结合代码级阿尔法表示、LLM驱动推理与进化搜索，平衡逻辑一致性与创造性探索，有效拓展阿尔法搜索空间",
      "在A股市场实验验证，该框架挖掘的阿尔法具有更优的预测精度、鲁棒性与泛化性，优于现有阿尔法挖掘方法"
    ],
    "processed_at": "2025-11-28T11:40:27.568745"
  },
  {
    "id": "2511.18177v1",
    "title": "Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models",
    "abstract": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.",
    "authors": [
      "Elias Lumer",
      "Matt Melich",
      "Olivia Zino",
      "Elena Kim",
      "Sara Dieter",
      "Pradeep Honaganahalli Basavaraju",
      "Vamse Kumar Subbiah",
      "James A. Burke",
      "Roberto Hernandez"
    ],
    "published": "2025-11-22",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18177v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18177v1",
    "fetched_at": "2025-11-28T11:06:32.634258",
    "chinese_title": "重新思考检索：金融领域大语言模型从传统检索增强生成到智能体与非向量推理系统的演进",
    "chinese_summary": "本文首次系统对比金融文档的向量基智能体RAG（混合搜索+元数据过滤）与非向量层次节点基RAG架构，评估交叉编码器重排序、小到大分块检索两种增强技术的效果；基于1200份SEC文件和150题基准，测检索指标、回答质量、 latency及成本，发现向量基智能体RAG更优，两种增强技术显著提升性能。",
    "tags": [
      "LLM",
      "NLP",
      "Financial Agent",
      "Benchmark"
    ],
    "key_contributions": [
      "首次系统对比金融文档的向量基与非向量RAG架构，多维度评估其检索准确率、回答质量、 latency及成本",
      "验证交叉编码器重排序、小到大分块检索对向量基RAG的提升效果，给出实证结果"
    ],
    "processed_at": "2025-11-28T11:40:45.283565"
  },
  {
    "id": "2511.18671v2",
    "title": "Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition",
    "abstract": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.",
    "authors": [
      "Yan Wang",
      "Ke Deng",
      "Yongli Ren"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18671v2",
    "arxiv_url": "https://arxiv.org/abs/2511.18671v2",
    "fetched_at": "2025-11-28T11:07:00.945684",
    "chinese_title": "带单调非线性批评者分解的多智能体交叉熵方法",
    "chinese_summary": "合作多智能体强化学习（MARL）的集中训练分散执行（CTDE）框架存在集中分散不匹配（CDM）问题，现有价值分解方法面临线性表达有限或非线性需集中梯度的权衡；论文提出带单调非线性批评者分解（NCD）的多智能体交叉熵方法（MCEM），通过提升高价值联合动作概率排除次优行为，还扩展了带修正k步回报和Retrace的离策略学习，实验表明其在连续与离散动作基准上优于当前最优方法。",
    "tags": [
      "Reinforcement Learning",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出带单调非线性批评者分解（NCD）的多智能体交叉熵方法（MCEM），解决CTDE中的CDM问题，平衡价值分解的表达能力与梯度分散性；",
      "扩展离策略学习引入修正k步回报和Retrace提升样本效率，在连续与离散动作基准上优于SOTA合作多智能体强化学习方法。"
    ],
    "processed_at": "2025-11-28T11:41:22.817702"
  },
  {
    "id": "2511.21101v1",
    "title": "MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing",
    "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.",
    "authors": [
      "Manish Jain",
      "Satheesh Kumar Ponnambalam",
      "Salman Faroz",
      "Chandrakanth Lns",
      "Vinay Sharma"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21101v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21101v1",
    "fetched_at": "2025-11-28T11:07:19.617929",
    "chinese_title": "MortgageLLM：带残差指令迁移、对齐调优和任务特定路由的领域自适应预训练",
    "chinese_summary": "论文提出抵押贷款领域专用大模型MortgageLLM，采用双轨特化框架从LLaMA-3.1-8B构建对话问答与结构化任务两个专家模型，通过指令残差技术恢复指令遵循能力并设计智能任务路由机制，在领域基准上显著优于基础模型。",
    "tags": [
      "LLM",
      "NLP",
      "Financial Agent",
      "Transformer"
    ],
    "key_contributions": [
      "将指令残差技术应用于抵押贷款金融领域",
      "提出结合对话问答与结构化任务的双专家架构",
      "设计基于专家模型自身少样本分类的智能任务路由机制"
    ],
    "processed_at": "2025-11-28T11:41:37.385028"
  },
  {
    "id": "2511.19330v1",
    "title": "Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data",
    "abstract": "A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.",
    "authors": [
      "Dominik Luszczynski"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19330v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19330v1",
    "fetched_at": "2025-11-28T11:07:38.239953",
    "chinese_title": "定向操纵：基于斜率的金融时间序列数据攻击",
    "chinese_summary": "本文针对金融时间序列预测的对抗攻击研究不足问题，提出通用斜率攻击与最小二乘斜率攻击两种方法，可使N-HiTS模型的股票预测斜率翻倍；该攻击能绕过CNN判别器等标准安全机制，还可结合GAN生成逼真合成数据，同时设计样本恶意软件证明需保护模型推理全流程。",
    "tags": [
      "Deep Learning",
      "Time Series",
      "Anomaly",
      "Algorithmic Trading"
    ],
    "key_contributions": [
      "提出两种基于斜率的对抗攻击方法，可操纵N-HiTS模型的股票预测趋势",
      "证明该攻击能绕过标准安全机制，结合GAN生成逼真合成数据，且强调需保护模型推理全流程"
    ],
    "processed_at": "2025-11-28T11:41:56.792208"
  },
  {
    "id": "2511.18613v1",
    "title": "KAN vs LSTM Performance in Time Series Forecasting",
    "abstract": "This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements.",
    "authors": [
      "Tabish Ali Rather",
      "S M Mahmudul Hasan Joy",
      "Nadezda Sukhorukova",
      "Federico Frascoli"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18613v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18613v1",
    "fetched_at": "2025-11-28T11:07:38.240023",
    "chinese_title": "KAN与LSTM在时间序列预测中的性能比较",
    "chinese_summary": "本文以均方根误差（RMSE）评估预测准确性与可解释性的权衡，比较Kolmogorov-Arnold网络（KAN）与长短期记忆网络（LSTM）在非确定性股票价格时间序列预测中的性能；研究发现LSTM在所有测试预测 horizon 上表现显著更优，标准KAN误差高且实用性有限，但KAN在资源受限、精度要求较低的场景下计算效率有优势，结果支持LSTM用于实际金融预测并建议持续研究专门化KAN架构。",
    "tags": [
      "Deep Learning",
      "Time Series",
      "Asset Pricing"
    ],
    "key_contributions": [
      "验证LSTM在非确定性股票价格时间序列预测中，所有测试预测 horizon 上的性能显著优于标准KAN，确认其在精度关键型时间序列应用中的主导地位",
      "揭示标准KAN虽具理论可解释性但实际预测误差高、实用性有限，同时指出其在资源受限且精度要求较低场景下的计算效率优势，并建议持续研究专门化KAN架构以优化性能"
    ],
    "processed_at": "2025-11-28T11:42:28.950459"
  },
  {
    "id": "2511.19930v1",
    "title": "Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities",
    "abstract": "Recent advances in machine learning and big data analytics have intensified the demand for high-quality cross-domain datasets and accelerated the growth of data trading across organizations. As data become increasingly recognized as an economic asset, data marketplaces have emerged as a key infrastructure for data-driven innovation. However, unlike mature product or service markets, data-trading environments remain nascent and suffer from pronounced information asymmetry. Buyers cannot verify the content or quality before purchasing data, making trust and quality assurance central challenges. To address these issues, this study develops a multi-agent data-market simulator that models participant behavior and evaluates the institutional mechanisms for trust formation. Focusing on the manufacturing sector, where initiatives such as GAIA-X and Catena-X are advancing, the simulator integrates reinforcement learning (RL) for adaptive agent behavior and inverse reinforcement learning (IRL) to estimate utility functions from empirical behavioral data. Using the simulator, we examine the market-level effects of five representative reputation systems-Time-decay, Bayesian-beta, PageRank, PowerTrust, and PeerTrust-and found that PeerTrust achieved the strongest alignment between data price and quality, while preventing monopolistic dominance. Building on these results, we develop a hybrid reputation mechanism that integrates the strengths of existing systems to achieve improved price-quality consistency and overall market stability. This study extends simulation-based data-market analysis by incorporating trust and reputation as endogenous mechanisms and offering methodological and institutional insights into the design of reliable and efficient data ecosystems.",
    "authors": [
      "Kenta Yamamoto",
      "Teruaki Hayashi"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.GT",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19930v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19930v1",
    "fetched_at": "2025-11-28T08:32:42.017454",
    "chinese_title": "设计制造数据交易市场的声誉系统：结合Q学习和IRL估计效用的多智能体评估",
    "chinese_summary": "论文针对制造数据交易市场的信息不对称问题，开发了集成强化学习（Q学习）与逆强化学习（IRL）的多智能体模拟器，评估五种声誉系统的市场效果并提出融合优势的混合机制，以提升数据价格与质量的对齐度。",
    "tags": [
      "Financial Agent",
      "Reinforcement Learning",
      "Market Microstructure",
      "Asset Pricing"
    ],
    "key_contributions": [
      "评估五种声誉系统（时间衰减、贝叶斯-beta等）并提出混合机制，优化数据价格与质量的对齐度"
    ],
    "processed_at": "2025-11-28T08:35:49.522448"
  },
  {
    "id": "2511.21622v1",
    "title": "On the Origin of Algorithmic Progress in AI",
    "abstract": "Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.",
    "authors": [
      "Hans Gundlach",
      "Alex Fogelson",
      "Jayson Lynch",
      "Ana Trisovic",
      "Jonathan Rosenfeld",
      "Anmol Sandhu",
      "Neil Thompson"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21622v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21622v1",
    "fetched_at": "2025-11-28T08:32:45.446868",
    "chinese_title": "论AI算法进步的起源",
    "chinese_summary": "论文通过小尺度消融实验、文献调查及缩放实验发现，2012-2023年AI训练FLOP效率提升的大部分（6930倍）来自算法的尺度依赖改进（如LSTM到Transformer的compute-optimal scaling law指数差异），而非小模型算法创新；指出此前对算法进步的估计高估了小模型贡献，算法效率度量强依赖参考尺度。",
    "tags": [
      "Transformer",
      "Deep Learning",
      "LLM",
      "Algorithmic Trading"
    ],
    "key_contributions": [
      "揭示AI算法效率提升的核心来源是尺度依赖的算法改进（如Transformer相对LSTM的scale-dependent优势），而非小模型算法创新",
      "指出算法效率度量具有强参考尺度依赖性，修正了此前对小模型算法进步的高估"
    ],
    "processed_at": "2025-11-28T08:36:05.753534"
  },
  {
    "id": "2511.21569v1",
    "title": "Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit",
    "abstract": "If a language model cannot reliably disclose its AI identity in expert contexts, users cannot trust its competence boundaries. This study examines self-transparency in models assigned professional personas within high-stakes domains where false expertise risks user harm. Using a common-garden design, sixteen open-weight models (4B--671B parameters) were audited across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure initially, while a Neurosurgeon persona elicited only 3.5%. This creates preconditions for a \"Reverse Gell-Mann Amnesia\" effect, where transparency in some domains leads users to overgeneralize trust to contexts where disclosure fails. Disclosure ranged from 2.8% to 73.6%, with a 14B model reaching 61.4% while a 70B produced just 4.1%. Model identity predicted behavior better than parameter count ($ΔR_{adj}^{2} = 0.359$ vs 0.018). Reasoning optimization actively suppressed self-transparency in some models, with reasoning variants showing up to 48.4% lower disclosure than base counterparts. Bayesian validation with Rogan--Gladen correction confirmed robustness to measurement error ($κ= 0.908$). These findings demonstrate transparency reflects training factors rather than scale. Organizations cannot assume safety properties transfer to deployment contexts, requiring deliberate behavior design and empirical verification.",
    "authors": [
      "Alex Diep"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21569v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21569v1",
    "fetched_at": "2025-11-28T08:32:45.446897",
    "chinese_title": "专家角色大语言模型的自我透明性失败：大规模行为审计",
    "chinese_summary": "该研究采用common-garden设计，对16个开源LLM（4B-671B参数）开展19200次试验，审计其专家角色下的自我透明性（披露AI身份）；发现模型在不同领域披露率差异显著（如金融顾问30.8% vs神经外科医生3.5%），模型身份比参数规模更能预测行为，推理优化可能抑制透明性，强调透明性由训练因素决定而非规模，需刻意设计与实证验证。",
    "tags": [
      "LLM",
      "Behavioral Finance",
      "NLP"
    ],
    "key_contributions": [
      "揭示专家角色LLM的自我透明性存在领域特异性不一致，模型身份比参数规模更能预测披露行为",
      "发现推理优化可能抑制自我透明性，透明性由训练因素而非模型规模决定，需部署中刻意设计与实证验证"
    ],
    "processed_at": "2025-11-28T08:36:20.998706"
  },
  {
    "id": "2511.21285v1",
    "title": "PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark",
    "abstract": "Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.",
    "authors": [
      "Robert Belanec",
      "Branislav Pecher",
      "Ivan Srba",
      "Maria Bielikova"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21285v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21285v1",
    "fetched_at": "2025-11-28T08:32:45.446921",
    "chinese_title": "PEFT-Bench：一种参数高效微调方法基准",
    "chinese_summary": "针对大语言模型（LLM）微调成本高且现有参数高效微调（PEFT）方法评估存在局限的问题，论文提出PEFT-Bench统一端到端基准，覆盖27个NLP数据集和6种PEFT方法；同时引入PSCP指标，综合可训练参数、推理速度与训练内存等因素评估PEFT方法。",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark"
    ],
    "key_contributions": [
      "提出PEFT-Bench统一基准，可评估不同PEFT方法在自回归LLM上的表现，覆盖多数据集与方法",
      "引入PSCP指标，综合多维度因素量化评估PEFT方法的性能与资源消耗"
    ],
    "processed_at": "2025-11-28T08:36:30.649002"
  },
  {
    "id": "2511.21032v1",
    "title": "A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems",
    "abstract": "Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.",
    "authors": [
      "Yuxuan Zhu",
      "Cong Fu",
      "Yabo Ni",
      "Anxiang Zeng",
      "Yuan Fang"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21032v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21032v1",
    "fetched_at": "2025-11-28T08:32:45.446945",
    "chinese_title": "工业级推荐系统中时间分布泛化的概率框架",
    "chinese_summary": "针对时间分布偏移（TDS）削弱推荐系统长期准确率的问题，现有方法存在泛化不稳定、表征崩塌等局限，本文提出ELBO_TDS概率框架——先通过统计分析识别关键时间偏移因素并设计数据增强策略扩展训练分布，再基于因果图推导自监督变分目标以避免表征崩塌，实验证明其提升用户GMV且已部署于Shopee产品搜索。",
    "tags": [
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "提出ELBO_TDS概率框架，可无缝集成工业级增量学习 pipeline，有效应对时间分布偏移（TDS）问题",
      "设计基于统计分析的时间偏移因素数据增强策略，结合因果图推导自监督变分目标，提升 temporal generalization 并避免表征崩塌，实验验证GMV提升且成功部署于Shopee产品搜索"
    ],
    "processed_at": "2025-11-28T08:36:49.420208"
  },
  {
    "id": "2511.20601v1",
    "title": "The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting",
    "abstract": "Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.",
    "authors": [
      "Heman Shakeri"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20601v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20601v1",
    "fetched_at": "2025-11-28T08:32:45.446963",
    "chinese_title": "驱动因子盲视现象：深度序列模型为何在血糖预测中默认依赖自相关性",
    "chinese_summary": "论文指出深度序列模型在血糖预测中存在“驱动因子盲视”（无法利用胰岛素、饮食、活动等临床信息），通过Δ_drivers（多变量模型相对单变量基线的性能增益）量化该现象；分析其源于架构偏向自相关、驱动因子数据保真度不足、生理异质性三个交互因素；提出生理特征编码器、因果正则化、个性化等缓解策略，并建议未来工作常规报告Δ_drivers避免此类模型被视为最优。",
    "tags": [
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "提出“驱动因子盲视”现象并通过Δ_drivers量化该现象",
      "分析其三大成因并提出缓解策略及未来研究建议"
    ],
    "processed_at": "2025-11-28T08:37:02.021540"
  },
  {
    "id": "2511.20497v1",
    "title": "Quantifying the Privacy Implications of High-Fidelity Synthetic Network Traffic",
    "abstract": "To address the scarcity and privacy concerns of network traffic data, various generative models have been developed to produce synthetic traffic. However, synthetic traffic is not inherently privacy-preserving, and the extent to which it leaks sensitive information, and how to measure such leakage, remain largely unexplored. This challenge is further compounded by the diversity of model architectures, which shape how traffic is represented and synthesized. We introduce a comprehensive set of privacy metrics for synthetic network traffic, combining standard approaches like membership inference attacks (MIA) and data extraction attacks with network-specific identifiers and attributes. Using these metrics, we systematically evaluate the vulnerability of different representative generative models and examine the factors that influence attack success. Our results reveal substantial variability in privacy risks across models and datasets. MIA success ranges from 0% to 88%, and up to 100% of network identifiers can be recovered from generated traffic, highlighting serious privacy vulnerabilities. We further identify key factors that significantly affect attack outcomes, including training data diversity and how well the generative model fits the training data. These findings provide actionable guidance for designing and deploying generative models that minimize privacy leakage, establishing a foundation for safer synthetic network traffic generation.",
    "authors": [
      "Van Tran",
      "Shinan Liu",
      "Tian Li",
      "Nick Feamster"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20497v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20497v1",
    "fetched_at": "2025-11-28T08:32:45.446986",
    "chinese_title": "量化高保真合成网络流量的隐私影响",
    "chinese_summary": "针对网络流量数据稀缺与隐私问题，现有合成流量并非天生隐私保护，论文提出结合成员推理攻击、数据提取攻击及网络特定标识符的综合隐私度量，系统评估不同生成模型的隐私漏洞，发现模型/数据集间风险差异显著，识别训练数据多样性等关键影响因素，为安全合成网络流量生成提供指导。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出一套融合标准攻击与网络特定属性的合成网络流量综合隐私度量体系",
      "系统评估不同生成模型的隐私漏洞，揭示训练数据多样性等关键影响因素，为安全合成流量生成提供 actionable 指导"
    ],
    "processed_at": "2025-11-28T08:37:18.423424"
  },
  {
    "id": "2511.20395v1",
    "title": "Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI",
    "abstract": "Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.   We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.   Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.   To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.",
    "authors": [
      "M. C. Schoppema",
      "B. H. M. van der Velden",
      "A. Hürriyetoğlu",
      "M. D. Klijnstra",
      "E. J. Faassen",
      "A. Gerssen",
      "H. J. van der Fels-Klerx"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20395v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20395v1",
    "fetched_at": "2025-11-28T08:32:45.447016",
    "chinese_title": "使用可解释人工智能识别与双壳类软体动物河豚毒素污染相关的环境因素",
    "chinese_summary": "该研究开发了基于可解释深度学习的模型，以气象和水文特征为输入预测双壳类软体动物河豚毒素（TTX）污染的有无；模型识别出有效日照时长（日长+总辐射）、水温、氯化物浓度等关键关联环境因素，为食品行业和监管部门缓解海洋毒素风险提供工具。",
    "tags": [
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "构建了可解释深度学习模型，实现双壳类软体动物TTX污染的有无预测",
      "明确有效日照时长、总辐射、水温、氯化物浓度为TTX污染的关键关联环境因素"
    ],
    "processed_at": "2025-11-28T08:37:36.897782"
  },
  {
    "id": "2511.20224v1",
    "title": "DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation",
    "abstract": "Duo-Tok is a source-aware dual-codebook tokenizer for vocal-accompaniment music that targets the growing tension between reconstruction quality and language-model (LM) learnability in modern lyrics-to-song systems. Existing codecs either prioritize high-fidelity reconstruction with difficult-to-model acoustic tokens or compress aggressively into semantic tokens that are LM-friendly but lossy, and they rarely make the tokenizer itself aware of dual-track structure. Duo-Tok follows a four-stage, SSL-centered pipeline: we first pretrain a BEST-RQ-style encoder on large-scale audio, then stabilize and factorize the representation with Gaussian replacement noise and multi-task supervision, before freezing the encoder to learn SimVQ-based dual codebooks with hard routing for vocals and accompaniment, and finally training latent diffusion decoders on top of the discrete tokens. Duo-Tok at 0.75 kbps shifts the empirical reconstruction-generation Pareto frontier, achieving the best music-tagging AP and the lowest vocabulary-normalized LM perplexity among compared codecs while maintaining reconstruction quality comparable to state-of-the-art music tokenizers.",
    "authors": [
      "Rui Lin",
      "Zhiyue Wu",
      "Jiahe Le",
      "Kangdi Wang",
      "Weixiong Chen",
      "Junyu Dai",
      "Tao Jiang"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20224v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20224v1",
    "fetched_at": "2025-11-28T08:32:45.447045",
    "chinese_title": "DUO-TOK：用于人声伴奏生成的双轨语义音乐Tokenizer",
    "chinese_summary": "针对现有歌词到歌曲系统中重建质量与语言模型可学习性的矛盾，提出Duo-Tok源感知双码本Tokenizer，采用四阶段SSL中心 pipeline，先预训练编码器再分解表示、学习双码本并训练扩散解码器，在0.75kbps下提升Pareto前沿，实现最优音乐标记AP和最低LM困惑度，同时保持高重建质量。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出源感知双码本Tokenizer，平衡音乐重建质量与语言模型可学习性",
      "构建四阶段SSL中心 pipeline，优化表示稳定性与分解效果，提升Pareto前沿"
    ],
    "processed_at": "2025-11-28T08:37:57.257230"
  },
  {
    "id": "2511.19997v1",
    "title": "Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test",
    "abstract": "Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a \"reversal curse,\" and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.",
    "authors": [
      "Mihir Sahasrabudhe"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19997v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19997v1",
    "fetched_at": "2025-11-28T08:32:45.447062",
    "chinese_title": "Transformer中的方向优化不对称性：一种合成压力测试",
    "chinese_summary": "论文设计了完全合成、熵控制的基准（基于可调分支因子K的随机字符串映射），构建零条件熵的前向任务与解析熵下限的反向任务；发现即使无语言先验等语义信息，GPT-2等Transformer存在强方向优化差距（远大于MLP），预训练初始化不消除该差距，LoRA在高熵反向映射遇容量墙，隔离了因果Transformer训练固有的方向摩擦特征。",
    "tags": [
      "Transformer",
      "Benchmark",
      "Deep Learning",
      "LLM"
    ],
    "key_contributions": [
      "构建无语义、熵控制的合成基准，排除语言统计等干扰，清晰隔离Transformer方向优化不对称性的来源",
      "揭示因果Transformer训练存在固有方向摩擦（无语言先验下仍显著），该差距远大于MLP，预训练/LoRA无法消除或存在容量限制"
    ],
    "processed_at": "2025-11-28T08:38:10.216290"
  },
  {
    "id": "2511.19980v1",
    "title": "Operator Learning at Machine Precision",
    "abstract": "Neural operator learning methods have garnered significant attention in scientific computing for their ability to approximate infinite-dimensional operators. However, increasing their complexity often fails to substantially improve their accuracy, leaving them on par with much simpler approaches such as kernel methods and more traditional reduced-order models. In this article, we set out to address this shortcoming and introduce CHONKNORIS (Cholesky Newton--Kantorovich Neural Operator Residual Iterative System), an operator learning paradigm that can achieve machine precision. CHONKNORIS draws on numerical analysis: many nonlinear forward and inverse PDE problems are solvable by Newton-type methods. Rather than regressing the solution operator itself, our method regresses the Cholesky factors of the elliptic operator associated with Tikhonov-regularized Newton--Kantorovich updates. The resulting unrolled iteration yields a neural architecture whose machine-precision behavior follows from achieving a contractive map, requiring far lower accuracy than end-to-end approximation of the solution operator. We benchmark CHONKNORIS on a range of nonlinear forward and inverse problems, including a nonlinear elliptic equation, Burgers' equation, a nonlinear Darcy flow problem, the Calderón problem, an inverse wave scattering problem, and a problem from seismic imaging. We also present theoretical guarantees for the convergence of CHONKNORIS in terms of the accuracy of the emulated Cholesky factors. Additionally, we introduce a foundation model variant, FONKNORIS (Foundation Newton--Kantorovich Neural Operator Residual Iterative System), which aggregates multiple pre-trained CHONKNORIS experts for diverse PDEs to emulate the solution map of a novel nonlinear PDE. Our FONKNORIS model is able to accurately solve unseen nonlinear PDEs such as the Klein--Gordon and Sine--Gordon equations.",
    "authors": [
      "Aras Bacho",
      "Aleksei G. Sorokin",
      "Xianjin Yang",
      "Théo Bourdais",
      "Edoardo Calvello",
      "Matthieu Darcy",
      "Alexander Hsu",
      "Bamdad Hosseini",
      "Houman Owhadi"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG",
      "math.NA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19980v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19980v1",
    "fetched_at": "2025-11-28T08:32:45.447093",
    "chinese_title": "机器精度下的算子学习",
    "chinese_summary": "针对神经算子学习复杂度提升但精度难显著提升的问题，本文提出CHONKNORIS方法，基于牛顿类方法回归Tikhonov正则化牛顿-Kantorovich更新关联椭圆算子的Cholesky因子，通过收缩映射实现机器精度，且所需精度低于端到端解算子近似，在多类非线性正反问题中验证有效。",
    "tags": [
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "提出CHONKNORIS算子学习范式，结合牛顿类方法与Cholesky因子回归，实现机器精度的算子近似",
      "在多类非线性正反问题（含PDE、逆散射、地震成像等）中验证方法有效性，且证明其所需精度低于端到端解算子近似"
    ],
    "processed_at": "2025-11-28T08:38:37.083657"
  },
  {
    "id": "2511.19798v1",
    "title": "KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)",
    "abstract": "Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.",
    "authors": [
      "Weizhi Liu",
      "Xi Chen",
      "Zekun Jiang",
      "Liang Zhao",
      "Kunyuan Jiang",
      "Ruisi Tang",
      "Li Wang",
      "Mingke You",
      "Hanyu Zhou",
      "Hongyu Chen",
      "Qiankun Xiong",
      "Yong Nie",
      "Kang Li",
      "Jian Li"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19798v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19798v1",
    "fetched_at": "2025-11-28T08:32:45.447142",
    "chinese_title": "KOM：用于膝骨关节炎（KOA）精准管理的多智能体人工智能系统",
    "chinese_summary": "针对全球超6亿膝骨关节炎（KOA）患者个性化多学科干预资源不足的问题，开发多智能体系统KOM，可自动化KOA评估、风险预测与治疗处方；实验显示其在影像分析和处方生成上优于通用大模型，且与临床医生协作能提升诊疗效率（减少38.5%时间）和治疗质量。",
    "tags": [
      "LLM",
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "开发多智能体系统KOM，实现KOA自动化评估、风险预测与治疗处方全流程支持",
      "验证KOM与临床医生协作可显著提升KOA诊疗效率与治疗质量"
    ],
    "processed_at": "2025-11-28T08:38:45.277635"
  },
  {
    "id": "2511.19755v1",
    "title": "Clustering Approaches for Mixed-Type Data: A Comparative Study",
    "abstract": "Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.",
    "authors": [
      "Badih Ghattas",
      "Alvaro Sanchez San-Benito"
    ],
    "published": "2025-11-24",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19755v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19755v1",
    "fetched_at": "2025-11-28T08:32:45.447162",
    "chinese_title": "混合类型数据的聚类方法：一项比较研究",
    "chinese_summary": "该研究系统比较混合类型数据的聚类方法，涵盖k-prototypes等距离类方法及KAMILA等概率类方法；通过模拟实验分析簇数量、重叠度等因素对性能的影响，发现KAMILA、LCM、k-prototypes表现最优，且所有方法均有R包实现。",
    "tags": [
      "Benchmark",
      "Factor Mining"
    ],
    "key_contributions": [
      "系统比较混合类型数据的聚类方法（含距离类与概率类），分析实验因素对性能的影响",
      "发现KAMILA、LCM、k-prototypes表现最优，且所有方法均提供R实现"
    ],
    "processed_at": "2025-11-28T08:39:19.049904"
  },
  {
    "id": "2511.19359v1",
    "title": "Enhancing Conformal Prediction via Class Similarity",
    "abstract": "Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.",
    "authors": [
      "Ariel Fargion",
      "Lahav Dabah",
      "Tom Tirer"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19359v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19359v1",
    "fetched_at": "2025-11-28T08:32:45.447182",
    "chinese_title": "通过类别相似性增强保形预测",
    "chinese_summary": "该论文聚焦保形预测（CP）的增强，针对类别语义分组场景，提出用分组惩罚项优化CP评分函数，理论证明其对分组指标的优势及降低平均预测集大小的效果；进一步提出无需人工语义分区的模型特定变体，再缩小预测集规模；并通过多CP方法、模型及数据集实证验证有效性。",
    "tags": [
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "提出基于类别分区惩罚项的CP评分函数增强策略，理论证明其对分组指标的优势及降低平均预测集大小的效果",
      "提出无需人工语义分区的模型特定CP变体，进一步优化预测集大小并通过实证验证有效性"
    ],
    "processed_at": "2025-11-28T08:39:41.073121"
  },
  {
    "id": "2511.19328v1",
    "title": "Understanding the Staged Dynamics of Transformers in Learning Latent Structure",
    "abstract": "While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.",
    "authors": [
      "Rohan Saha",
      "Farzane Aminmansour",
      "Alona Fyshe"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19328v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19328v1",
    "fetched_at": "2025-11-28T08:32:45.447202",
    "chinese_title": "理解Transformer学习潜在结构的阶段性动态",
    "chinese_summary": "本文使用Alchemy基准，训练仅解码器Transformer并在三类任务（推断缺失规则、组合简单规则解决多步序列、分解复杂例子推断中间步骤）上研究其潜在结构学习的动态；发现模型分阶段掌握能力（先学粗粒度规则再学完整结构），且存在“能稳健组合基本规则但难分解复杂例子发现规则”的关键不对称性，为理解Transformer的潜在结构学习提供了细粒度视角。",
    "tags": [
      "Transformer",
      "Benchmark",
      "Deep Learning",
      "LLM"
    ],
    "key_contributions": [
      "揭示Transformer学习潜在结构时的阶段性动态（先掌握粗粒度规则，再学习完整潜在结构）",
      "发现模型在规则组合与分解能力上的关键不对称性（能稳健组合基本规则，但难以分解复杂例子发现基本规则）"
    ],
    "processed_at": "2025-11-28T08:39:57.502331"
  },
  {
    "id": "2511.19325v1",
    "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
    "abstract": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
    "authors": [
      "Olivia Macmillan-Scott",
      "Roksana Goworek",
      "Eda B. Özyiğit"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19325v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19325v1",
    "fetched_at": "2025-11-28T08:32:45.447223",
    "chinese_title": "多语言大语言模型的生成式查询扩展用于跨语言信息检索",
    "chinese_summary": "该研究采用多语言大语言模型（mLLMs）开展生成式查询扩展（以伪文档生成替代传统同义词增强），评估不同策略及微调变体以识别跨语言检索性能驱动因素；发现查询长度决定有效提示技术、语言脚本差异显著制约跨语言检索、微调仅在训练测试格式匹配时增益明显，强调需更平衡的多语言训练与评估资源。",
    "tags": [
      "LLM",
      "NLP"
    ],
    "key_contributions": [
      "系统评估多语言大语言模型（mLLMs）的生成式查询扩展策略，揭示查询长度、语言脚本差异、训练测试格式匹配度对跨语言检索性能的影响规律",
      "指出跨语言检索的现有不足（不同脚本语言检索效果差、基线较弱语言提升空间大），强调需构建更平衡的多语言训练与评估资源"
    ],
    "processed_at": "2025-11-28T08:40:19.846135"
  },
  {
    "id": "2511.21590v1",
    "title": "An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids",
    "abstract": "Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.",
    "authors": [
      "Muhammad Siddique",
      "Sohaib Zafar"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21590v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21590v1",
    "fetched_at": "2025-11-28T08:32:52.101952",
    "chinese_title": "基于人工智能的智能电网自适应控制混合信息物理框架",
    "chinese_summary": "本文提出基于机器学习的智能电网全流程数字取证框架，整合传感器数据采集、认证通信、云存储与自动化取证分析，采用随机森林、SVM、深度神经网络等算法实现实时异常检测、事件重建及入侵分析；经仿真实验验证，该框架准确可扩展，能抵御数据篡改等网络攻击，云服务适配大数据驱动的取证工作流。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "提出整合多环节的智能电网全流程机器学习数字取证框架，覆盖数据采集到自动化分析全流程",
      "实验验证框架准确可扩展，能抵御多种网络攻击，证明云服务适配智能电网大数据取证工作流"
    ],
    "processed_at": "2025-11-28T08:40:37.725822"
  },
  {
    "id": "2511.21337v1",
    "title": "Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure",
    "abstract": "This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.",
    "authors": [
      "Munish Rathee",
      "Boris Bačić",
      "Maryam Doborjeh"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21337v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21337v1",
    "fetched_at": "2025-11-28T08:32:52.102008",
    "chinese_title": "混合SIFT-SNN用于交通流控制基础设施的高效异常检测",
    "chinese_summary": "论文提出SIFT-SNN框架，整合SIFT空间特征编码、延迟驱动脉冲转换层与LIF脉冲神经网络（SNN），实现交通基础设施结构异常的实时检测；在含6000帧标注（含真实/合成不安全案例）的奥克兰海港大桥数据集上，该框架实现92.3%±0.8%准确率、单帧9.5ms推理时间，低延迟低功耗适配边缘部署，且相比CNN更具可解释性与硬件效率。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出低延迟SIFT-SNN框架，整合空间特征编码与脉冲神经网络，实现交通流控制基础设施结构异常的实时高效检测，单帧推理<10ms、准确率达92.3%±0.8%，适配边缘部署",
      "相比传统CNN，显式保留空间特征grounding，增强可解释性与透明决策能力，且在嵌入式硬件上高效运行；构建含多场景标注的奥克兰海港大桥数据集用于验证"
    ],
    "processed_at": "2025-11-28T08:40:59.515373"
  },
  {
    "id": "2511.21208v1",
    "title": "I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation",
    "abstract": "Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.",
    "authors": [
      "Lucas Thil",
      "Jesse Read",
      "Rim Kaddah",
      "Guillaume Doquet"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21208v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21208v1",
    "fetched_at": "2025-11-28T08:32:52.102039",
    "chinese_title": "I-GLIDE：用于退化估计中潜在健康指标的输入分组",
    "chinese_summary": "该论文针对剩余使用寿命（RUL）预测中健康指标（HI）的不足，首次将RaPP作为HI并证明其优于传统重建误差；提出输入分组范式（I-GLIDE）隔离传感器子集建模特定退化，结合MC dropout和概率潜在空间量化不确定性以提升鲁棒性，在航空航天和制造系统数据上表现更优且具可解释性。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "首次将Reconstruction along Projected Pathways（RaPP）作为剩余使用寿命（RUL）预测的健康指标（HI），性能优于传统重建误差指标",
      "提出输入分组（indicator groups）范式，通过隔离传感器子集建模系统特定退化，形成可解释的I-GLIDE方法，同时结合不确定性量化提升RUL预测鲁棒性"
    ],
    "processed_at": "2025-11-28T08:41:13.859799"
  },
  {
    "id": "2511.20500v1",
    "title": "From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection",
    "abstract": "Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.",
    "authors": [
      "Sidahmed Benabderrahmane",
      "Talal Rahwan"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20500v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20500v1",
    "fetched_at": "2025-11-28T08:32:52.102059",
    "chinese_title": "从一个攻击域到另一个攻击域：基于孪生网络的对比迁移学习用于高级持续威胁检测",
    "chinese_summary": "针对高级持续威胁（APT）检测中传统方法类不平衡、高维特征、跨域迁移性差等问题，论文提出整合迁移学习、可解释AI（SHAP）、对比学习与孪生网络的混合框架——用注意力自动编码器实现跨域知识转移，SHAP筛选稳定特征降维，孪生编码器对齐源-目标表示提升异常可分性；在DARPA TC真实数据及合成攻击场景下，该方法检测性能优于经典与深度基线，具备可扩展、可解释、跨域迁移能力。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出整合迁移学习、XAI、对比学习与孪生网络的混合框架，解决APT检测跨域迁移、高维特征与可解释性问题",
      "通过注意力自动编码器、SHAP特征选择及对比孪生编码器，提升跨域APT检测性能并在真实/合成数据上验证有效"
    ],
    "processed_at": "2025-11-28T08:41:28.298474"
  },
  {
    "id": "2511.20480v1",
    "title": "Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders",
    "abstract": "Advanced Persistent Threats (APTs) pose a significant challenge in cybersecurity due to their stealthy and long-term nature. Modern supervised learning methods require extensive labeled data, which is often scarce in real-world cybersecurity environments. In this paper, we propose an innovative approach that leverages AutoEncoders for unsupervised anomaly detection, augmented by active learning to iteratively improve the detection of APT anomalies. By selectively querying an oracle for labels on uncertain or ambiguous samples, we minimize labeling costs while improving detection rates, enabling the model to improve its detection accuracy with minimal data while reducing the need for extensive manual labeling. We provide a detailed formulation of the proposed Attention Adversarial Dual AutoEncoder-based anomaly detection framework and show how the active learning loop iteratively enhances the model. The framework is evaluated on real-world imbalanced provenance trace databases produced by the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\\% of the data. The datasets span multiple operating systems, including Android, Linux, BSD, and Windows, and cover two attack scenarios. The results have shown significant improvements in detection rates during active learning and better performance compared to other existing approaches.",
    "authors": [
      "Sidahmed Benabderrahmane",
      "James Cheney",
      "Talal Rahwan"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20480v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20480v1",
    "fetched_at": "2025-11-28T08:32:52.102080",
    "chinese_title": "基于主动学习辅助注意力对抗双自动编码器的排序增强异常检测",
    "chinese_summary": "论文针对APT攻击的异常检测问题，提出主动学习辅助的注意力对抗双自动编码器框架，通过选择性查询标注减少成本并提升检测率；在DARPA多操作系统真实不平衡溯源数据上验证，性能优于现有方法。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出主动学习辅助的注意力对抗双自动编码器异常检测框架，平衡标注成本与检测准确率",
      "在多操作系统、低比例APT攻击的真实不平衡数据上验证，性能优于现有方法"
    ],
    "processed_at": "2025-11-28T08:41:37.217760"
  },
  {
    "id": "2511.20088v1",
    "title": "Explainable Visual Anomaly Detection via Concept Bottleneck Models",
    "abstract": "In recent years, Visual Anomaly Detection (VAD) has gained significant attention due to its ability to identify anomalous images using only normal images during training. Many VAD models work without supervision but are still able to provide visual explanations by highlighting the anomalous regions within an image. However, although these visual explanations can be helpful, they lack a direct and semantically meaningful interpretation for users. To address this limitation, we propose extending Concept Bottleneck Models (CBMs) to the VAD setting. By learning meaningful concepts, the network can provide human-interpretable descriptions of anomalies, offering a novel and more insightful way to explain them. Our contributions are threefold: (i) we develop a Concept Dataset to support research on CBMs for VAD; (ii) we improve the CBM architecture to generate both concept-based and visual explanations, bridging semantic and localization interpretability; and (iii) we introduce a pipeline for synthesizing artificial anomalies, preserving the VAD paradigm of minimizing dependence on rare anomalous samples. Our approach, Concept-Aware Visual Anomaly Detection (CONVAD), achieves performance comparable to classic VAD methods while providing richer, concept-driven explanations that enhance interpretability and trust in VAD systems.",
    "authors": [
      "Arianna Stropeni",
      "Valentina Zaccaria",
      "Francesco Borsatti",
      "Davide Dalle Pezze",
      "Manuel Barusco",
      "Gian Antonio Susto"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20088v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20088v1",
    "fetched_at": "2025-11-28T08:32:52.102106",
    "chinese_title": "基于概念瓶颈模型的可解释视觉异常检测",
    "chinese_summary": "本文针对视觉异常检测（VAD）模型解释缺乏语义意义的问题，提出基于概念瓶颈模型（CBM）的CONVAD方法；其核心贡献包括开发VAD用概念数据集、改进CBM架构实现概念与视觉双重解释、引入人工异常合成 pipeline 减少对稀有异常样本的依赖，且性能与经典VAD方法相当并提升可解释性。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "开发支持视觉异常检测（VAD）的概念瓶颈模型（CBM）研究的概念数据集",
      "改进CBM架构以生成概念与视觉双重解释，桥接语义与定位可解释性",
      "引入人工异常合成 pipeline，最小化对稀有异常样本的依赖"
    ],
    "processed_at": "2025-11-28T08:41:49.786657"
  },
  {
    "id": "2511.19246v1",
    "title": "Neural Architecture Search for Quantum Autoencoders",
    "abstract": "In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.   This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.",
    "authors": [
      "Hibah Agha",
      "Samuel Yen-Chi Chen",
      "Huan-Hsin Tseng",
      "Shinjae Yoo"
    ],
    "published": "2025-11-24",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19246v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19246v1",
    "fetched_at": "2025-11-28T08:32:52.102156",
    "chinese_title": "量子自编码器的神经架构搜索",
    "chinese_summary": "本文提出基于遗传算法的神经架构搜索框架，自动化设计量子自编码器的变分量子电路配置，解决量子电路架构设计复杂的问题；在图像数据集上验证方法有效性，为遗传算法应用于量子架构搜索奠定基础，适配噪声近中期量子时代需求。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出基于遗传算法的神经架构搜索框架，自动化优化量子自编码器的变分量子电路配置",
      "在图像数据集上验证方法有效性，为遗传算法应用于量子架构搜索奠定基础"
    ],
    "processed_at": "2025-11-28T08:42:02.688814"
  },
  {
    "id": "2511.19232v1",
    "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations",
    "abstract": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.",
    "authors": [
      "Christos-Nikolaos Zacharopoulos",
      "Revekka Kyriakoglou"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19232v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19232v1",
    "fetched_at": "2025-11-28T08:32:52.102176",
    "chinese_title": "机器N400：精确定位因果语言模型检测语义违规的位置",
    "chinese_summary": "该研究以因果语言模型phi-2为对象，通过精心构建的含合理/不合理结尾句子的语料，分析各层隐藏状态；利用线性探针和有效维度分析发现，语义违规检测在模型低层难区分、中层后准确率提升，且表征子空间先扩后缩，还联系了人类心理语言学的相关发现。",
    "tags": [
      "LLM",
      "NLP",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "揭示因果语言模型phi-2检测语义违规的层分布特征（低层难区分，中层后准确率显著提升）",
      "发现语义违规的表征有效维度变化规律（先扩后缩），并关联人类心理语言学的在线语义处理发现"
    ],
    "processed_at": "2025-11-28T08:42:11.806888"
  },
  {
    "id": "2511.18766v1",
    "title": "Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment",
    "abstract": "Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.",
    "authors": [
      "Xintao Chen",
      "Xiaohao Xu",
      "Bozhong Zheng",
      "Yun Liu",
      "Yingna Wu"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18766v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18766v1",
    "fetched_at": "2025-11-28T08:32:52.102200",
    "chinese_title": "基于渐进单应性引导对齐的无监督多视图视觉异常检测",
    "chinese_summary": "本文提出无监督多视图视觉异常检测框架ViewSense-AD（VSAD），核心通过单应性引导的多视图对齐模块（MVAM）对齐相邻视图特征区域，集成到视图对齐潜在扩散模型（VALDM）实现渐进多阶段对齐，结合融合细化模块（FRM）增强特征一致性，通过对比多级别特征与正常原型记忆库完成异常检测，在RealIAD和MANTA数据集上取得SOTA。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出ViewSense-AD框架，引入单应性引导的多视图对齐模块（MVAM）建模视图间几何一致性，学习视角不变表示",
      "将MVAM集成到视图对齐潜在扩散模型（VALDM）实现渐进多阶段对齐，结合FRM提升特征全局一致性，多视图异常检测任务达SOTA"
    ],
    "processed_at": "2025-11-28T08:42:24.734914"
  },
  {
    "id": "2511.18739v1",
    "title": "A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection",
    "abstract": "Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection.",
    "authors": [
      "Kaixiang Yang",
      "Jiarong Liu",
      "Yupeng Song",
      "Shuanghua Yang",
      "Yujue Zhou"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18739v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18739v1",
    "fetched_at": "2025-11-28T08:32:52.102225",
    "chinese_title": "面向问题的时间序列异常检测评估指标分类",
    "chinese_summary": "本文针对时间序列异常检测评估的挑战，提出面向问题的指标分类框架，将20余种常用指标按6个维度（如基本准确率、时效性、标签不精确容忍度等）分类；通过实验量化各指标区分有效检测与随机噪声的能力，揭示指标适用性需与任务目标对齐，为评估提供统一分析视角。",
    "tags": [
      "Anomaly",
      "Time Series",
      "Benchmark"
    ],
    "key_contributions": [
      "提出面向问题的时间序列异常检测评估指标分类框架，按6个维度（非数学形式）归类20+常用指标",
      "通过实验量化各指标区分有效检测与随机噪声的能力，揭示指标适用性需与任务/应用目标对齐，为评估提供统一分析视角"
    ],
    "processed_at": "2025-11-28T08:42:34.153521"
  },
  {
    "id": "2511.18698v1",
    "title": "Multimodal Real-Time Anomaly Detection and Industrial Applications",
    "abstract": "This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy.",
    "authors": [
      "Aman Verma",
      "Keshav Samdani",
      "Mohd. Samiuddin Shafi"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18698v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18698v1",
    "fetched_at": "2025-11-28T08:32:52.102246",
    "chinese_title": "多模态实时异常检测及其工业应用",
    "chinese_summary": "论文提出集成视频与音频处理的多模态实时异常检测系统，包含轻量版（YOLOv8、ByteTrack、AST）和进阶版（多音频模型集成、混合目标检测、跨模态注意力等）；进阶版结合三类音频模型、双目标检测器及复杂融合机制，在通用监控与工业安全场景中实现实时高性能。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "进阶版通过多音频模型集成、混合目标检测及跨模态融合机制，提升了检测精度、鲁棒性与工业适用性"
    ],
    "processed_at": "2025-11-28T08:42:43.088426"
  },
  {
    "id": "2511.18627v1",
    "title": "Functional Localization Enforced Deep Anomaly Detection Using Fundus Images",
    "abstract": "Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.   On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.",
    "authors": [
      "Jan Benedikt Ruhland",
      "Thorsten Papenbrock",
      "Jan-Peter Sowa",
      "Ali Canbay",
      "Nicole Eter",
      "Bernd Freisleben",
      "Dominik Heider"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18627v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18627v1",
    "fetched_at": "2025-11-28T08:32:52.102289",
    "chinese_title": "基于眼底图像的功能定位强化深度异常检测",
    "chinese_summary": "论文针对眼底图像视网膜疾病检测的质量变异、早期表现细微等挑战，系统评估Vision Transformer（ViT）在多数据集和增强策略下的性能，发现几何与颜色增强稳定提升效果，ViT在Papila数据集AUC优于卷积集成基线；同时开发基于GANomaly的异常检测器，提供重建可解释性并泛化到 unseen 数据，结合GUESS实现概率校准。",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "系统评估ViT在多眼底数据集和增强策略下的视网膜疾病检测性能，明确几何/颜色增强的有效性及ViT优于部分卷积基线的优势",
      "开发具备重建可解释性和泛化能力的GANomaly异常检测器，结合GUESS实现概率校准"
    ],
    "processed_at": "2025-11-28T08:42:51.604718"
  },
  {
    "id": "2511.18172v1",
    "title": "MEDIC: a network for monitoring data quality in collider experiments",
    "abstract": "Data Quality Monitoring (DQM) is a crucial component of particle physics experiments and ensures that the recorded data is of the highest quality, and suitable for subsequent physics analysis. Due to the extreme environmental conditions, unprecedented data volumes, and the sheer scale and complexity of the detectors, DQM orchestration has become a very challenging task. Therefore, the use of Machine Learning (ML) to automate anomaly detection, improve efficiency, and reduce human error in the process of collecting high-quality data is unavoidable. Since DQM relies on real experimental data, it is inherently tied to the specific detector substructure and technology in operation. In this work, a simulation-driven approach to DQM is proposed, enabling the study and development of data-quality methodologies in a controlled environment. Using a modified version of Delphes -- a fast, multi-purpose detector simulation -- the preliminary realization of a framework is demonstrated which leverages ML to identify detector anomalies as well as localize the malfunctioning components responsible. We introduce MEDIC (Monitoring for Event Data Integrity and Consistency), a neural network designed to learn detector behavior and perform DQM tasks to look for potential faults. Although the present implementation adopts a simplified setup for computational ease, where large detector regions are deliberately deactivated to mimic faults, this work represents an initial step toward a comprehensive ML-based DQM framework. The encouraging results underline the potential of simulation-driven studies as a foundation for developing more advanced, data-driven DQM systems for future particle detectors.",
    "authors": [
      "Juvenal Bassa",
      "Arghya Chattopadhyay",
      "Sudhir Malik",
      "Mario Escabi Rivera"
    ],
    "published": "2025-11-22",
    "categories": [
      "hep-ex",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18172v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18172v1",
    "fetched_at": "2025-11-28T08:32:52.102313",
    "chinese_title": "MEDIC：用于对撞机实验数据质量监控的网络",
    "chinese_summary": "论文针对粒子物理实验数据质量监控（DQM）的挑战，提出模拟驱动方法，利用修改后的Delphes探测器模拟工具，开发MEDIC神经网络框架，实现探测器行为学习、异常识别及故障组件定位，为综合DQM系统奠定初步基础。",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出模拟驱动的DQM方法，支持受控环境下数据质量方法研究",
      "开发MEDIC神经网络，实现探测器异常识别与故障组件定位"
    ],
    "processed_at": "2025-11-28T08:42:58.074202"
  },
  {
    "id": "2511.20941v1",
    "title": "Fusion of classical and quantum kernels enables accurate and robust two-sample tests",
    "abstract": "Two-sample tests have been extensively employed in various scientific fields and machine learning such as evaluation on the effectiveness of drugs and A/B testing on different marketing strategies to discriminate whether two sets of samples come from the same distribution or not. Kernel-based procedures for hypothetical testing have been proposed to efficiently disentangle high-dimensional complex structures in data to obtain accurate results in a model-free way by embedding the data into the reproducing kernel Hilbert space (RKHS). While the choice of kernels plays a crucial role for their performance, little is understood about how to choose kernel especially for small datasets. Here we aim to construct a hypothetical test which is effective even for small datasets, based on the theoretical foundation of kernel-based tests using maximum mean discrepancy, which is called MMD-FUSE. To address this, we enhance the MMD-FUSE framework by incorporating quantum kernels and propose a novel hybrid testing strategy that fuses classical and quantum kernels. This approach creates a powerful and adaptive test by combining the domain-specific inductive biases of classical kernels with the unique expressive power of quantum kernels. We evaluate our method on various synthetic and real-world clinical datasets, and our experiments reveal two key findings: 1) With appropriate hyperparameter tuning, MMD-FUSE with quantum kernels consistently improves test power over classical counterparts, especially for small and high-dimensional data. 2) The proposed hybrid framework demonstrates remarkable robustness, adapting to different data characteristics and achieving high test power across diverse scenarios. These results highlight the potential of quantum-inspired and hybrid kernel strategies to build more effective statistical tests, offering a versatile tool for data analysis where sample sizes are limited.",
    "authors": [
      "Yu Terada",
      "Yugo Ogio",
      "Ken Arai",
      "Hiroyuki Tezuka",
      "Yu Tanaka"
    ],
    "published": "2025-11-26",
    "categories": [
      "quant-ph",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20941v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20941v1",
    "fetched_at": "2025-11-28T08:33:02.121432",
    "chinese_title": "经典与量子核的融合实现准确且鲁棒的两样本检验",
    "chinese_summary": "该论文针对小数据集下核选择难题，基于最大均值差异（MMD）核检验框架提出MMD-FUSE方法，通过融合经典核与量子核的混合测试策略，结合经典核的领域归纳偏置和量子核的表达能力；实验验证表明，经超参数调优后，含量子核的MMD-FUSE测试能力显著优于经典核方法，适用于合成及真实临床数据集。",
    "tags": [
      "Deep Learning",
      "Anomaly"
    ],
    "key_contributions": [
      "提出融合经典与量子核的MMD-FUSE混合测试策略，解决小数据集下核选择难题",
      "实验验证含量子核的MMD-FUSE测试能力显著优于经典核方法，适用于合成及真实数据集"
    ],
    "processed_at": "2025-11-28T08:43:18.536681"
  },
  {
    "id": "2511.19818v1",
    "title": "Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana",
    "abstract": "Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.",
    "authors": [
      "Koena Ronny Mabokela",
      "Tim Schlippe",
      "Mpho Raborife",
      "Turgay Celik"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19818v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19818v1",
    "fetched_at": "2025-11-28T08:33:02.121488",
    "chinese_title": "基于远程监督的语言无关情感标注：以英语、Sepedi语和Setswana语为例",
    "chinese_summary": "本文针对非洲低资源语言缺乏情感标注数据的问题，提出一种基于情感表情和词汇的自动语言无关情感标注方法；利用SAfriSenti南非多语言情感语料实验，英语、Sepedi语和Setswana语标注准确率分别达66%、69%和63%，平均仅需修正34%自动标注结果，有效降低手动标注成本。",
    "tags": [
      "NLP",
      "Sentiment Analysis"
    ],
    "key_contributions": [
      "提出基于情感表情和词汇的自动语言无关情感标注方法，适配低资源非洲语言",
      "实验验证方法有效性，大幅减少手动标注工作量（平均仅需修正34%）"
    ],
    "processed_at": "2025-11-28T08:43:36.295579"
  },
  {
    "id": "2511.21465v1",
    "title": "Ensemble Performance Through the Lens of Linear Independence of Classifier Votes in Data Streams",
    "abstract": "Ensemble learning improves classification performance by combining multiple base classifiers. While increasing the number of classifiers generally enhances accuracy, excessively large ensembles can lead to computational inefficiency and diminishing returns. This paper investigates the relationship between ensemble size and performance through the lens of linear independence among classifier votes in data streams. We propose that ensembles composed of linearly independent classifiers maximize representational capacity, particularly under a geometric model. We then generalize the importance of linear independence to the weighted majority voting problem. By modeling the probability of achieving linear independence among classifier outputs, we derive a theoretical framework that explains the trade-off between ensemble size and accuracy. Our analysis leads to a theoretical estimate of the ensemble size required to achieve a user-specified probability of linear independence. We validate our theory through experiments on both real-world and synthetic datasets using two ensemble methods, OzaBagging and GOOWE. Our results confirm that this theoretical estimate effectively identifies the point of performance saturation for robust ensembles like OzaBagging. Conversely, for complex weighting schemes like GOOWE, our framework reveals that high theoretical diversity can trigger algorithmic instability. Our implementation is publicly available to support reproducibility and future research.",
    "authors": [
      "Enes Bektas",
      "Fazli Can"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21465v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21465v1",
    "fetched_at": "2025-11-28T08:33:12.225791",
    "chinese_title": "数据流中分类器投票线性独立性视角下的集成性能研究",
    "chinese_summary": "本文从数据流中分类器投票的线性独立性视角，研究集成规模与性能的关系，推导了达到指定线性独立概率所需集成规模的理论估计；通过OzaBagging和GOOWE等集成方法在真实及合成数据集上验证，发现该估计可有效识别鲁棒集成的性能饱和点，且复杂加权方案的高理论多样性可能引发算法不稳定。",
    "tags": [
      "Algorithmic Trading",
      "Time Series",
      "Anomaly"
    ],
    "key_contributions": [
      "提出线性独立分类器构成的集成可最大化表征能力，推导了数据流中达到指定线性独立概率所需集成规模的理论估计，解释集成规模与性能的权衡关系",
      "通过实验验证该理论估计能有效识别OzaBagging等鲁棒集成的性能饱和点，并揭示GOOWE等复杂加权方案中高理论多样性引发算法不稳定的现象"
    ],
    "processed_at": "2025-11-28T08:44:02.164647"
  },
  {
    "id": "2511.21342v1",
    "title": "Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures",
    "abstract": "Separating the individual elements in a musical mixture is an essential process for music analysis and practice. While this is generally addressed using neural networks optimized to mask or transform the time-frequency representation of a mixture to extract the target sources, the flexibility and generalization capabilities of generative diffusion models are giving rise to a novel class of solutions for this complicated task. In this work, we explore singing voice separation from real music recordings using a diffusion model which is trained to generate the solo vocals conditioned on the corresponding mixture. Our approach improves upon prior generative systems and achieves competitive objective scores against non-generative baselines when trained with supplementary data. The iterative nature of diffusion sampling enables the user to control the quality-efficiency trade-off, and also refine the output when needed. We present an ablation study of the sampling algorithm, highlighting the effects of the user-configurable parameters.",
    "authors": [
      "Genís Plaja-Roglans",
      "Yun-Ning Hung",
      "Xavier Serra",
      "Igor Pereira"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21342v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21342v1",
    "fetched_at": "2025-11-28T08:33:12.225815",
    "chinese_title": "基于音乐混合条件的扩散模型分离人声",
    "chinese_summary": "本文提出以音乐混合音频为条件的扩散模型，用于从真实录音中分离独唱人声；该模型改进现有生成系统，补充数据训练后客观指标与非生成基线相当，且扩散采样迭代性支持用户控制质量-效率权衡及输出优化。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出以音乐混合为条件的扩散模型，实现真实录音的人声分离，改进现有生成系统",
      "扩散采样迭代特性支持用户灵活控制质量-效率权衡，补充数据训练后达竞争基线水平"
    ],
    "processed_at": "2025-11-28T08:44:18.908696"
  },
  {
    "id": "2511.21050v1",
    "title": "Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs",
    "abstract": "Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.",
    "authors": [
      "Dongkyu Derek Cho",
      "Huan Song",
      "Arijit Ghosh Chowdhury",
      "Haotian An",
      "Yawei Wang",
      "Rohit Thekkanal",
      "Negin Sokhandan",
      "Sharlina Keshava",
      "Hannah Marlowe"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21050v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21050v1",
    "fetched_at": "2025-11-28T08:33:12.225847",
    "chinese_title": "打破安全-能力权衡：带可验证奖励的强化学习在大语言模型中维持安全护栏",
    "chinese_summary": "论文指出大语言模型微调存在安全-能力权衡，现有SFT、RLHF等方法无法避免；首次对带可验证奖励的强化学习（RLVR）进行理论与实证分析，推导KL约束下安全漂移上界并证明消除安全退化的条件，实验显示RLVR可同时提升推理能力与安全护栏，挑战了必然权衡的假设。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "首次全面理论与实证分析RLVR的安全特性，推导KL约束下安全漂移上界并证明消除安全退化的条件",
      "实验验证RLVR可同时提升推理能力并维持/改善安全护栏，打破安全-能力权衡的普遍假设"
    ],
    "processed_at": "2025-11-28T08:44:27.320896"
  },
  {
    "id": "2511.20977v1",
    "title": "Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems",
    "abstract": "Efficiency and reliability are both crucial for energy management, especially in multi-microgrid systems (MMSs) integrating intermittent and distributed renewable energy sources. This study investigates an economic and reliable energy management problem in MMSs under a distributed scheme, where each microgrid independently updates its energy management policy in a decentralized manner to optimize the long-term system performance collaboratively. We introduce the mean and variance of the exchange power between the MMS and the main grid as indicators for the economic performance and reliability of the system. Accordingly, we formulate the energy management problem as a mean-variance team stochastic game (MV-TSG), where conventional methods based on the maximization of expected cumulative rewards are unsuitable for variance metrics. To solve MV-TSGs, we propose a fully distributed independent policy gradient algorithm, with rigorous convergence analysis, for scenarios with known model parameters. For large-scale scenarios with unknown model parameters, we further develop a deep reinforcement learning algorithm based on independent policy gradients, enabling data-driven policy optimization. Numerical experiments in two scenarios validate the effectiveness of the proposed methods. Our approaches fully leverage the distributed computational capabilities of MMSs and achieve a well-balanced trade-off between economic performance and operational reliability.",
    "authors": [
      "Junkai Hu",
      "Li Xia"
    ],
    "published": "2025-11-26",
    "categories": [
      "eess.SY",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20977v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20977v1",
    "fetched_at": "2025-11-28T08:33:12.225868",
    "chinese_title": "独立策略梯度强化学习在多微电网系统经济可靠能源管理中的应用",
    "chinese_summary": "针对多微电网系统（MMS）的经济可靠能源管理问题，本文将其建模为均值-方差团队随机博弈（MV-TSG），提出已知参数下的分布式独立策略梯度算法（含收敛分析）及未知参数下的数据驱动深度强化学习方法；数值实验验证方法可平衡经济性能与运行可靠性，充分利用MMS分布式计算能力。",
    "tags": [
      "Reinforcement Learning",
      "Risk Management",
      "Deep Learning",
      "Portfolio Optimization"
    ],
    "key_contributions": [
      "构建均值-方差团队随机博弈模型，刻画多微电网系统经济性能（交换功率均值）与可靠性（交换功率方差）的协同优化问题",
      "提出分布式独立策略梯度算法（含收敛分析）及未知参数下的深度强化学习方法，实现数据驱动的能源管理策略优化"
    ],
    "processed_at": "2025-11-28T08:44:42.907353"
  },
  {
    "id": "2511.20909v1",
    "title": "Evolved SampleWeights for Bias Mitigation: Effectiveness Depends on Optimization Objectives",
    "abstract": "Machine learning models trained on real-world data may inadvertently make biased predictions that negatively impact marginalized communities. Reweighting is a method that can mitigate such bias in model predictions by assigning a weight to each data point used during model training. In this paper, we compare three methods for generating these weights: (1) evolving them using a Genetic Algorithm (GA), (2) computing them using only dataset characteristics, and (3) assigning equal weights to all data points. Model performance under each strategy was evaluated using paired predictive and fairness metrics, which also served as optimization objectives for the GA during evolution. Specifically, we used two predictive metrics (accuracy and area under the Receiver Operating Characteristic curve) and two fairness metrics (demographic parity difference and subgroup false negative fairness). Using experiments on eleven publicly available datasets (including two medical datasets), we show that evolved sample weights can produce models that achieve better trade-offs between fairness and predictive performance than alternative weighting methods. However, the magnitude of these benefits depends strongly on the choice of optimization objectives. Our experiments reveal that optimizing with accuracy and demographic parity difference metrics yields the largest number of datasets for which evolved weights are significantly better than other weighting strategies in optimizing both objectives.",
    "authors": [
      "Anil K. Saini",
      "Jose Guadalupe Hernandez",
      "Emily F. Wong",
      "Debanshi Misra",
      "Jason H. Moore"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20909v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20909v1",
    "fetched_at": "2025-11-28T08:33:12.225893",
    "chinese_title": "用于偏差缓解的进化样本权重：有效性取决于优化目标",
    "chinese_summary": "本文比较遗传算法进化样本权重、基于数据集特征计算权重及等权重三种方法，以预测（准确率、AUC）和公平性（人口 parity差、 subgroup假阴性公平）指标为优化目标开展实验；发现进化权重可实现更优的公平-预测性能权衡，但有效性依赖优化目标，其中准确率与人口 parity差的组合在最多数据集上优于其他策略。",
    "tags": [
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "提出遗传算法进化样本权重方法，证明其在公平性与预测性能权衡上优于等权重及基于数据集特征的权重方法",
      "揭示进化样本权重的有效性强烈依赖优化目标选择，其中准确率+人口 parity差的组合在最多数据集上表现最优"
    ],
    "processed_at": "2025-11-28T08:45:05.188484"
  },
  {
    "id": "2511.19849v1",
    "title": "Reinforcement Learning with $ω$-Regular Objectives and Constraints",
    "abstract": "Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.   We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.",
    "authors": [
      "Dominik Wagner",
      "Leon Witzman",
      "Luke Ong"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19849v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19849v1",
    "fetched_at": "2025-11-28T08:33:12.225917",
    "chinese_title": "带ω-正则目标与约束的强化学习",
    "chinese_summary": "针对强化学习（RL）标量奖励难以表达时序/安全目标、易奖励 hacking，且单标量掩盖风险-性能 trade-off的问题，论文结合ω-正则目标与显式约束，分开处理安全要求和优化目标；提出基于线性规划的模型基RL算法，极限下生成最大化ω-正则目标满足概率且满足约束阈值的策略，并建立到带最优性保证的约束极限平均问题的转换。",
    "tags": [
      "Reinforcement Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "提出结合ω-正则目标与显式约束的RL框架，解决标量奖励局限及风险-性能 trade-off问题",
      "开发基于线性规划的模型基RL算法，保证极限下生成满足约束的最优策略，且建立到约束极限平均问题的最优性转换"
    ],
    "processed_at": "2025-11-28T08:45:13.509194"
  },
  {
    "id": "2511.18876v1",
    "title": "Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification",
    "abstract": "The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs.",
    "authors": [
      "Lilian Say",
      "Christophe Denis",
      "Rafael Pinot"
    ],
    "published": "2025-11-24",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18876v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18876v1",
    "fetched_at": "2025-11-28T08:33:12.225939",
    "chinese_title": "公平性与隐私的融合：多分类中差分隐私与人口统计均等的整合",
    "chinese_summary": "本文挑战隐私与公平性常被视为冲突的观点，设计后处理算法DP2DP，同时满足差分隐私与人口统计均等；理论分析显示其收敛速率仅比非私有公平方法多对数因子，实验验证了最优的准确率、公平性与隐私三目标权衡。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "1. 提出后处理算法DP2DP，实现多分类任务中差分隐私与人口统计均等的同时保障；2. 理论证明算法收敛速率接近非私有公平方法，实验验证最优准确率/公平性/隐私权衡。"
    ],
    "processed_at": "2025-11-28T08:45:38.815154"
  },
  {
    "id": "2511.18567v1",
    "title": "In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm",
    "abstract": "The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of \"goodness\", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \\texttt{game\\_theoretic\\_local} achieved 97.15\\% accuracy on MNIST, \\texttt{softmax\\_energy\\_margin\\_local} reached 82.84\\% on FashionMNIST, and \\texttt{triplet\\_margin\\_local} attained 37.69\\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \\href{https://github.com/aryashah2k/In-Search-of-Goodness}{Github} for reference and reproducibility.",
    "authors": [
      "Arya Shah",
      "Vaibhav Tripathi"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18567v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18567v1",
    "fetched_at": "2025-11-28T08:33:12.225977",
    "chinese_title": "探寻优度函数：前向-前向算法优度函数的大规模基准测试",
    "chinese_summary": "该论文针对前向-前向（FF）算法的优度函数，在4个标准图像数据集上对21种不同优度函数进行大规模基准测试，评估分类准确率、能耗及碳足迹；发现部分替代优度函数显著优于默认平方和指标，且存在预测性能与环境成本的权衡，证明优度函数是FF设计的关键超参数，并开源代码供复现。",
    "tags": [
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "首次大规模基准测试前向-前向算法的21种优度函数，验证部分替代函数优于默认平方和指标",
      "揭示优度函数的预测性能与计算效率（能耗、碳足迹）的权衡关系，明确其为FF设计的核心超参数并开源代码"
    ],
    "processed_at": "2025-11-28T08:46:01.872576"
  },
  {
    "id": "2511.18181v1",
    "title": "MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning",
    "abstract": "This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.",
    "authors": [
      "Adam Callaghan",
      "Karl Mason",
      "Patrick Mannion"
    ],
    "published": "2025-11-22",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18181v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18181v1",
    "fetched_at": "2025-11-28T08:33:12.225998",
    "chinese_title": "MOMA-AC：连续多目标多智能体强化学习的偏好驱动演员-评论员框架",
    "chinese_summary": "本文针对连续状态与动作空间的多目标多智能体强化学习（MOMARL）空白，提出首个内循环演员-评论员框架MOMA-AC，基于TD3/DDPG实例化为MOMA-TD3/DDPG，结合多头演员、集中式评论员与偏好条件架构，可编码冲突目标下最优策略的帕累托前沿；还构建连续MOMARL测试套件，在合作任务中显著提升预期效用与超体积，且随智能体数量增加可稳定扩展。",
    "tags": [
      "Reinforcement Learning",
      "Deep Learning",
      "Financial Agent"
    ],
    "key_contributions": [
      "提出首个针对连续状态/动作空间MOMARL的内循环演员-评论员框架MOMA-AC，能编码冲突目标下最优策略的帕累托前沿",
      "构建连续MOMARL测试套件，在合作任务中显著提升性能且随智能体数量稳定扩展"
    ],
    "processed_at": "2025-11-28T08:46:20.854073"
  },
  {
    "id": "2511.21638v1",
    "title": "Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO",
    "abstract": "Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.",
    "authors": [
      "Daniel R. Jiang",
      "Jalaj Bhandari",
      "Yukai Yang",
      "Rémi Munos",
      "Tyler Lu"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21638v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21638v1",
    "fetched_at": "2025-11-28T08:33:15.569760",
    "chinese_title": "使用迭代PPO对齐大语言模型以实现多轮对话结果",
    "chinese_summary": "针对多轮对话强化学习（RL）中稀疏长 horizon 奖励及响应与token级生成差异的挑战，论文将多轮RL问题简化为一系列单轮RLHF式问题，用学习到的多轮Q函数作为单轮奖励模型，并证明标准token级PPO解决单轮问题等价于多轮问题的策略改进步骤；基于此提出迭代PPO算法，交替拟合Q函数与改进策略，直接利用现成单轮RLHF工具，兼具在线更新适应性与离线训练稳定性。",
    "tags": [
      "LLM",
      "NLP",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "证明了以多轮Q函数为奖励模型的单轮PPO等价于多轮RL的策略改进步骤",
      "提出迭代PPO算法，兼具在线更新适应性与离线训练稳定性且易实现"
    ],
    "processed_at": "2025-11-28T08:46:32.876878"
  },
  {
    "id": "2511.21669v1",
    "title": "DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving",
    "abstract": "Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.",
    "authors": [
      "Fengze Yu",
      "Leshu Li",
      "Brad McDanel",
      "Saiqian Zhang"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21669v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21669v1",
    "fetched_at": "2025-11-28T08:33:22.371477",
    "chinese_title": "DSD：一种用于边云敏捷大模型服务的分布式推测解码方案",
    "chinese_summary": "针对大语言模型（LLM）推理延迟高、边云异构环境扩展性有限的问题，现有推测解码（SD）仅支持单节点，论文提出DSD分布式框架通过协调草稿-目标执行扩展到多设备；同时设计DSD-Sim模拟器捕捉网络等动态，并提出自适应窗口控制（AWC）策略优化吞吐量；实验表明DSD比现有SD基线最多提升1.1倍速度和9.7%吞吐量，实现边云敏捷可扩展LLM服务。",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出DSD分布式推测解码框架，突破单节点局限，支持边云异构环境下多设备LLM推理加速",
      "设计DSD-Sim离散事件模拟器与自适应窗口控制（AWC）策略，优化推测解码的吞吐量与效率"
    ],
    "processed_at": "2025-11-28T08:46:50.537307"
  },
  {
    "id": "2511.21635v1",
    "title": "Mechanisms of Non-Monotonic Scaling in Vision Transformers",
    "abstract": "Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.",
    "authors": [
      "Anantha Padmanaban Krishna Kumar"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21635v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21635v1",
    "fetched_at": "2025-11-28T08:33:22.371506",
    "chinese_title": "视觉Transformer中非单调缩放的机制",
    "chinese_summary": "本文针对视觉Transformer（ViT）深层性能劣于浅层的问题，通过对ViT-S/B/L在ImageNet上的实证分析，发现其表示演化存在“悬崖-平台-攀升”三阶段模式；提出信息混合指数量化信息扩散，揭示深层ViT额外层多为信息扩散而非任务提升，指出架构应注重深度校准而非单纯增参，该指数可用于模型诊断与未来设计。",
    "tags": [
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "揭示视觉Transformer深度演化的“悬崖-平台-攀升”三阶段模式，解释深层模型性能劣于浅层的机制",
      "提出信息混合指数量化信息扩散，为模型诊断与架构设计提供有效工具"
    ],
    "processed_at": "2025-11-28T08:47:03.583989"
  },
  {
    "id": "2511.21591v1",
    "title": "On the Limits of Innate Planning in Large Language Models",
    "abstract": "Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.",
    "authors": [
      "Charles Schepanowski",
      "Charles Ling"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21591v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21591v1",
    "fetched_at": "2025-11-28T08:33:22.371526",
    "chinese_title": "论大语言模型天生规划能力的局限性",
    "chinese_summary": "该研究以经典8-puzzle任务（无需外部工具）测试大语言模型（LLM）的规划与状态推理能力，在不同提示条件及反馈下评估多模型表现；发现无外部工具时LLM规划能力存在显著局限（状态表示脆弱、启发式规划弱），指出需显式状态维护与结构化搜索机制提升规划能力。",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark"
    ],
    "key_contributions": [
      "通过8-puzzle任务明确当前LLM无外部工具时天生规划能力的核心局限（状态表示脆弱、启发式规划弱）",
      "指出LLM提升规划能力需引入显式状态维护与结构化搜索机制"
    ],
    "processed_at": "2025-11-28T08:47:12.548500"
  },
  {
    "id": "2511.21572v1",
    "title": "BAMAS: Structuring Budget-Aware Multi-Agent Systems",
    "abstract": "Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.",
    "authors": [
      "Liming Yang",
      "Junyu Luo",
      "Xuanzhe Liu",
      "Yiling Lou",
      "Zhenpeng Chen"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21572v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21572v1",
    "fetched_at": "2025-11-28T08:33:22.371552",
    "chinese_title": "BAMAS：构建预算感知型多智能体系统",
    "chinese_summary": "该论文针对现有LLM多智能体系统缺乏显式预算约束下结构设计的问题，提出BAMAS方法——先通过整数线性规划选择平衡性能与成本的最优LLM集合，再用强化学习确定智能体交互拓扑并实例化执行；实验表明BAMAS在保持相当性能的同时，成本最多降低86%。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "NLP"
    ],
    "key_contributions": [
      "提出预算感知型多智能体系统BAMAS，解决显式预算约束下的结构设计问题",
      "提出整数线性规划+强化学习的方法，实现性能与成本的平衡，成本最多降低86%且性能相当"
    ],
    "processed_at": "2025-11-28T08:47:27.608618"
  },
  {
    "id": "2511.21460v1",
    "title": "MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning",
    "abstract": "Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.",
    "authors": [
      "Junjian Wang",
      "Lidan Zhao",
      "Xi Sheryl Zhang"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21460v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21460v1",
    "fetched_at": "2025-11-28T08:33:22.371573",
    "chinese_title": "MADRA：面向风险感知具身规划的多智能体辩论框架",
    "chinese_summary": "针对具身AI任务规划中现有方法计算成本高或过度拒绝的问题，提出无训练的MADRA多智能体辩论风险评估框架，通过多个LLM代理辩论指令安全性、关键评估者打分及迭代共识投票，减少误拒并保持危险任务敏感性；还引入分层认知协作规划框架提升任务成功率，贡献SafeAware-VH基准数据集，实验验证其性能优于现有方法。",
    "tags": [
      "LLM",
      "Risk Management",
      "Benchmark"
    ],
    "key_contributions": [
      "提出无训练的MADRA多智能体辩论风险评估框架，通过集体推理提升安全性且降低安全任务误拒率",
      "引入分层认知协作规划框架及SafeAware-VH基准数据集，实验验证方法在安全感知任务规划中的优势"
    ],
    "processed_at": "2025-11-28T08:47:38.776280"
  },
  {
    "id": "2511.21438v1",
    "title": "Conversational no-code and multi-agentic disease module identification and drug repurposing prediction with ChatDRex",
    "abstract": "Repurposing approved drugs offers a time-efficient and cost-effective alternative to traditional drug development. However, in silico prediction of repurposing candidates is challenging and requires the effective collaboration of specialists in various fields, including pharmacology, medicine, biology, and bioinformatics. Fragmented, specialized algorithms and tools often address only narrow aspects of the overall problem, and heterogeneous, unstructured data landscapes require specialized users to be involved. Hence, these data services do not integrate smoothly across workflows. With ChatDRex, we present a conversation-based, multi-agent system that facilitates the execution of complex bioinformatic analyses aiming for network-based drug repurposing prediction. It builds on the integrated systems medicine knowledge graph NeDRex. ChatDRex provides natural language access to its extensive biomedical KG and integrates bioinformatics agents for network analysis and drug repurposing, complemented by agents for functional coherence evaluation for in silico validation, as well as agents for literature mining and for discussing the obtained results in a scientific context. Its flexible multi-agent design assigns specific tasks to specialized agents, including query routing, data retrieval, algorithm execution, and result visualization. A dedicated reasoning module keeps the user in the loop and allows for hallucination detection. By enabling physicians and researchers without computer science expertise to control complex analyses in natural language, ChatDRex democratizes access to bioinformatics as an important resource for drug repurposing. It enables clinical experts to generate hypotheses and explore drug repurposing opportunities, ultimately accelerating the discovery of novel therapies and advancing personalized medicine and translational research.",
    "authors": [
      "Simon Süwer",
      "Kester Bagemihl",
      "Sylvie Baier",
      "Lucia Dicunta",
      "Markus List",
      "Jan Baumbach",
      "Andreas Maier",
      "Fernando M. Delgado-Chaves"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21438v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21438v1",
    "fetched_at": "2025-11-28T08:33:22.371603",
    "chinese_title": "基于ChatDRex的对话式无代码多智能体疾病模块识别与药物重定位预测",
    "chinese_summary": "论文提出对话式无代码多智能体系统ChatDRex，基于集成系统医学知识图谱NeDRex，整合生物信息学代理（网络分析、药物重定位、功能验证等）与推理模块（防幻觉）；解决传统药物重定位预测中工具碎片化、数据异构、需专业用户等问题，支持非计算背景研究者通过自然语言完成复杂分析。",
    "tags": [
      "LLM",
      "NLP",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "1. 构建对话式无代码多智能体系统ChatDRex，整合多类生物信息学代理与推理模块，实现端到端药物重定位预测分析；",
      "2. 基于集成系统医学知识图谱NeDRex，支持非计算背景用户通过自然语言访问与分析，打破专业工具使用壁垒。"
    ],
    "processed_at": "2025-11-28T08:47:56.117981"
  },
  {
    "id": "2511.21408v1",
    "title": "Subjective Depth and Timescale Transformers: Learning Where and When to Compute",
    "abstract": "The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.",
    "authors": [
      "Frederico Wieser",
      "Martin Benfeghoul",
      "Haitham Bou Ammar",
      "Jun Wang",
      "Zafeirios Fountas"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IT"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21408v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21408v1",
    "fetched_at": "2025-11-28T08:33:22.371628",
    "chinese_title": "主观深度与时标Transformer：学习在何处及何时计算",
    "chinese_summary": "针对标准Transformer计算分配僵化统一限制效率可扩展性的问题，论文提出主观深度Transformer（SDT）和时标Transformer（STT）两种架构，利用贝叶斯惊喜信号动态路由计算：SDT通过交替决策层与动态层基于惊喜做Top-K路由，STT扩展到时间域用过渡网络动态执行/绕过块；两者训练中呈现从 novelty到prediction驱动门控的变化，且自注意力计算减少75%、KV缓存需求减少50%。",
    "tags": [
      "Transformer",
      "Deep Learning",
      "LLM",
      "Time Series"
    ],
    "key_contributions": [
      "提出基于贝叶斯惊喜信号动态路由计算的SDT和STT两种Transformer架构，优化计算分配效率",
      "实现显著计算效率提升（自注意力降75%、KV缓存降50%），并揭示训练中门控机制的变化规律"
    ],
    "processed_at": "2025-11-28T08:48:08.974561"
  },
  {
    "id": "2511.21402v1",
    "title": "Text-to-SQL as Dual-State Reasoning: Integrating Adaptive Context and Progressive Generation",
    "abstract": "Recent divide-and-conquer reasoning approaches, particularly those based on Chain-of-Thought (CoT), have substantially improved the Text-to-SQL capabilities of Large Language Models (LLMs). However, when applied to complex enterprise databases, such methods struggle to maintain coherent reasoning due to limited context capacity, unreliable schema linking, and weak grounding in database semantics. To overcome these issues, we introduce DSR-SQL, a \\textbf{D}ual-\\textbf{S}tate \\textbf{R}easoning framework that models Text-to-SQL as an interaction between an adaptive context state and a progressive generation state. The first constructs a compact, semantically faithful environment by refining large schemas and selecting relevant structures, while the second formalizes SQL synthesis as feedback-guided state transitions, enabling the model to self-correct and align with user intent. Without any post-training or in-context examples, DSR-SQL achieves competitive performance, reaching 35.28\\% execution accuracy on Spider 2.0-Snow and 68.32\\% on BIRD development set. Our implementation will be open-sourced at: https://github.com/DMIRLAB-Group/DSR-SQL.",
    "authors": [
      "Zhifeng Hao",
      "Qibin Song",
      "Ruichu Cai",
      "Boyan Xu"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21402v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21402v1",
    "fetched_at": "2025-11-28T08:33:22.371655",
    "chinese_title": "双状态推理的文本转SQL：融合自适应上下文与渐进式生成",
    "chinese_summary": "针对现有分治推理方法在复杂企业数据库文本转SQL中存在的上下文容量有限、schema链接不可靠及语义 grounding弱的问题，论文提出DSR-SQL双状态推理框架，通过自适应上下文状态精炼schema构建紧凑语义环境，渐进生成状态以反馈引导状态转移实现自纠正；无需后训练或上下文示例，在Spider2.0-Snow和BIRD开发集上取得竞争力性能。",
    "tags": [
      "LLM",
      "NLP",
      "Transformer"
    ],
    "key_contributions": [
      "提出DSR-SQL双状态推理框架，建模文本转SQL为自适应上下文与渐进生成状态的交互，解决复杂数据库中的上下文、schema链接及语义问题",
      "无需后训练或上下文示例，在Spider2.0-Snow和BIRD开发集上取得竞争力执行准确率，且开源实现"
    ],
    "processed_at": "2025-11-28T08:48:24.231808"
  },
  {
    "id": "2511.21398v1",
    "title": "Prune4Web: DOM Tree Pruning Programming for Web Agent",
    "abstract": "Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.",
    "authors": [
      "Jiayuan Zhang",
      "Kaiquan Chen",
      "Zhihao Lu",
      "Enshen Zhou",
      "Qian Yu",
      "Jing Zhang"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21398v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21398v1",
    "fetched_at": "2025-11-28T08:33:22.371681",
    "chinese_title": "Prune4Web：面向Web Agent的DOM树剪枝编程",
    "chinese_summary": "针对基于大语言模型（LLM）的Web Agent处理复杂DOM结构时存在的信息丢失或效率不足问题，提出Prune4Web范式，通过让LLM生成可执行Python评分脚本动态过滤DOM元素，实现25-50倍候选元素减少；同时设计专用数据标注 pipeline和两回合对话训练策略，联合优化模型性能。",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出DOM树剪枝编程方法，让LLM生成可执行脚本动态过滤DOM元素，大幅减少候选元素（25-50倍），平衡精度与可扩展性",
      "设计专用数据标注 pipeline和两回合对话训练策略，联合优化Planner与Programmatic Filter模块"
    ],
    "processed_at": "2025-11-28T08:48:37.330246"
  },
  {
    "id": "2511.21232v1",
    "title": "RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI",
    "abstract": "The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.",
    "authors": [
      "Muhammed Yildirim",
      "Ozcan Ozturk"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21232v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21232v1",
    "fetched_at": "2025-11-28T08:33:22.371701",
    "chinese_title": "基于RISC-V的边缘AI深度可分离卷积TinyML加速器",
    "chinese_summary": "本文针对轻量CNN中深度可分离卷积（DSC）的层间数据传输内存瓶颈，提出一种基于RISC-V的融合逐像素数据流硬件加速器架构（作为自定义功能单元CFU），消除中间缓冲区，数据移动减少达87%；经FPGA和ASIC评估，该架构比RISC-V软件执行提速最多59.3x，且面积功耗表现优异，验证了TinyML资源范围内零缓冲区数据流的可行性。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出融合逐像素数据流的RISC-V CFU架构，消除DSC中间缓冲区，大幅降低数据移动开销",
      "通过FPGA和ASIC评估验证了该架构在TinyML资源约束下的高性能（提速、低功耗面积）可行性"
    ],
    "processed_at": "2025-11-28T08:48:49.205957"
  },
  {
    "id": "2511.21192v1",
    "title": "When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models",
    "abstract": "Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.",
    "authors": [
      "Hui Lu",
      "Yi Yu",
      "Yiming Yang",
      "Chenyu Yi",
      "Qixin Zhang",
      "Bingquan Shen",
      "Alex C. Kot",
      "Xudong Jiang"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21192v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21192v1",
    "fetched_at": "2025-11-28T08:33:22.371730",
    "chinese_title": "当机器人服从补丁：视觉-语言-动作模型的通用可迁移对抗补丁攻击",
    "chinese_summary": "现有视觉-语言-动作（VLA）模型的对抗攻击中，通用可迁移补丁研究不足（多过拟合单模型）；论文提出UPA-RFAS框架，结合特征空间目标、鲁棒增强两阶段优化及VLA特有的补丁注意力支配与语义错位损失，学习单个物理补丁，可跨模型、任务、视角迁移，暴露攻击面并建立防御基线。",
    "tags": [
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "针对VLA模型通用可迁移对抗补丁的研究空白，提出UPA-RFAS框架，实现单个物理补丁跨未知架构、微调变体及sim-to-real场景的有效攻击",
      "设计融合特征空间优化、鲁棒两阶段min-max过程及VLA特有的注意力/语义损失的统一方法，实验验证其跨模型、任务、视角的迁移性，建立防御基线"
    ],
    "processed_at": "2025-11-28T08:49:08.486360"
  },
  {
    "id": "2511.21104v1",
    "title": "BRIDGE: Building Representations In Domain Guided Program Verification",
    "abstract": "Large language models (LLMs) have achieved impressive results in code generation, yet struggle with program verification, especially in interactive proof frameworks such as Lean4. A central challenge is scalability: verified synthesis requires not just code, but also precise specifications and correctness proofs, and existing approaches rarely span all three domains. We present BRIDGE, the first systematic study of structured prompting for scalable verified program generation. BRIDGE decomposes verification into three interconnected domains: Code (executable implementations), Specifications (formal intent statements), and Proofs (constructive correctness arguments). Our key idea is to elicit distinct reasoning behaviors functional, specification-driven, and proof-oriented as intermediate representations that preserve semantic structure and connect these domains. Through systematic ablations, we show that this approach substantially improves both accuracy and efficiency beyond standard error feedback methods. For example, functional reasoning improves correctness of code in formal languages (Lean4) by nearly 1.5x (pass@5) over direct baselines. In inference-time compute, functional reasoning is also 2x more efficient, achieving higher pass rates with fewer generations and lower total sampling budgets. Similarly, we find that specification-driven prompting boosts Python coding pass rates by up to 17.5%. These findings suggest that structured domain alignment is a promising direction for advancing verified synthesis. BRIDGE establishes a foundation for training via expert iteration or RLVR, enabling models to internalize these reasoning strategies across code, specifications, and proofs.",
    "authors": [
      "Robert Joseph George",
      "Carson Eisenach",
      "Udaya Ghai",
      "Dominique Perrault-Joncas",
      "Anima Anandkumar",
      "Dean Foster"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21104v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21104v1",
    "fetched_at": "2025-11-28T08:33:22.371755",
    "chinese_title": "BRIDGE：领域引导的程序验证中的表示构建",
    "chinese_summary": "论文提出BRIDGE方法，首次系统研究结构化提示用于可扩展的验证程序生成；该方法将验证分解为代码、规格说明和证明三个互联领域，通过引出功能、规格驱动和证明导向三种推理行为作为中间表示连接各领域，实验表明其在准确性和效率上显著优于标准错误反馈方法。",
    "tags": [
      "LLM",
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "提出BRIDGE方法，首次系统研究结构化提示用于可扩展的验证程序生成，将验证任务分解为代码、规格说明和证明三个互联领域；",
      "通过引出功能、规格驱动和证明导向三种不同推理行为作为中间表示，显著提升验证程序生成的准确性（如Lean4中代码正确性提升近1.5倍）和推理计算效率（提升2倍）。"
    ],
    "processed_at": "2025-11-28T08:49:24.406457"
  },
  {
    "id": "2511.20993v1",
    "title": "Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning",
    "abstract": "Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game \"Crafter\" demonstrate the effectiveness of our proposed method.",
    "authors": [
      "Shanwei Fan"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20993v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20993v1",
    "fetched_at": "2025-11-28T08:33:22.371773",
    "chinese_title": "用于LLM引导的开放世界强化学习的子目标图增强规划",
    "chinese_summary": "针对LLM引导强化学习中规划-执行不对齐（子目标不可行/无关、单LLM生成与自验证混淆）的问题，论文提出SGA-ACR框架，整合环境特定子目标图、结构化实体知识与多LLM规划流水线（分离生成、批判、细化），并通过子目标跟踪器监控进度、提供辅助奖励及更新子目标图；在开放世界游戏Crafter的22个任务中验证了该方法的有效性。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "提出SGA-ACR框架，整合环境特定子目标图、结构化实体知识与多LLM规划流水线（分离生成-批判-细化），解决LLM引导RL的规划-执行不对齐问题",
      "设计子目标跟踪器，监控执行进度、提供辅助奖励并自适应更新子目标图，维持计划与行动对齐"
    ],
    "processed_at": "2025-11-28T08:49:43.365336"
  },
  {
    "id": "2511.20940v1",
    "title": "Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs",
    "abstract": "Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.",
    "authors": [
      "Reham Omar",
      "Abdelghny Orogat",
      "Ibrahim Abdelaziz",
      "Omij Mangukiya",
      "Panos Kalnis",
      "Essam Mansour"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20940v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20940v1",
    "fetched_at": "2025-11-28T08:33:22.371799",
    "chinese_title": "Chatty-KG：面向知识图谱的按需对话式问答多智能体AI系统",
    "chinese_summary": "论文提出模块化多智能体系统Chatty-KG，针对现有RAG序列化结构、多轮上下文处理不足及传统KGQA单轮问答等局限，通过任务专用LLM智能体协作（上下文解释、对话跟踪等）结合RAG检索与结构化执行，实现自然问题到可执行SPARQL查询的准确低延迟转换；实验在多规模多样化KG上显著优于SOTA基线，模块化设计支持动态KG无需微调预训练。",
    "tags": [
      "LLM",
      "NLP",
      "Financial Agent"
    ],
    "key_contributions": [
      "提出Chatty-KG模块化多智能体系统，融合RAG检索与结构化执行，解决现有方法在多轮上下文、结构保留及动态KG支持等方面的不足",
      "任务专用LLM智能体协作实现自然问题到SPARQL查询的准确低延迟转换，模块化设计支持动态KG无需微调预训练，实验在单轮/多轮设置下优于SOTA"
    ],
    "processed_at": "2025-11-28T08:50:00.672875"
  },
  {
    "id": "2511.20848v1",
    "title": "NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities",
    "abstract": "Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.",
    "authors": [
      "Tasha Kim",
      "Yingke Wang",
      "Hanvit Cho",
      "Alex Hodges"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.SY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20848v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20848v1",
    "fetched_at": "2025-11-28T08:33:22.371823",
    "chinese_title": "NOIR 2.0：面向日常活动的神经信号操控智能机器人",
    "chinese_summary": "该论文提出增强版脑机接口系统NOIR 2.0，通过更快更准确的脑信号解码算法将人类意图转化为机器人指令，任务完成时间减少46%；同时采用结合基础模型的少样本机器人学习算法，提升对个体用户的适应性与意图预测效率，显著降低人类操作时间65%。",
    "tags": [
      "Deep Learning",
      "LLM"
    ],
    "key_contributions": [
      "提出增强版脑机接口NOIR 2.0，优化脑信号解码算法提升指令转化速度与准确性，任务完成时间减少46%",
      "采用结合基础模型的少样本机器人学习算法，提升个体用户适应性与意图预测效率，人类操作时间降低65%"
    ],
    "processed_at": "2025-11-28T08:50:15.022417"
  },
  {
    "id": "2511.20564v1",
    "title": "E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems",
    "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.",
    "authors": [
      "Rui Xue",
      "Shichao Zhu",
      "Liang Qin",
      "Guangmou Pan",
      "Yang Song",
      "Tianfu Wu"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20564v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20564v1",
    "fetched_at": "2025-11-28T08:33:29.049873",
    "chinese_title": "E2E-GRec：图神经网络与推荐系统的端到端联合训练框架",
    "chinese_summary": "针对现有推荐系统中GNN与推荐模型两阶段部署的计算开销高、缺乏联合优化的问题，本文提出端到端联合训练框架E2E-GRec，通过高效子图采样、Graph Feature Auto-Encoder辅助自监督及两级特征融合+Gradnorm动态损失平衡，实现可扩展且稳定的训练，离线评估与在线A/B测试验证了其有效性。",
    "tags": [
      "Graph Neural Network",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出E2E-GRec端到端联合训练框架，解决现有两阶段 pipeline 导致的计算开销高、GNN嵌入对推荐任务次优的问题",
      "设计高效子图采样、辅助自监督任务及动态损失平衡机制，实现大规模图下稳定的端到端训练，经实验验证性能提升"
    ],
    "processed_at": "2025-11-28T08:50:31.737232"
  },
  {
    "id": "2511.20333v1",
    "title": "NNGPT: Rethinking AutoML with Large Language Models",
    "abstract": "Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.",
    "authors": [
      "Roman Kochnev",
      "Waleed Khalid",
      "Tolgay Atinc Uzun",
      "Xi Zhang",
      "Yashkumar Sanjaybhai Dhameliya",
      "Furui Qin",
      "Chandini Vysyaraju",
      "Raghuvir Duvvuri",
      "Avi Goyal",
      "Dmitry Ignatov",
      "Radu Timofte"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20333v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20333v1",
    "fetched_at": "2025-11-28T08:33:29.049909",
    "chinese_title": "NNGPT：基于大语言模型的自动机器学习重新思考",
    "chinese_summary": "论文提出开源框架NNGPT，将大语言模型转化为自改进的神经网络开发AutoML引擎（侧重计算机视觉），通过生成-评估-自改进的闭环系统整合5个LLM驱动的协同管道，实现端到端模型验证与学习，且在执行率、预测精度等指标上优于部分传统方法。",
    "tags": [
      "LLM",
      "Deep Learning",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "提出开源NNGPT框架，将大语言模型转化为自改进的AutoML引擎，通过生成-评估-自改进闭环实现神经网络开发的持续优化",
      "整合多LLM驱动管道，在执行率、超参数预测精度等方面优于传统方法（如Optuna）"
    ],
    "processed_at": "2025-11-28T08:50:48.721325"
  },
  {
    "id": "2511.20297v1",
    "title": "Improving Language Agents through BREW",
    "abstract": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.",
    "authors": [
      "Shashank Kirtania",
      "Param Biyani",
      "Priyanshu Gupta",
      "Yasharth Bajpai",
      "Roshni Iyer",
      "Sumit Gulwani",
      "Gustavo Soares"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20297v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20297v1",
    "fetched_at": "2025-11-28T08:33:29.049937",
    "chinese_title": "通过BREW提升语言智能体性能",
    "chinese_summary": "针对当前LLM智能体训练（如PPO等）计算开销大、策略难解释的问题，论文提出BREW框架，通过构建与优化经验环境知识的结构化记忆，结合任务评分器、行为 rubric及状态空间搜索提升智能体性能；实证在OSWorld等基准上实现10-20%精度提升、10-15%工具调用减少且计算高效。",
    "tags": [
      "LLM",
      "Financial Agent",
      "Reinforcement Learning",
      "Transformer"
    ],
    "key_contributions": [
      "提出BREW框架，以经验环境知识的结构化记忆优化替代高计算开销的RL类智能体训练方法",
      "实证证明BREW在多基准上提升任务精度、减少工具调用并保持计算效率"
    ],
    "processed_at": "2025-11-28T08:51:00.725804"
  },
  {
    "id": "2511.20200v1",
    "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025",
    "abstract": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution",
    "authors": [
      "Yitian Huang",
      "Yuxuan Lei",
      "Jianxun Lian",
      "Hao Liao"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20200v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20200v1",
    "fetched_at": "2025-11-28T08:33:29.050000",
    "chinese_title": "由大语言模型驱动的交互式AI NPC：CPDC 2025挑战技术报告",
    "chinese_summary": "本文介绍团队MSRA_SC在CPDC 2025挑战中的解决方案，提出包含上下文工程（动态工具剪枝、角色裁剪等）和GPU赛道GRPO强化学习训练的框架；该框架提升工具调用稳定性与任务导向对话性能，最终在多个任务中取得优异排名（如Task2 API赛道第一）。",
    "tags": [
      "LLM",
      "NLP",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "提出统一GPU与API赛道的有效框架，通过上下文工程提升工具调用稳定性与角色扮演引导性",
      "GPU赛道采用GRPO强化学习训练替代监督微调，缓解小样本过拟合并增强任务导向对话性能",
      "在CPDC 2025挑战多任务中取得优异成绩（Task2 API第一、Task1 API第二等）"
    ],
    "processed_at": "2025-11-28T08:51:18.274683"
  },
  {
    "id": "2511.19829v1",
    "title": "A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization",
    "abstract": "Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.",
    "authors": [
      "Ke Chen",
      "Yifeng Wang",
      "Hassan Almosapeeh",
      "Haohan Wang"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19829v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19829v1",
    "fetched_at": "2025-11-28T08:33:29.050023",
    "chinese_title": "面向查询依赖的提示优化的统一评估指导框架",
    "chinese_summary": "现有提示优化方法多针对静态模板或依赖不稳定反馈，提示质量缺乏统一系统定义；本文建立面向性能的提示评估框架，开发无需执行的评估器预测多维度质量分数，进而指导度量感知优化器以可解释的查询依赖方式优化提示；实验表明评估器预测准确，优化方法在8个数据集3个骨干模型上超越基线，实现稳定可解释的改进。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "建立面向性能的系统全面提示评估框架，解决提示质量缺乏统一系统定义的问题",
      "提出评估指导的提示优化 pipeline，包含无需执行的评估器和度量感知优化器，实现可解释的查询依赖优化并超越多基线"
    ],
    "processed_at": "2025-11-28T08:51:30.651821"
  },
  {
    "id": "2511.19355v1",
    "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks",
    "abstract": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.",
    "authors": [
      "Franklin Cardenoso",
      "Wouter Caarls"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19355v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19355v1",
    "fetched_at": "2025-11-28T08:33:29.050043",
    "chinese_title": "利用大语言模型进行强化学习控制任务中的奖励函数设计",
    "chinese_summary": "论文提出无需初步指标和环境源代码的LEARN-Opt框架，能从系统与任务目标的文本描述中自主生成、执行并评估奖励函数候选，还可自主推导性能指标实现无监督评估，实验表现媲美或优于EUREKA且需先验知识更少。",
    "tags": [
      "LLM",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "提出LEARN-Opt框架，无需初步评估指标和环境源代码，可从文本描述自主生成、执行、评估奖励函数候选",
      "自主推导性能指标实现无监督奖励函数评估，实验表现媲美SOTA（EUREKA）且需先验知识更少"
    ],
    "processed_at": "2025-11-28T08:51:38.429140"
  },
  {
    "id": "2511.19253v1",
    "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization",
    "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.",
    "authors": [
      "Boyuan Wu"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19253v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19253v1",
    "fetched_at": "2025-11-28T08:33:29.050060",
    "chinese_title": "MAESTRO：通过任务与奖励优化的多智能体环境塑造",
    "chinese_summary": "针对合作多智能体强化学习（MARL）中密集奖励函数设计和课程构建的瓶颈，论文提出MAESTRO框架，将大语言模型（LLM）移出执行环作为离线训练架构师，包含语义课程生成器（创建性能驱动的交通场景）和自动奖励合成器（生成适配课程难度的Python奖励函数），引导标准MARL算法（MADDPG）提升交通信号控制任务的性能与稳定性，且不增加部署推理成本。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出MAESTRO框架，将LLM作为离线训练架构师，解决MARL中奖励设计与课程构建的瓶颈问题",
      "设计语义课程生成器和自动奖励合成器，在不增加部署推理成本的前提下提升MARL算法的性能与稳定性"
    ],
    "processed_at": "2025-11-28T08:51:55.970601"
  },
  {
    "id": "2511.19055v1",
    "title": "Large Language Model-Assisted Planning of Electric Vehicle Charging Infrastructure with Real-World Case Study",
    "abstract": "The growing demand for electric vehicle (EV) charging infrastructure presents significant planning challenges, requiring efficient strategies for investment and operation to deliver cost-effective charging services. However, the potential benefits of EV charging assignment, particularly in response to varying spatial-temporal patterns of charging demand, remain under-explored in infrastructure planning. This paper proposes an integrated approach that jointly optimizes investment decisions and charging assignments while accounting for spatial-temporal demand dynamics and their interdependencies. To support efficient model development, we leverage a large language model (LLM) to assist in generating and refining the mathematical formulation from structured natural-language descriptions, significantly reducing the modeling burden. The resulting optimization model enables optimal joint decision-making for investment and operation. Additionally, we propose a distributed optimization algorithm based on the Alternating Direction Method of Multipliers (ADMM) to address computational complexity in high-dimensional scenarios, which can be executed on standard computing platforms. We validate our approach through a case study using 1.5 million real-world travel records from Chengdu, China, demonstrating a 30% reduction in total cost compared to a baseline without EV assignment.",
    "authors": [
      "Xinda Zheng",
      "Canchen Jiang",
      "Hao Wang"
    ],
    "published": "2025-11-24",
    "categories": [
      "eess.SY",
      "cs.AI",
      "math.OC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19055v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19055v1",
    "fetched_at": "2025-11-28T08:33:29.050082",
    "chinese_title": "大语言模型辅助的电动汽车充电基础设施规划（含真实案例研究）",
    "chinese_summary": "本文提出一种集成方法，联合优化电动汽车充电基础设施的投资决策与充电分配，同时考虑时空需求动态及相互依赖；利用大语言模型（LLM）辅助从结构化自然语言描述生成并优化数学公式，显著降低建模负担；提出基于交替方向乘子法（ADMM）的分布式优化算法应对高维计算复杂度，通过成都150万真实出行记录的案例验证，总成本较无充电分配基线降低30%。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出联合优化电动汽车充电基础设施投资决策与充电分配的集成框架，考虑时空需求动态及相互依赖；",
      "借助大语言模型辅助数学建模以显著降低建模负担，提出基于ADMM的分布式优化算法解决高维计算问题，经真实案例验证总成本显著降低。"
    ],
    "processed_at": "2025-11-28T08:52:16.043059"
  },
  {
    "id": "2511.18368v1",
    "title": "Wireless Power Transfer and Intent-Driven Network Optimization in AAVs-assisted IoT for 6G Sustainable Connectivity",
    "abstract": "Autonomous Aerial Vehicle (AAV)-assisted Internet of Things (IoT) represents a collaborative architecture in which AAV allocate resources over 6G links to jointly enhance user-intent interpretation and overall network performance. Owing to this mutual dependence, improvements in intent inference and policy decisions on one component reinforce the efficiency of others, making highly reliable intent prediction and low-latency action execution essential. Although numerous approaches can model intent relationships, they encounter severe obstacles when scaling to high-dimensional action sequences and managing intensive on-board computation. We propose an Intent-Driven Framework for Autonomous Network Optimization comprising prediction and decision modules. First, implicit intent modeling is adopted to mitigate inaccuracies arising from ambiguous user expressions. For prediction, we introduce Hyperdimensional Transformer (HDT), which embeds data into a Hyperdimensional space via Hyperdimensional vector encoding and replaces standard matrix and attention operations with symbolic Hyperdimensional computations. For decision-making, where AAV must respond to user intent while planning trajectories, we design Double Actions based Multi-Agent Proximal Policy Optimization (DA-MAPPO). Building upon MAPPO, it samples actions through two independently parameterized networks and cascades the user-intent network into the trajectory network to maintain action dependencies. We evaluate our framework on a real IoT action dataset with authentic wireless data. Experimental results demonstrate that HDT and DA-MAPPO achieve superior performance across diverse scenarios.",
    "authors": [
      "Yue Hu",
      "Xiaoming He",
      "Rui Yuan",
      "Shahid Mumtaz"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18368v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18368v1",
    "fetched_at": "2025-11-28T08:33:29.050104",
    "chinese_title": "面向6G可持续连接的无人机辅助物联网中无线电能传输与意图驱动网络优化",
    "chinese_summary": "针对6G下无人机辅助物联网的资源分配问题，论文提出意图驱动的自主网络优化框架，预测模块用超维Transformer（HDT）嵌入数据到超维空间并替换传统操作，决策模块设计双动作多智能体近端策略优化（DA-MAPPO）响应意图与规划轨迹，解决高维动作序列扩展及机载计算密集的障碍。",
    "tags": [
      "Reinforcement Learning",
      "Transformer"
    ],
    "key_contributions": [
      "提出意图驱动的自主网络优化框架，包含隐式意图建模、超维Transformer（HDT）预测模块及双动作多智能体近端策略优化（DA-MAPPO）决策模块",
      "有效缓解高维动作序列扩展与机载计算密集的挑战，提升用户意图推断准确性与网络整体性能"
    ],
    "processed_at": "2025-11-28T08:52:30.339932"
  },
  {
    "id": "2511.18269v1",
    "title": "A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks",
    "abstract": "Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$κ$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.",
    "authors": [
      "Ved Mohan",
      "El Mehdi Er Raqabi",
      "Pascal Van Hentenryck"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18269v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18269v1",
    "fetched_at": "2025-11-28T08:33:29.050125",
    "chinese_title": "大规模网络中资源替代的公平OR-ML框架",
    "chinese_summary": "针对大规模物流网络资源供需不平衡及分散场景下协调难、需兼顾公平与调度员偏好的问题，论文提出结合运筹学（OR）与机器学习（ML）的通用框架；OR组件建模求解带公平约束的资源替代问题，ML组件通过历史数据学习偏好、引导决策探索并动态选top-κ资源提升效率，输出高质量解供调度员选择。",
    "tags": [
      "Portfolio Optimization",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出结合OR与ML的通用框架，首次在大规模网络资源替代中同时考虑公平性与调度员偏好",
      "ML组件通过学习偏好、动态选top-κ资源提升计算效率，OR组件建模带公平约束的优化问题"
    ],
    "processed_at": "2025-11-28T08:52:54.207870"
  },
  {
    "id": "2511.18258v1",
    "title": "Hybrid Agentic AI and Multi-Agent Systems in Smart Manufacturing",
    "abstract": "The convergence of Agentic AI and MAS enables a new paradigm for intelligent decision making in SMS. Traditional MAS architectures emphasize distributed coordination and specialized autonomy, while recent advances in agentic AI driven by LLMs introduce higher order reasoning, planning, and tool orchestration capabilities. This paper presents a hybrid agentic AI and multi agent framework for a Prescriptive Maintenance use case, where LLM based agents provide strategic orchestration and adaptive reasoning, complemented by rule based and SLMs agents performing efficient, domain specific tasks on the edge. The proposed framework adopts a layered architecture that consists of perception, preprocessing, analytics, and optimization layers, coordinated through an LLM Planner Agent that manages workflow decisions and context retention. Specialized agents autonomously handle schema discovery, intelligent feature analysis, model selection, and prescriptive optimization, while a HITL interface ensures transparency and auditability of generated maintenance recommendations. This hybrid design supports dynamic model adaptation, cost efficient maintenance scheduling, and interpretable decision making. An initial proof of concept implementation is validated on two industrial manufacturing datasets. The developed framework is modular and extensible, supporting seamless integration of new agents or domain modules as capabilities evolve. The results demonstrate the system capability to automatically detect schema, adapt preprocessing pipelines, optimize model performance through adaptive intelligence, and generate actionable, prioritized maintenance recommendations. The framework shows promise in achieving improved robustness, scalability, and explainability for RxM in smart manufacturing, bridging the gap between high level agentic reasoning and low level autonomous execution.",
    "authors": [
      "Mojtaba A. Farahani",
      "Md Irfan Khan",
      "Thorsten Wuest"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18258v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18258v1",
    "fetched_at": "2025-11-28T08:33:29.050147",
    "chinese_title": "智能制造中的混合智能体AI与多智能体系统",
    "chinese_summary": "本文提出面向智能制造预测性维护的混合智能体AI与多智能体框架，以LLM智能体实现战略编排与自适应推理，结合规则及SLM智能体完成边缘端高效领域任务，分层架构由LLM Planner协调工作流；该框架支持动态模型适应、成本高效维护调度及可解释决策，经工业数据集验证且模块化可扩展。",
    "tags": [
      "LLM",
      "Anomaly",
      "Transformer"
    ],
    "key_contributions": [
      "提出融合LLM战略推理与规则/SLM边缘高效任务的混合智能体框架，分层架构支持动态适应与可解释决策",
      "经工业数据集验证框架有效性，模块化设计可无缝扩展新智能体/领域模块，实现成本高效预测性维护调度"
    ],
    "processed_at": "2025-11-28T08:53:18.652798"
  },
  {
    "id": "2511.18384v1",
    "title": "NSTR: Neural Spectral Transport Representation for Space-Varying Frequency Fields",
    "abstract": "Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals such as images, audio, and 3D scenes. However, existing INR frameworks -- including MLPs with Fourier features, SIREN, and multiresolution hash grids -- implicitly assume a \\textit{global and stationary} spectral basis. This assumption is fundamentally misaligned with real-world signals whose frequency characteristics vary significantly across space, exhibiting local high-frequency textures, smooth regions, and frequency drift phenomena. We propose \\textbf{Neural Spectral Transport Representation (NSTR)}, the first INR framework that \\textbf{explicitly models a spatially varying local frequency field}. NSTR introduces a learnable \\emph{frequency transport equation}, a PDE that governs how local spectral compositions evolve across space. Given a learnable local spectrum field $S(x)$ and a frequency transport network $F_θ$ enforcing $\\nabla S(x) \\approx F_θ(x, S(x))$, NSTR reconstructs signals by spatially modulating a compact set of global sinusoidal bases. This formulation enables strong local adaptivity and offers a new level of interpretability via visualizing frequency flows. Experiments on 2D image regression, audio reconstruction, and implicit 3D geometry show that NSTR achieves significantly better accuracy-parameter trade-offs than SIREN, Fourier-feature MLPs, and Instant-NGP. NSTR requires fewer global frequencies, converges faster, and naturally explains signal structure through spectral transport fields. We believe NSTR opens a new direction in INR research by introducing explicit modeling of space-varying spectrum.",
    "authors": [
      "Plein Versace"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18384v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18384v1",
    "fetched_at": "2025-11-28T08:33:39.256432",
    "chinese_title": "NSTR：用于空间变化频率场的神经谱传输表示",
    "chinese_summary": "现有隐式神经表示（INR）假设全局平稳谱基，无法适配现实信号的空间频率变化；本文提出NSTR，显式建模空间变化局部频率场，引入可学习频率传输方程约束局部谱演化，通过空间调制全局正弦基重建信号，实验表明其精度-参数权衡优于SIREN等现有方法，收敛更快且具谱流解释性。",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "提出首个显式建模空间变化局部频率场的隐式神经表示框架NSTR",
      "引入可学习频率传输方程约束局部谱演化，实现空间谱成分自适应，实验精度-参数权衡优于现有INR方法"
    ],
    "processed_at": "2025-11-28T08:53:44.038332"
  },
  {
    "id": "2511.18151v1",
    "title": "AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems",
    "abstract": "Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution \"context stream\" for real-time awareness and a low-frequency, high-fidelity \"insight stream\" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.",
    "authors": [
      "Rajat Bhattacharjya",
      "Sing-Yao Wu",
      "Hyunwoo Oh",
      "Chaewon Nam",
      "Suyeon Koo",
      "Mohsen Imani",
      "Elaheh Bozorgzadeh",
      "Nikil Dutt"
    ],
    "published": "2025-11-22",
    "categories": [
      "cs.DC",
      "cs.AR",
      "cs.CV",
      "cs.LG",
      "cs.NI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18151v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18151v1",
    "fetched_at": "2025-11-28T08:33:39.256474",
    "chinese_title": "AVERY：基于具身自我意识的自适应VLM分割计算以实现高效灾害响应系统",
    "chinese_summary": "针对灾害响应中无人机机载CNN语义推理能力不足、VLM部署资源需求高及云卸载低带宽失效问题，提出AVERY框架，采用认知启发的双流分割（高频低分辨率上下文流+低频高保真洞察流），并通过轻量化具身自我意识控制器动态适配网络条件与操作员意图以平衡精度-吞吐量 trade-off；实验表明其在LISA-7B VLM上表现优于静态配置，精度较原始图像压缩提升11.2%且能耗较全边缘执行降低93.98%，支持资源受限平台的实时可查询智能。",
    "tags": [
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出认知启发的双流VLM分割架构，分离高频低分辨率上下文流与低频高保真洞察流，突破传统深度分割局限",
      "设计轻量化具身自我意识机载控制器，动态选择预训练压缩模型，实现精度、能耗与实时性的最优平衡"
    ],
    "processed_at": "2025-11-28T08:54:06.922722"
  },
  {
    "id": "2511.20284v1",
    "title": "Can LLMs Make (Personalized) Access Control Decisions?",
    "abstract": "Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.   Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.",
    "authors": [
      "Friederike Groschupp",
      "Daniele Lain",
      "Aritra Dhar",
      "Lara Magdalena Lazier",
      "Srdjan Čapkun"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20284v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20284v1",
    "fetched_at": "2025-11-28T08:34:06.450767",
    "chinese_title": "LLM能否做出（个性化）访问控制决策？",
    "chinese_summary": "针对用户访问控制决策认知负荷过重问题，论文提出利用LLM实现动态上下文感知的访问控制决策方法；通过用户研究构建含307条隐私陈述与14682条用户决策的数据集，对比通用及个性化LLM的决策效果，发现通用LLM匹配多数用户偏好准确率达86%，且个性化存在提升个体匹配与违反安全最佳实践的权衡。",
    "tags": [
      "LLM",
      "NLP",
      "Behavioral Finance"
    ],
    "key_contributions": [
      "构建了包含307条自然语言隐私陈述和14682条用户访问控制决策的数据集",
      "验证通用LLM匹配多数用户偏好准确率达86%，揭示个性化LLM在个体匹配与安全最佳实践间的权衡"
    ],
    "processed_at": "2025-11-28T08:54:35.464804"
  },
  {
    "id": "2511.18933v1",
    "title": "Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations",
    "abstract": "Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project",
    "authors": [
      "Ryan Wong",
      "Hosea David Yu Fei Ng",
      "Dhananjai Sharma",
      "Glenn Jun Jie Ng",
      "Kavishvaran Srinivasan"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18933v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18933v1",
    "fetched_at": "2025-11-28T08:34:06.450802",
    "chinese_title": "考虑负责任AI的大语言模型越狱攻击防御",
    "chinese_summary": "该文系统分类现有大语言模型（LLM）越狱防御的三类干预（提示层、模型层、训练时），提出三个防御策略——提示层防御框架、基于logit的推理时引导防御、基于MetaGPT的领域特定Agent防御；实验表明攻击成功率显著下降，Agent防御可完全缓解攻击，同时指出防御需权衡安全、性能与可扩展性。",
    "tags": [
      "LLM",
      "Deep Learning",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "系统分类现有LLM越狱防御的三类干预，提出三个针对性防御策略",
      "实验验证策略有效性，尤其是领域特定Agent防御实现完全攻击缓解，分析防御的安全-性能-可扩展性权衡"
    ],
    "processed_at": "2025-11-28T08:54:55.876519"
  },
  {
    "id": "2511.18538v1",
    "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
    "abstract": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
    "authors": [
      "Jian Yang",
      "Wei Zhang",
      "Shark Liu",
      "Jiajun Wu",
      "Shawn Guo",
      "Yizhi Li"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.SE",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18538v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18538v1",
    "fetched_at": "2025-11-28T08:34:06.450829",
    "chinese_title": "从代码基础模型到智能体与应用：代码智能实用指南",
    "chinese_summary": "论文系统梳理代码大语言模型（LLM）的全生命周期（含数据整理、预训练、微调、强化学习等），分析通用及代码专用LLM的能力与技术权衡，指出学术研究与实际部署的差距并映射实用研究方向，还开展系列实验验证。",
    "tags": [
      "LLM",
      "Transformer",
      "Reinforcement Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "全面梳理代码LLM从数据整理到后训练的全生命周期技术框架及设计权衡",
      "对比分析通用与代码专用LLM的代码能力，明确学术研究与实际部署的核心差距",
      "提出衔接学术与实践的研究方向，并开展系列实验验证相关结论"
    ],
    "processed_at": "2025-11-28T08:55:09.965365"
  },
  {
    "id": "2511.18405v1",
    "title": "A Multimodal Conversational Agent for Tabular Data Analysis",
    "abstract": "Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.",
    "authors": [
      "Mohammad Nour Al Awad",
      "Sergey Ivanov",
      "Olga Tikhonova",
      "Ivan Khodnenko"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18405v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18405v1",
    "fetched_at": "2025-11-28T08:34:06.450852",
    "chinese_title": "用于表格数据分析的多模态对话代理",
    "chinese_summary": "本文提出多模态对话代理Talk2Data，整合LLM、自动语音识别（Whisper）、文本转语音（Coqui）、代码生成（Qwen-coder）及沙盒执行工具，支持语音/文本交互与多轮上下文感知对话；实验在3个数据集48个任务上实现95.8%准确率，7B参数LLM在准确率、延迟与成本间取得最优平衡，计算过程可验证。",
    "tags": [
      "LLM",
      "NLP",
      "Financial Agent"
    ],
    "key_contributions": [
      "提出整合多模态交互（语音/文本）、代码生成与沙盒执行的对话代理Talk2Data，支持上下文感知多轮对话；",
      "实验验证7B参数LLM在准确率、延迟与成本间最优平衡，且计算过程可验证。"
    ],
    "processed_at": "2025-11-28T08:55:30.386042"
  },
  {
    "id": "2511.20216v1",
    "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents",
    "abstract": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.",
    "authors": [
      "Haebin Seong",
      "Sungmin Kim",
      "Minchan Kim",
      "Yongjun Cho",
      "Myunchul Joe",
      "Suhwan Choi",
      "Jaeyoon Jung",
      "Jiyong Youn",
      "Yoonshik Kim",
      "Samwoo Seong",
      "Yubeen Park",
      "Youngjae Yu",
      "Yunsung Lee"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20216v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20216v1",
    "fetched_at": "2025-11-28T08:34:09.813270",
    "chinese_title": "CostNav：具身智能体成本感知评估的导航基准",
    "chinese_summary": "现有导航基准忽略商业部署的经济可行性，论文提出首个成本感知导航基准CostNav，基于行业参数建模全生命周期成本（硬件、训练、能源、维护等）与收入（含服务等级协议SLA）；定量揭示任务成功优化与商业可行的差距，基线虽有43% SLA合规但亏损（碰撞维护占99.7%成本），并建立多类导航方法的评估基础。",
    "tags": [
      "Benchmark",
      "Financial Agent",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出CostNav——首个面向具身智能体的成本感知导航基准，整合全生命周期经济参数与商业SLA，填补任务成功与商业可行性的评估空白",
      "定量分析表明任务成功优化≠商业可行，碰撞诱导的维护成本是核心痛点，为导航方法优化指明经济导向目标",
      "建立规则型、模仿学习、学习型等多类导航方法的评估框架，为商业部署的智能体导航提供经济评估依据"
    ],
    "processed_at": "2025-11-28T08:55:54.154409"
  },
  {
    "id": "2511.21448v1",
    "title": "Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework",
    "abstract": "Phishing and spam emails remain a major cybersecurity threat, with attackers increasingly leveraging Large Language Models (LLMs) to craft highly deceptive content. This study presents a comprehensive email dataset containing phishing, spam, and legitimate messages, explicitly distinguishing between human- and LLM-generated content. Each email is annotated with its category, emotional appeal (e.g., urgency, fear, authority), and underlying motivation (e.g., link-following, credential theft, financial fraud). We benchmark multiple LLMs on their ability to identify these emotional and motivational cues and select the most reliable model to annotate the full dataset. To evaluate classification robustness, emails were also rephrased using several LLMs while preserving meaning and intent. A state-of-the-art LLM was then assessed on its performance across both original and rephrased emails using expert-labeled ground truth. The results highlight strong phishing detection capabilities but reveal persistent challenges in distinguishing spam from legitimate emails. Our dataset and evaluation framework contribute to improving AI-assisted email security systems. To support open science, all code, templates, and resources are available on our project site.",
    "authors": [
      "Rebeka Toth",
      "Tamas Bisztray",
      "Richard Dubniczky"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21448v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21448v1",
    "fetched_at": "2025-11-28T08:34:16.578229",
    "chinese_title": "构建与基准测试：面向基于文本的钓鱼和垃圾邮件检测框架的带标签邮件数据集",
    "chinese_summary": "本研究构建了包含钓鱼、垃圾邮件及合法邮件（明确区分人类与LLM生成内容）的综合带标签数据集，标注类别、情感诉求及动机；通过基准测试筛选可靠LLM标注全数据集，评估其在原始与重写邮件上的检测性能，发现钓鱼检测能力较强但垃圾邮件与合法邮件区分存挑战，开源相关代码与资源。",
    "tags": [
      "LLM",
      "NLP",
      "Sentiment Analysis",
      "Benchmark"
    ],
    "key_contributions": [
      "构建了区分人类与LLM生成内容、标注情感诉求及动机的钓鱼/垃圾/合法邮件综合数据集",
      "提出评估框架，基准测试LLM标注能力并评估其检测性能，开源相关资源支持开放科学"
    ],
    "processed_at": "2025-11-28T08:56:12.148225"
  },
  {
    "id": "2511.21121v1",
    "title": "Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval",
    "abstract": "Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.   We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.   VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.",
    "authors": [
      "Anup Roy",
      "Rishabh Gyanendra Upadhyay",
      "Animesh Rameshbhai Panara",
      "Robin Mills"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21121v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21121v1",
    "fetched_at": "2025-11-28T08:34:16.578281",
    "chinese_title": "超越补丁聚合：用于视觉增强文档检索的三通道金字塔索引",
    "chinese_summary": "针对现有文档检索RAG管道依赖OCR、丢失空间线索及视觉优先模型内存开销大等问题，提出无OCR且模型无关的多模态检索系统VisionRAG；采用三通道金字塔索引框架构建含全局摘要、视觉热点等的轻量检索代理向量，检索时融合多信号排名，再将原始页面图像传给多模态LLM完成QA，降低存储与部署复杂度。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出无OCR、模型无关的多模态检索系统VisionRAG，避免传统RAG管道的OCR依赖与结构丢失问题",
      "设计三通道金字塔索引框架，构建轻量检索代理向量降低存储开销，结合多模态LLM实现高效文档检索与QA"
    ],
    "processed_at": "2025-11-28T08:56:32.062013"
  },
  {
    "id": "2511.20944v1",
    "title": "Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection",
    "abstract": "Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies and exploits psychological vulnerabilities, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual adjusted losses, presenting significant economic asymmetry: the cost of a False Negative (fraud loss) exceeds the cost of a False Positive (manual review) by orders of magnitude (approximately 1 to 5,480).   This paper examines two detection paradigms for BEC: the Forensic Psycholinguistic Stream, which utilizes CatBoost to analyze psycholinguistic cues with high interpretability and low latency, and the Semantic Stream, which employs DistilBERT for deep learning-based contextual language understanding, offering superior accuracy at higher computational cost. We evaluated DistilBERT on an adversarially poisoned dataset (N = 7,990) generated via our Black Hole protocol, benchmarked on Tesla T4 GPU infrastructure, achieving superior detection (AUC = 1.0000, F1 = 0.9981) with acceptable real-time latency (7.403 milliseconds). CatBoost achieves competitive detection (AUC = 0.9905, F1 = 0.9486) at 8.4x lower latency (0.885 milliseconds), consuming negligible computational resources. For organizations with GPU infrastructure, DistilBERT offers superior accuracy. CatBoost is preferable for edge deployments or cost-sensitive environments due to comparable security and lower operational costs. Both approaches demonstrate return on investment exceeding 99.96% when optimized through cost-sensitive learning, by significantly reducing false negatives and associated financial losses.",
    "authors": [
      "Yaw Osei Adjei"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20944v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20944v1",
    "fetched_at": "2025-11-28T08:34:16.578303",
    "chinese_title": "语义优势与取证效率：深度学习与心理语言学在商务邮件欺诈检测中的比较分析",
    "chinese_summary": "论文针对每年造成超29亿美元损失的商务邮件欺诈（BEC），比较两种检测范式——基于DistilBERT的语义流（高精度需GPU）和基于CatBoost的取证心理语言流（低延迟低成本），在对抗性数据集上验证性能，为不同部署场景提供选型参考。",
    "tags": [
      "NLP",
      "Deep Learning",
      "Transformer",
      "Anomaly"
    ],
    "key_contributions": [
      "系统比较深度学习（DistilBERT）与心理语言学（CatBoost）两种BEC检测范式，明确精度、延迟、成本的权衡关系",
      "生成7990样本的对抗性中毒数据集（Black Hole协议），并验证方法性能，为不同场景（GPU/边缘/成本敏感）提供部署建议"
    ],
    "processed_at": "2025-11-28T08:56:51.966313"
  },
  {
    "id": "2511.19925v1",
    "title": "Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity",
    "abstract": "Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.",
    "authors": [
      "Qiyao Wei",
      "Edward Morrell",
      "Lea Goetz",
      "Mihaela van der Schaar"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19925v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19925v1",
    "fetched_at": "2025-11-28T08:34:31.120374",
    "chinese_title": "Semantic-KG：利用知识图谱构建语义相似度测量基准",
    "chinese_summary": "该论文针对现有语义相似度方法侧重语法词汇而非语义、基准存在成本高/领域有限等问题，提出基于知识图谱生成多领域（含金融）语义相似/不相似自然语言对的基准构建方法；通过对比传统NLP方法与LLM-as-judge，发现语义变异子类型和领域会影响方法性能，无方法一致最优，为LLM语义检测应用提供启示。",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "系统对比传统NLP方法与LLM-as-judge，揭示语义变异子类型和领域对性能的影响，为语义相似度评估提供参考"
    ],
    "processed_at": "2025-11-28T08:57:06.315064"
  },
  {
    "id": "2511.21689v1",
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "abstract": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "authors": [
      "Hongjin Su",
      "Shizhe Diao",
      "Ximing Lu",
      "Mingjie Liu",
      "Jiacheng Xu",
      "Xin Dong",
      "Yonggan Fu",
      "Peter Belcak",
      "Hanrong Ye",
      "Hongxu Yin",
      "Yi Dong",
      "Evelina Bakhturina",
      "Tao Yu",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21689v1",
    "arxiv_url": "https://arxiv.org/abs/2511.21689v1",
    "fetched_at": "2025-11-28T08:34:34.604426",
    "chinese_title": "ToolOrchestra：通过高效模型与工具编排提升智能",
    "chinese_summary": "论文提出ToolOrchestra方法，通过结合结果、效率及用户偏好的强化学习训练小型编排器（8B模型），协调智能工具解决复杂任务；该编排器在HLE等基准上比GPT-5准确率更高、成本更低（如HLE得分37.1%超GPT-5的35.1%且效率2.5x），实现性能与成本的最优权衡，还能泛化到未见过的工具。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "提出多目标强化学习驱动的ToolOrchestra方法，训练小型编排器协调智能工具，兼顾性能、效率与用户偏好；",
      "训练出的8B编排器在多个基准上超越GPT-5，同时降低成本（仅约30%）并提升效率（2.5x），且泛化到未见过的工具。"
    ],
    "processed_at": "2025-11-28T08:57:22.935101"
  },
  {
    "id": "2511.19942v1",
    "title": "Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning",
    "abstract": "It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \\textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \\textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\\% improvements on AIME24 dataset.",
    "authors": [
      "Jingchu Gai",
      "Guanning Zeng",
      "Huaqing Zhang",
      "Aditi Raghunathan"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19942v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19942v1",
    "fetched_at": "2025-11-28T08:34:34.604515",
    "chinese_title": "差分平滑缓解锐化并提升大语言模型推理能力",
    "chinese_summary": "该论文形式化证明了大语言模型RL微调因选择与强化偏差导致多样性坍塌的机制，提出差分平滑方法——仅对正确轨迹应用奖励修改，可同时提升正确性与多样性，理论优于现有启发式方法，实验在AIME24等数据集验证了效果。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Deep Learning",
      "NLP"
    ],
    "key_contributions": [
      "形式化证明RL微调导致多样性坍塌的选择与强化偏差机制",
      "提出差分平滑方法，可同时提升LLM推理的正确性与多样性并实验验证其优势"
    ],
    "processed_at": "2025-11-28T08:57:31.744693"
  },
  {
    "id": "2511.19504v1",
    "title": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.",
    "authors": [
      "Subramanyam Sahoo",
      "Aman Chadha",
      "Vinija Jain",
      "Divya Chaudhary"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19504v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19504v1",
    "fetched_at": "2025-11-28T08:34:34.604608",
    "chinese_title": "立场：完美AI对齐的复杂性——RLHF三难困境的形式化",
    "chinese_summary": "本文形式化RLHF的对齐三难困境（无法同时实现多样化人类价值代表性、多项式计算复杂度、对抗扰动鲁棒性）；通过结合统计学习与鲁棒优化的复杂度分析，证明全球规模代表性与鲁棒性需超多项式操作；指出当前RLHF以牺牲代表性解决该困境，框架解释了偏好坍塌等病理问题并给出缓解方向。",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Deep Learning"
    ],
    "key_contributions": [
      "形式化RLHF的对齐三难困境，明确多样化价值代表性、多项式复杂度、鲁棒性三个目标无法同时满足",
      "通过复杂度分析揭示当前RLHF牺牲代表性的本质，解释偏好坍塌等病理问题并提出缓解方向"
    ],
    "processed_at": "2025-11-28T08:57:46.700382"
  },
  {
    "id": "2511.19935v1",
    "title": "EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning",
    "abstract": "The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \\textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.",
    "authors": [
      "Songlin Zhao",
      "Michael Pitts",
      "Zhuwei Qin"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19935v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19935v1",
    "fetched_at": "2025-11-28T08:34:55.708707",
    "chinese_title": "EfficientXpert：基于传播感知剪枝的大语言模型高效领域自适应",
    "chinese_summary": "针对大语言模型（LLM）领域适配中现有压缩方法跨域泛化差或开销高的问题，本文提出EfficientXpert框架，结合传播感知剪枝准则与高效适配器更新算法，集成LoRA微调实现通用模型到稀疏领域专家的一步转换；在健康、法律任务中40%稀疏度下保留98%密集模型性能，优于现有方法，且揭示领域结构变化需自适应剪枝策略。",
    "tags": [
      "LLM",
      "NLP",
      "Transformer"
    ],
    "key_contributions": [
      "提出EfficientXpert轻量领域剪枝框架，融合传播感知剪枝（Foresight Mask）与高效适配器更新（Partial Brain Surgeon），集成LoRA实现通用预训练模型到稀疏领域专家的一步转换",
      "实验验证在健康、法律任务中40%稀疏度下保留98%密集模型性能，优于SOTA，且揭示领域依赖结构变化对通用剪枝的负面影响，强调自适应领域感知剪枝的必要性"
    ],
    "processed_at": "2025-11-28T08:58:03.234366"
  },
  {
    "id": "2511.20102v1",
    "title": "SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space",
    "abstract": "The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.",
    "authors": [
      "Zhenyi Shen",
      "Junru Lu",
      "Lin Gui",
      "Jiazheng Li",
      "Yulan He",
      "Di Yin",
      "Xing Sun"
    ],
    "published": "2025-11-25",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20102v1",
    "arxiv_url": "https://arxiv.org/abs/2511.20102v1",
    "fetched_at": "2025-11-28T08:34:59.326006",
    "chinese_title": "SSA：通过特征空间对齐全注意力与稀疏注意力输出的稀疏稀疏注意力",
    "chinese_summary": "全注意力的二次复杂度限制大语言模型（LLM）长上下文高效处理，现有稀疏注意力存在梯度更新不足（低秩键值对无梯度）导致稀疏度 paradox（低于全注意力）。论文提出SSA统一训练框架，层间双向对齐全/稀疏注意力输出，保留所有token梯度流并促进更强稀疏性；在多常识基准上实现稀疏/全推理SOTA，且支持模型平滑适配不同稀疏预算。",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出SSA统一训练框架，通过特征空间双向对齐全/稀疏注意力输出，解决现有稀疏注意力梯度更新不足问题，促进更强稀疏性",
      "在多常识基准上实现稀疏/全推理SOTA，且支持模型平滑适配不同稀疏预算，灵活平衡计算与性能"
    ],
    "processed_at": "2025-11-28T08:58:14.702121"
  },
  {
    "id": "2511.19550v1",
    "title": "The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication",
    "abstract": "This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $λ$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.",
    "authors": [
      "Davide Picca"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.IT",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19550v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19550v1",
    "fetched_at": "2025-11-28T08:34:59.326047",
    "chinese_title": "符号信道原理：衡量大语言模型通信中的意义容量",
    "chinese_summary": "论文提出符号框架分析大语言模型（LLM），用信息论工具量化表达丰富度（源熵）与解释稳定性（消息和人类解释的互信息）的权衡，引入生成复杂度参数λ建模该权衡，定义符号信道并提出意义传输的容量约束，同时展示其在模型 profiling、提示设计等四方面的应用价值。",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "提出基于符号学与信息论的LLM分析框架，量化表达丰富度与解释稳定性的权衡并引入λ参数建模",
      "定义符号信道与意义传输容量约束，展示在模型分析、提示设计等多场景的实用价值"
    ],
    "processed_at": "2025-11-28T08:58:26.258082"
  },
  {
    "id": "2511.18936v1",
    "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression",
    "abstract": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.",
    "authors": [
      "Santhosh G S",
      "Saurav Prakash",
      "Balaraman Ravindran"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18936v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18936v1",
    "fetched_at": "2025-11-28T08:34:59.326067",
    "chinese_title": "SWAN：基于无解压KV缓存压缩的稀疏筛选注意力机制以降低推理内存占用",
    "chinese_summary": "本文提出免微调框架SWAN，通过离线正交矩阵旋转剪枝KV缓存，无需解压即可直接用于注意力计算；实验表明，SWAN结合小稠密缓冲区可实现50-60%的单token内存节省且性能接近无压缩基线，还支持动态调整压缩级别，是LLM长上下文服务的高效实用方案。",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出免微调的SWAN框架，通过离线正交矩阵旋转剪枝KV缓存，实现无解压直接注意力计算，消除传统压缩的解压开销",
      "支持动态压缩级别调整，结合小稠密缓冲区可在50-60%内存节省下保持接近基线性能，适配LLM长上下文服务需求"
    ],
    "processed_at": "2025-11-28T08:58:42.210670"
  },
  {
    "id": "2511.18890v1",
    "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
    "abstract": "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.",
    "authors": [
      "Yonggan Fu",
      "Xin Dong",
      "Shizhe Diao",
      "Matthijs Van keirsbilck",
      "Hanrong Ye",
      "Wonmin Byeon",
      "Yashaswi Karnati",
      "Lucas Liebenwein",
      "Hannah Zhang",
      "Nikolaus Binder",
      "Maksim Khadkevich",
      "Alexander Keller",
      "Jan Kautz",
      "Yingyan Celine Lin",
      "Pavlo Molchanov"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18890v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18890v1",
    "fetched_at": "2025-11-28T08:34:59.326136",
    "chinese_title": "Nemotron-Flash：面向延迟最优的混合小语言模型",
    "chinese_summary": "本文针对小语言模型（SLM）实际部署的延迟约束，指出参数效率未必对应实际设备速度提升，识别深度-宽度比（影响小批量延迟）和算子选择（影响延迟与吞吐量）为关键架构因素；通过进化搜索框架自动发现混合SLM中算子的延迟最优组合，结合权重归一化优化训练，推动准确率-延迟前沿。",
    "tags": [
      "LLM",
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "识别深度-宽度比和算子选择为影响SLM实际设备延迟的核心架构因素，揭示深窄模型未必处于准确率-延迟前沿",
      "提出进化搜索框架自动发现混合SLM中算子的延迟最优组合，结合权重归一化增强训练，提升准确率-延迟前沿"
    ],
    "processed_at": "2025-11-28T08:59:01.161733"
  },
  {
    "id": "2511.19518v1",
    "title": "Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning",
    "abstract": "Recent advances in vision-language models (VLMs) have shown remarkable performance across multimodal tasks, yet their ever-growing scale poses severe challenges for deployment and efficiency. Existing compression methods often rely on heuristic importance metrics or empirical pruning rules, lacking theoretical guarantees about information preservation. In this work, we propose InfoPrune, an information-theoretic framework for adaptive structural compression of VLMs. Grounded in the Information Bottleneck principle, we formulate pruning as a trade-off between retaining task-relevant semantics and discarding redundant dependencies. To quantify the contribution of each attention head, we introduce an entropy-based effective rank (eRank) and employ the Kolmogorov--Smirnov (KS) distance to measure the divergence between original and compressed structures. This yields a unified criterion that jointly considers structural sparsity and informational efficiency. Building on this foundation, we further design two complementary schemes: (1) a training-based head pruning guided by the proposed information loss objective, and (2) a training-free FFN compression via adaptive low-rank approximation. Extensive experiments on VQAv2, TextVQA, and GQA demonstrate that InfoPrune achieves up to 3.2x FLOP reduction and 1.8x acceleration with negligible performance degradation, establishing a theoretically grounded and practically effective step toward efficient multimodal large models.",
    "authors": [
      "Zhaoqi Xu",
      "Yingying Zhang",
      "Jian Li",
      "Jianwei Guo",
      "Qiannan Zhu",
      "Hua Huang"
    ],
    "published": "2025-11-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IT",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.19518v1",
    "arxiv_url": "https://arxiv.org/abs/2511.19518v1",
    "fetched_at": "2025-11-28T08:34:59.326162",
    "chinese_title": "面向高效视觉语言模型（VLMs）：基于信息论驱动的自适应结构剪枝压缩方法",
    "chinese_summary": "针对视觉语言模型（VLMs）规模过大导致部署困难且现有压缩方法缺乏理论保证的问题，提出基于信息论的自适应结构压缩框架InfoPrune，以信息瓶颈原理为基础，通过熵基有效秩（eRank）和KS距离量化注意力头贡献，设计训练型头剪枝与无训练FFN低秩近似互补方案；实验表明该方法可实现3.2x FLOP减少、1.8x加速且性能损失可忽略。",
    "tags": [
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "提出基于信息论的自适应结构压缩框架InfoPrune，以信息瓶颈原理量化注意力头贡献，统一考虑结构稀疏性与信息效率",
      "设计训练型头剪枝和无训练FFN低秩近似两种互补方案，在多模态任务上实现显著压缩且性能损失可忽略"
    ],
    "processed_at": "2025-11-28T08:59:25.994565"
  },
  {
    "id": "2511.18393v1",
    "title": "Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models",
    "abstract": "A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.",
    "authors": [
      "Heejoon Koo"
    ],
    "published": "2025-11-23",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.18393v1",
    "arxiv_url": "https://arxiv.org/abs/2511.18393v1",
    "fetched_at": "2025-11-28T08:35:36.907767",
    "chinese_title": "基于大语言模型的带噪声临床笔记下鲁棒公平下次就诊诊断预测研究",
    "chinese_summary": "针对临床笔记含噪声导致LLM诊断预测可靠性与公平性受影响的问题，该研究提出临床锚定的标签缩减方案及模拟医生推理的分层思维链策略，提升了噪声输入下预测的鲁棒性与亚组稳定性，推动LLM在临床决策支持系统中的可靠应用。",
    "tags": [
      "LLM",
      "NLP",
      "Transformer"
    ],
    "key_contributions": [
      "系统探究了噪声临床笔记对LLM下次就诊诊断预测的鲁棒性与公平性影响",
      "提出临床锚定标签缩减与分层思维链策略，提升噪声输入下预测的鲁棒性及亚组稳定性"
    ],
    "processed_at": "2025-11-28T08:59:37.797596"
  }
]