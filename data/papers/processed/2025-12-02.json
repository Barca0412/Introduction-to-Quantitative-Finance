[
  {
    "id": "2512.01967v1",
    "title": "Arbitrage-Free Option Price Surfaces via Chebyshev Tensor Bases and a Hamiltonian Fog Post-Fit",
    "abstract": "We study the construction of arbitrage-free option price surfaces from noisy bid-ask quotes across strike and maturity. Our starting point is a Chebyshev representation of the call price surface on a warped log-moneyness/maturity rectangle, together with linear sampling and no-arbitrage operators acting on a collocation grid. Static no-arbitrage requirements are enforced as linear inequalities, while the surface is fitted directly to prices via a coverage-seeking quadratic objective that trades off squared band misfit against spectral and transport-inspired regularisation of the Chebyshev coefficients. This yields a strictly convex quadratic program in the modal coefficients, solvable at practical scales with off-the-shelf solvers (OSQP).   On top of the global backbone, we introduce a local post-fit layer based on a discrete fog of risk-neutral densities on a three-dimensional lattice (m,t,u) and an associated Hamiltonian-type energy. On each patch of the (m,t) plane, the fog variables are coupled to a nodal price field obtained from the baseline surface, yielding a joint convex optimisation problem that reweights noisy quotes and applies noise-aware local corrections while preserving global static no-arbitrage and locality.   The method is designed such that for equity options panels, the combined procedure achieves high inside-spread coverage in stable regimes (in calm years, 98-99% of quotes are priced inside the bid-ask intervals) and low rates of static no-arbitrage violations (below 1%). In stressed periods, the fog layer provides a mechanism for controlled leakage outside the band: when local quotes are mutually inconsistent or unusually noisy, the optimiser allocates fog mass outside the bid-ask tube and justifies small out-of-band deviations of the post-fit surface, while preserving a globally arbitrage-free and well-regularised description of the option surface.",
    "authors": [
      "Robert Jenkinson Alvarez"
    ],
    "published": "2025-12-01",
    "categories": [
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01967v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01967v1",
    "fetched_at": "2025-12-02T08:34:38.678427",
    "chinese_title": "åŸºäºåˆ‡æ¯”é›ªå¤«å¼ é‡åŸºå’Œå“ˆå¯†é¡¿é›¾åæ‹Ÿåˆçš„æ— å¥—åˆ©æœŸæƒä»·æ ¼æ›²é¢æ„å»º",
    "chinese_summary": "è®ºæ–‡æå‡ºä»å¸¦å™ªå£°çš„ä¹°å–æŠ¥ä»·æ„å»ºæ— å¥—åˆ©æœŸæƒä»·æ ¼æ›²é¢çš„æ–¹æ³•ï¼šå…¨å±€å±‚é¢ç”¨åˆ‡æ¯”é›ªå¤«å¼ é‡åŸºè¡¨ç¤ºæ›²é¢ï¼Œå°†é™æ€æ— å¥—åˆ©è¦æ±‚è½¬åŒ–ä¸ºçº¿æ€§ä¸ç­‰å¼ï¼Œé€šè¿‡å‡¸äºŒæ¬¡è§„åˆ’é«˜æ•ˆæ±‚è§£ï¼›å±€éƒ¨å±‚é¢å¼•å…¥åŸºäºç¦»æ•£é£é™©ä¸­æ€§å¯†åº¦é›¾å’Œå“ˆå¯†é¡¿èƒ½é‡çš„åæ‹Ÿåˆå±‚ï¼Œåœ¨ä¿ç•™å…¨å±€æ— å¥—åˆ©çš„åŒæ—¶ä¿®æ­£å™ªå£°æŠ¥ä»·ï¼Œå®ç°é«˜ä¹°å–ä»·å·®å†…è¦†ç›–åº¦ä¸ä½æ— å¥—åˆ©è¿åç‡ã€‚",
    "tags": [
      "Options",
      "Asset Pricing",
      "Risk Management",
      "Volatility"
    ],
    "key_contributions": [
      "æå‡ºåŸºäºåˆ‡æ¯”é›ªå¤«å¼ é‡åŸºçš„å…¨å±€æ— å¥—åˆ©æœŸæƒä»·æ ¼æ›²é¢æ„å»ºæ¡†æ¶ï¼Œå°†é™æ€æ— å¥—åˆ©çº¦æŸè½¬åŒ–ä¸ºçº¿æ€§ä¸ç­‰å¼ï¼Œé€šè¿‡å‡¸äºŒæ¬¡è§„åˆ’é«˜æ•ˆæ±‚è§£ï¼Œå¯æ‰©å±•è‡³å®é™…è§„æ¨¡",
      "å¼•å…¥å“ˆå¯†é¡¿é›¾åæ‹Ÿåˆçš„å±€éƒ¨ä¿®æ­£å±‚ï¼Œåœ¨ä¿ç•™å…¨å±€æ— å¥—åˆ©æ€§è´¨çš„åŒæ—¶æå‡å™ªå£°æŠ¥ä»·çš„æ‹Ÿåˆç²¾åº¦ï¼Œå®ç°é«˜ä¹°å–ä»·å·®å†…è¦†ç›–åº¦ä¸ä½æ— å¥—åˆ©è¿åç‡"
    ],
    "processed_at": "2025-12-02T08:37:59.699092"
  },
  {
    "id": "2512.01623v1",
    "title": "Monopoly Pricing of Weather Index Insurance",
    "abstract": "This study models the monopoly pricing of weather index insurance as a Bowley-type sequential game involving a profit-maximizing insurer (leader) and a farmer (follower). The farmer chooses an insurance payoff to minimize a convex distortion risk measure, while the insurer anticipates this best response and selects a premium principle and its parameters to maximize profit net of administrative costs. For the insurer, we adopt three different premium-principle parameterizations: (i) an expected premium with a single risk-loading factor, (ii) a two-parameter distortion premium based on a power transform, and (iii) a fully flexible pricing kernel drawn from the general Choquet integral representation with nondecreasing distortions. For the farmer, we model index payoffs using neural networks and compare solutions under fully connected architectures with those under convolutional neural networks (CNNs). We solve the game using a penalized bilevel programming algorithm that employs a function-value-gap penalty and delivers convergence guarantees without requiring the lower-level objective to be strongly convex. Based on Iowa's soybean yields and high-dimensional PRISM weather data, we find that CNN-based designs yield smoother, less noisy payoffs that reduce basis risk and push insurer profits closer to indemnity insurance levels. Moreover, expanding pricing flexibility from a single loading to a two-parameter distortion premium, and ultimately to a flexible pricing kernel, systematically increases equilibrium profits.",
    "authors": [
      "Tim J. Boonen",
      "Wenyuan Li",
      "Zixiao Quan"
    ],
    "published": "2025-12-01",
    "categories": [
      "q-fin.RM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01623v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01623v1",
    "fetched_at": "2025-12-02T08:34:38.678469",
    "chinese_title": "å¤©æ°”æŒ‡æ•°ä¿é™©çš„å„æ–­å®šä»·",
    "chinese_summary": "æœ¬æ–‡æ„å»ºå¤©æ°”æŒ‡æ•°ä¿é™©å„æ–­å®šä»·çš„Bowleyå‹åºè´¯åšå¼ˆæ¨¡å‹ï¼ˆä¿é™©å…¬å¸ä¸ºé¢†å¯¼è€…ã€å†œæ°‘ä¸ºè·Ÿéšè€…ï¼‰ï¼Œé‡‡ç”¨ä¸‰ç§ä¿è´¹å®šä»·å‚æ•°åŒ–ã€ç¥ç»ç½‘ç»œï¼ˆå…¨è¿æ¥ä¸CNNï¼‰è®¾è®¡æŒ‡æ•°èµ”ä»˜ï¼Œå¹¶ç”¨å¸¦æƒ©ç½šçš„åŒå±‚è§„åˆ’ç®—æ³•æ±‚è§£ï¼›å‘ç°CNNè®¾è®¡å¯é™ä½åŸºå·®é£é™©ï¼Œä½¿ä¿é™©å…¬å¸åˆ©æ¶¦æ›´æ¥è¿‘èµ”å¿ä¿é™©æ°´å¹³ï¼Œä¸”å®šä»·çµæ´»æ€§æ‰©å±•å¯¹åˆ©æ¶¦æœ‰ç³»ç»Ÿå½±å“ã€‚",
    "tags": [
      "Risk Management",
      "Deep Learning"
    ],
    "key_contributions": [
      "æ„å»ºäº†è€ƒè™‘ä¿é™©å…¬å¸åˆ©æ¶¦æœ€å¤§åŒ–ä¸å†œæ°‘å‡¸æ‰­æ›²é£é™©åº¦é‡æœ€å°åŒ–çš„å¤©æ°”æŒ‡æ•°ä¿é™©å„æ–­å®šä»·åºè´¯åšå¼ˆæ¨¡å‹ï¼Œé‡‡ç”¨å¤šç±»ä¿è´¹å®šä»·å‚æ•°åŒ–ä¸ç¥ç»ç½‘ç»œè®¾è®¡èµ”ä»˜å‡½æ•°",
      "å®è¯å‘ç°CNNè®¾è®¡çš„æŒ‡æ•°èµ”ä»˜å¯é™ä½åŸºå·®é£é™©ï¼Œä½¿ä¿é™©å…¬å¸åˆ©æ¶¦æ›´æ¥è¿‘èµ”å¿ä¿é™©æ°´å¹³ï¼Œä¸”å®šä»·çµæ´»æ€§æ‰©å±•å¯¹åˆ©æ¶¦æœ‰ç³»ç»Ÿå½±å“"
    ],
    "processed_at": "2025-12-02T08:38:21.331784"
  },
  {
    "id": "2512.01408v1",
    "title": "Bayesian Distributionally Robust Merton Problem with Nonlinear Wasserstein Projections",
    "abstract": "We revisit Merton's continuous-time portfolio selection through a data-driven, distributionally robust lens. Our aim is to tap the benefits of frequent trading over short horizons while acknowledging that drift is hard to pin down, whereas volatility can be screened using realized or implied measures for appropriately selected assets. Rather than time-rectangular distributional robust control -- which replenishes adversarial power at every instant and induces over-pessimism -- we place a single ambiguity set on the drift prior within a Bayesian Merton model. This prior-level ambiguity preserves learning and tractability: a minimax swap reduces the robust control to optimizing a nonlinear functional of the prior, enabling Karatzas and Zhao \\cite{KZ98}-type's closed-form evaluation for each candidate prior. We then characterize small-radius worst-case priors under Wasserstein uncertainty via an explicit asymptotically optimal pushforward of the nominal prior, and we calibrate the ambiguity radius through a nonlinear Wasserstein projection tailored to the Merton functional. Synthetic and real-data studies demonstrate reduced pessimism relative to DRC and improved performance over myopic DRO-Markowitz under frequent rebalancing.",
    "authors": [
      "Jose Blanchet",
      "Jiayi Cheng",
      "Hao Liu",
      "Yang Liu"
    ],
    "published": "2025-12-01",
    "categories": [
      "math.OC",
      "math.PR",
      "math.ST",
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01408v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01408v1",
    "fetched_at": "2025-12-02T08:34:38.678500",
    "chinese_title": "å¸¦éçº¿æ€§WassersteinæŠ•å½±çš„è´å¶æ–¯åˆ†å¸ƒé²æ£’Mertoné—®é¢˜",
    "chinese_summary": "è®ºæ–‡ä»æ•°æ®é©±åŠ¨çš„åˆ†å¸ƒé²æ£’è§†è§’é‡æ–°ç ”ç©¶Mertonè¿ç»­æ—¶é—´æŠ•èµ„ç»„åˆé€‰æ‹©ï¼Œé’ˆå¯¹æ¼‚ç§»éš¾ä¼°è®¡ä½†æ³¢åŠ¨ç‡å¯æµ‹çš„ç‰¹ç‚¹ï¼Œåœ¨è´å¶æ–¯Mertonæ¨¡å‹æ¼‚ç§»å…ˆéªŒè®¾ç½®å•ä¸€æ¨¡ç³Šé›†ï¼ˆé¿å…æ—¶é—´çŸ©å½¢DRCçš„è¿‡åº¦æ‚²è§‚ï¼‰ï¼Œé€šè¿‡minimaxäº¤æ¢è½¬åŒ–ä¸ºä¼˜åŒ–å…ˆéªŒéçº¿æ€§æ³›å‡½ï¼Œç»“åˆKaratzas-Zhaoé—­å¼è§£æ±‚è§£ï¼›è¿›ä¸€æ­¥ç”¨Wassersteinä¸ç¡®å®šæ€§ä¸‹æ¸è¿‘æœ€ä¼˜æœ€åæƒ…å†µå…ˆéªŒå’Œéçº¿æ€§æŠ•å½±æ ¡å‡†æ¨¡ç³ŠåŠå¾„ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ¯”DRCæ‚²è§‚æ€§æ›´ä½ã€é¢‘ç¹å†å¹³è¡¡ä¸‹ä¼˜äºè¿‘è§†DRO-Markowitzã€‚",
    "tags": [
      "Portfolio Optimization",
      "Risk Management",
      "Volatility",
      "High Frequency"
    ],
    "key_contributions": [
      "æå‡ºè´å¶æ–¯åˆ†å¸ƒé²æ£’Mertonæ¨¡å‹ï¼Œåœ¨æ¼‚ç§»å…ˆéªŒè®¾ç½®å•ä¸€æ¨¡ç³Šé›†è€Œéæ—¶é—´çŸ©å½¢æ¨¡ç³Šï¼Œä¿ç•™å­¦ä¹ æ€§ä¸”é¿å…è¿‡åº¦æ‚²è§‚",
      "æ¨å¯¼Wassersteinä¸ç¡®å®šæ€§ä¸‹å°åŠå¾„æœ€åæƒ…å†µå…ˆéªŒçš„æ¸è¿‘æœ€ä¼˜å½¢å¼ï¼Œé€šè¿‡éçº¿æ€§æŠ•å½±æ ¡å‡†æ¨¡ç³ŠåŠå¾„ï¼ŒéªŒè¯æ€§èƒ½ä¼˜åŠ¿"
    ],
    "processed_at": "2025-12-02T08:38:44.429474"
  },
  {
    "id": "2512.01354v1",
    "title": "The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness",
    "abstract": "Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.   This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.   The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.   Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.",
    "authors": [
      "Zhongjie Jiang"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01354v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01354v1",
    "fetched_at": "2025-12-02T08:34:38.678524",
    "chinese_title": "ä¸å®Œç¾çš„å¿…è¦æ€§ï¼šé€šè¿‡æ¨¡æ‹Ÿè®¤çŸ¥æœ‰é™æ€§é€†è½¬æ¨¡å‹å´©å¡Œ",
    "chinese_summary": "æœ¬æ–‡æŒ‡å‡ºåˆæˆæ•°æ®å› è¿½æ±‚ç»Ÿè®¡å¹³æ»‘å»é™¤äººç±»æ–‡æœ¬çš„è®¤çŸ¥ç›¸å…³ä¸è§„åˆ™æ€§ï¼ŒåŠ é€Ÿæ¨¡å‹å´©å¡Œï¼›æå‡ºPrompté©±åŠ¨è®¤çŸ¥è®¡ç®—æ¡†æ¶ï¼ˆPMCSFï¼‰ï¼Œå«è®¤çŸ¥çŠ¶æ€è§£ç å™¨ï¼ˆCSDï¼‰å’Œå¸¦è®¤çŸ¥æ‰°åŠ¨ç®—å­çš„è®¤çŸ¥æ–‡æœ¬ç¼–ç å™¨ï¼ˆCTEï¼‰ï¼Œæ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹ç”Ÿæˆå¸¦å…¸å‹ä¸å®Œç¾çš„æ–‡æœ¬ï¼›éªŒè¯æ˜¾ç¤ºCTEæ–‡æœ¬ä¸äººç±»æ–‡æœ¬å·®å¼‚å°ï¼Œä¸”Aè‚¡å‹åŠ›æµ‹è¯•ä¸­ç­–ç•¥æœ€å¤§å›æ’¤é™ä½47.4%ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Behavioral Finance",
      "Risk Management"
    ],
    "key_contributions": [
      "æå‡ºPMCSFæ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹ç”Ÿæˆå«è®¤çŸ¥ä¸å®Œç¾çš„æ–‡æœ¬ï¼Œè§£å†³åˆæˆæ•°æ®ç»Ÿè®¡å¹³æ»‘å¼•å‘çš„æ¨¡å‹å´©å¡Œé—®é¢˜",
      "éªŒè¯è¯¥æ¡†æ¶ç”Ÿæˆçš„æ–‡æœ¬ä¸äººç±»æ–‡æœ¬é«˜åº¦åŒ¹é…ï¼Œä¸”åœ¨Aè‚¡å¸‚åœºç­–ç•¥ä¸­æœ‰æ•ˆé™ä½æœ€å¤§å›æ’¤ç­‰é£é™©æŒ‡æ ‡"
    ],
    "processed_at": "2025-12-02T08:39:08.062073"
  },
  {
    "id": "2512.01123v1",
    "title": "A Hybrid Architecture for Options Wheel Strategy Decisions: LLM-Generated Bayesian Networks for Transparent Trading",
    "abstract": "Large Language Models (LLMs) excel at understanding context and qualitative nuances but struggle with the rigorous and transparent reasoning required in high-stakes quantitative domains such as financial trading. We propose a model-first hybrid architecture for the options \"wheel\" strategy that combines the strengths of LLMs with the robustness of a Bayesian Network. Rather than using the LLM as a black-box decision-maker, we employ it as an intelligent model builder. For each trade decision, the LLM constructs a context-specific Bayesian network by interpreting current market conditions, including prices, volatility, trends, and news, and hypothesizing relationships among key variables. The LLM also selects relevant historical data from an 18.75-year, 8,919-trade dataset to populate the network's conditional probability tables. This selection focuses on scenarios analogous to the present context. The instantiated Bayesian network then performs transparent probabilistic inference, producing explicit probability distributions and risk metrics to support decision-making. A feedback loop enables the LLM to analyze trade outcomes and iteratively refine subsequent network structures and data selection, learning from both successes and failures. Empirically, our hybrid system demonstrates effective performance on the wheel strategy. Over nearly 19 years of out-of-sample testing, it achieves a 15.3% annualized return with significantly superior risk-adjusted performance (Sharpe ratio 1.08 versus 0.62 for market benchmarks) and dramatically lower drawdown (-8.2% versus -60%) while maintaining a 0% assignment rate through strategic option rolling. Crucially, each trade decision is fully explainable, involving on average 27 recorded decision factors (e.g., volatility level, option premium, risk indicators, market context).",
    "authors": [
      "Xiaoting Kuang",
      "Boken Lin"
    ],
    "published": "2025-11-30",
    "categories": [
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01123v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01123v1",
    "fetched_at": "2025-12-02T08:34:38.678545",
    "chinese_title": "æœŸæƒè½®åŠ¨ç­–ç•¥å†³ç­–çš„æ··åˆæ¶æ„ï¼šç”¨äºé€æ˜äº¤æ˜“çš„LLMç”Ÿæˆè´å¶æ–¯ç½‘ç»œ",
    "chinese_summary": "è®ºæ–‡æå‡ºLLMä¸è´å¶æ–¯ç½‘ç»œç»“åˆçš„æ··åˆæ¶æ„ï¼Œç”¨LLMæ„å»ºä¸Šä¸‹æ–‡ç‰¹å®šçš„è´å¶æ–¯ç½‘ç»œï¼ˆè€Œéé»‘ç›’å†³ç­–ï¼‰ï¼Œå¹¶é€‰æ‹©ç›¸å…³å†å²æ•°æ®å¡«å……æ¡ä»¶æ¦‚ç‡è¡¨å®ç°é€æ˜æ¨ç†ï¼›è¯¥æ¶æ„é€šè¿‡åé¦ˆå¾ªç¯è¿­ä»£ä¼˜åŒ–ï¼Œå®è¯åœ¨æœŸæƒè½®åŠ¨ç­–ç•¥ä¸Šè·15.3%å¹´åŒ–æ”¶ç›ŠåŠæ›´ä¼˜é£é™©è°ƒæ•´è¡¨ç°ã€‚",
    "tags": [
      "LLM",
      "Options",
      "Algorithmic Trading",
      "Risk Management"
    ],
    "key_contributions": [
      "æå‡ºLLMä½œä¸ºæ™ºèƒ½æ¨¡å‹æ„å»ºè€…çš„æ··åˆæ¶æ„ï¼Œç”¨LLMç”Ÿæˆä¸Šä¸‹æ–‡ç‰¹å®šçš„è´å¶æ–¯ç½‘ç»œå®ç°é€æ˜äº¤æ˜“å†³ç­–ï¼Œé¿å…LLMé»‘ç›’ç¼ºé™·",
      "å®è¯éªŒè¯è¯¥æ¶æ„åœ¨æœŸæƒè½®åŠ¨ç­–ç•¥ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œè¿‘19å¹´æ ·æœ¬å¤–æµ‹è¯•è·15.3%å¹´åŒ–æ”¶ç›ŠåŠæ˜¾è‘—æ›´ä¼˜é£é™©è°ƒæ•´è¡¨ç°"
    ],
    "processed_at": "2025-12-02T08:39:29.170709"
  },
  {
    "id": "2512.01112v1",
    "title": "Autodeleveraging: Impossibilities and Optimization",
    "abstract": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \\$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \\emph{trilemma}: no policy can simultaneously satisfy exchange \\emph{solvency}, \\emph{revenue}, and \\emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \\emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \\$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by approximately $8\\times$ relative to our optimal policy, imposing roughly \\$630 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency.",
    "authors": [
      "Tarun Chitra"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.GT",
      "q-fin.RM",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01112v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01112v1",
    "fetched_at": "2025-12-02T08:34:38.678565",
    "chinese_title": "è‡ªåŠ¨å»æ æ†ï¼ˆADLï¼‰ï¼šä¸å¯èƒ½å®šç†ä¸ä¼˜åŒ–",
    "chinese_summary": "è¯¥è®ºæ–‡é¦–æ¬¡æ„å»ºæ°¸ç»­æœŸè´§è‡ªåŠ¨å»æ æ†ï¼ˆADLï¼‰çš„ä¸¥æ ¼æ¨¡å‹ï¼Œè¯æ˜ADLæœºåˆ¶å­˜åœ¨äº¤æ˜“æ‰€å¿ä»˜èƒ½åŠ›ã€æ”¶å…¥ä¸äº¤æ˜“è€…å…¬å¹³æ€§çš„ä¸‰éš¾å›°å¢ƒï¼›æ„é€ ä¸‰ç±»æœ€ä¼˜ADLæœºåˆ¶ä»¥å¹³è¡¡è¯¥å›°å¢ƒï¼Œå¹¶é€šè¿‡Hyperliquidæ•°æ®é›†å®è¯å‘ç°ç°æœ‰ç”Ÿäº§æœºåˆ¶è¿‡åº¦ä½¿ç”¨ADLï¼Œç»™ç›ˆåˆ©äº¤æ˜“è€…é€ æˆå¤§é‡ä¸å¿…è¦æŸå¤±ã€‚",
    "tags": [
      "Risk Management",
      "Market Microstructure",
      "Algorithmic Trading"
    ],
    "key_contributions": [
      "é¦–æ¬¡å»ºç«‹è‡ªåŠ¨å»æ æ†ï¼ˆADLï¼‰çš„ä¸¥æ ¼ç†è®ºæ¨¡å‹ï¼Œè¯æ˜å…¶å­˜åœ¨å¿ä»˜èƒ½åŠ›ã€äº¤æ˜“æ‰€æ”¶å…¥ä¸äº¤æ˜“è€…å…¬å¹³æ€§çš„ä¸‰éš¾å›°å¢ƒ",
      "æ„é€ ä¸‰ç±»æœ€ä¼˜ADLæœºåˆ¶ä»¥å¹³è¡¡ä¸‰éš¾å›°å¢ƒï¼Œå¹¶é€šè¿‡å®è¯å‘ç°ç°æœ‰æœºåˆ¶è¿‡åº¦ä½¿ç”¨ADLï¼Œé€ æˆçº¦6.3äº¿ç¾å…ƒä¸å¿…è¦æŸå¤±"
    ],
    "processed_at": "2025-12-02T08:39:55.777615"
  },
  {
    "id": "2512.00916v1",
    "title": "An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts",
    "abstract": "Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.",
    "authors": [
      "Sotirios D. Nikolopoulos"
    ],
    "published": "2025-11-30",
    "categories": [
      "stat.ME",
      "q-fin.RM",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00916v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00916v1",
    "fetched_at": "2025-12-02T08:34:38.678584",
    "chinese_title": "æç«¯é£é™©é¢„æµ‹çš„ä¸å¹³è¡¡é²æ£’è¯„ä¼°æ¡†æ¶",
    "chinese_summary": "ä¼ ç»Ÿç¨€æœ‰äº‹ä»¶é¢„æµ‹æŒ‡æ ‡å› ç±»åˆ«ä¸å¹³è¡¡å¯¼è‡´é˜ˆå€¼é€€åŒ–ï¼Œè®ºæ–‡æå‡ºç¨€æœ‰äº‹ä»¶ç¨³å®šï¼ˆRESï¼‰æŒ‡æ ‡å®¶æ—ï¼Œå…¶æœ€ä¼˜é˜ˆå€¼åœ¨äº‹ä»¶æ¦‚ç‡è¶‹è¿‘é›¶æ—¶ä»ä¿æŒå†…éƒ¨ï¼›æ¨¡æ‹ŸåŠä¿¡ç”¨è¿çº¦åº”ç”¨éªŒè¯RESæŒ‡æ ‡å…·æœ‰ç¨³å®šé˜ˆå€¼ã€ä¸€è‡´æ¨¡å‹æ’ååŠ prevalenceä¸å˜æ€§ï¼Œä¸ºæç«¯é£é™©é¢„æµ‹æä¾›é²æ£’è¯„ä¼°æ¡†æ¶ã€‚",
    "tags": [
      "Risk Management",
      "Anomaly"
    ],
    "key_contributions": [
      "æå‡ºç¨€æœ‰äº‹ä»¶ç¨³å®šï¼ˆRESï¼‰æŒ‡æ ‡å®¶æ—ï¼Œæœ€ä¼˜é˜ˆå€¼åœ¨äº‹ä»¶æ¦‚ç‡è¶‹è¿‘é›¶æ—¶ä¿æŒä¸¥æ ¼å†…éƒ¨ï¼Œè§£å†³ä¼ ç»ŸæŒ‡æ ‡é˜ˆå€¼é€€åŒ–é—®é¢˜",
      "é€šè¿‡æ¨¡æ‹Ÿä¸ä¿¡ç”¨è¿çº¦åº”ç”¨éªŒè¯RESæŒ‡æ ‡çš„ç¨³å®šé˜ˆå€¼ã€ä¸€è‡´æ¨¡å‹æ’ååŠ prevalenceä¸å˜æ€§ï¼Œæ„å»ºæç«¯é£é™©é¢„æµ‹çš„é²æ£’è¯„ä¼°æ¡†æ¶"
    ],
    "processed_at": "2025-12-02T08:40:21.678897"
  },
  {
    "id": "2512.00893v1",
    "title": "Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election",
    "abstract": "We study how the 2024 U.S. presidential election, viewed as a major political risk event, affected cryptocurrency markets by distinguishing human-driven peer-to-peer stablecoin transactions from automated algorithmic activity. Using structural break analysis, we find that human-driven Ethereum Request for Comment 20 (ERC-20) transactions shifted on November 3, two days before the election, while exchange trading volumes reacted only on Election Day. Automated smart-contract activity adjusted much later, with structural breaks appearing in January 2025. We validate these shifts using surrogate-based robustness tests. Complementary energy-spectrum analysis of Bitcoin and Ethereum identifies pronounced post-election turbulence, and a structural vector autoregression confirms a regime shift in stablecoin dynamics. Overall, human-driven stablecoin flows act as early-warning indicators of political stress, preceding both exchange behavior and algorithmic responses.",
    "authors": [
      "Kundan Mukhia",
      "Buddha Nath Sharma",
      "Salam Rabindrajit Luwang",
      "Md. Nurujjaman",
      "Chittaranjan Hens",
      "Suman Saha",
      "Tanujit Chakraborty"
    ],
    "published": "2025-11-30",
    "categories": [
      "q-fin.ST",
      "physics.data-an",
      "q-fin.PM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00893v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00893v1",
    "fetched_at": "2025-12-02T08:34:38.678614",
    "chinese_title": "ç¨³å®šå¸å¸‚åœºæ”¿æ²»é£é™©çš„é¢„è­¦ä¿¡å·ï¼š2024å¹´ç¾å›½å¤§é€‰å‰åçš„äººç±»ä¸ç®—æ³•è¡Œä¸º",
    "chinese_summary": "æœ¬æ–‡ä»¥2024å¹´ç¾å›½å¤§é€‰ä¸ºæ”¿æ²»é£é™©äº‹ä»¶ï¼ŒåŒºåˆ†äººç±»é©±åŠ¨çš„P2Pç¨³å®šå¸äº¤æ˜“ä¸ç®—æ³•æ´»åŠ¨ï¼Œé€šè¿‡ç»“æ„çªå˜åˆ†æã€æ›¿ä»£åŸºæ£€éªŒç­‰æ–¹æ³•å‘ç°ï¼Œäººç±»é©±åŠ¨çš„ERC-20äº¤æ˜“åœ¨å¤§é€‰å‰2å¤©å‡ºç°å˜åŒ–ï¼Œæ—©äºäº¤æ˜“æ‰€äº¤æ˜“é‡ï¼ˆå¤§é€‰æ—¥ï¼‰å’Œç®—æ³•æ™ºèƒ½åˆçº¦æ´»åŠ¨ï¼ˆ2025å¹´1æœˆï¼‰ï¼›ç»“åˆèƒ½é‡è°±åˆ†æä¸ç»“æ„å‘é‡è‡ªå›å½’éªŒè¯ï¼Œäººç±»é©±åŠ¨çš„ç¨³å®šå¸æµåŠ¨å¯ä½œä¸ºæ”¿æ²»å‹åŠ›çš„é¢„è­¦æŒ‡æ ‡ã€‚",
    "tags": [
      "Risk Management",
      "Behavioral Finance",
      "Algorithmic Trading",
      "Time Series"
    ],
    "key_contributions": [
      "åŒºåˆ†äººç±»é©±åŠ¨çš„P2Pç¨³å®šå¸äº¤æ˜“ä¸ç®—æ³•æ´»åŠ¨ï¼Œæ­ç¤ºä¸‰è€…å¯¹2024ç¾å›½å¤§é€‰æ”¿æ²»é£é™©çš„å“åº”æ—¶åºå·®å¼‚ï¼ˆäººç±»æ—©äºäº¤æ˜“æ‰€åŠç®—æ³•ï¼‰",
      "éªŒè¯äººç±»é©±åŠ¨çš„ç¨³å®šå¸æµåŠ¨å¯ä½œä¸ºæ”¿æ²»å‹åŠ›çš„é¢„è­¦æŒ‡æ ‡"
    ],
    "processed_at": "2025-12-02T08:40:50.047827"
  },
  {
    "id": "2512.00830v1",
    "title": "Equilibrium Investment with Random Risk Aversion: (Non-)uniqueness, Optimality, and Comparative Statics",
    "abstract": "This paper investigates infinite-dimensional portfolio selection problem under a general distribution of the risk aversion parameter. We provide a complete characterization of all deterministic equilibrium investment strategies. Our results reveal that the solution structure depends critically on the distribution of risk aversion: the equilibrium is unique whenever it exists in the case of finite expected risk aversion, whereas an infinite expectation can lead to infinitely many equilibria or to a unique trivial one (pi equals 0). To address this multiplicity, we introduce three optimality criteria-optimal, uniformly optimal, and uniformly strictly optimal-and explicitly characterize the existence and uniqueness of the corresponding equilibria. Under the same necessary and sufficient condition, the optimal and uniformly optimal equilibria exist uniquely and coincide. Furthermore, by additionally assuming that the market price of risk is non-zero near the terminal time, we show that the optimal (and hence uniformly optimal) equilibrium is also uniformly strictly optimal. Finally, we perform comparative statics to demonstrate that a risk aversion distribution dominating another in the reverse hazard rate order leads to a less aggressive equilibrium strategy.",
    "authors": [
      "Cheng Weilun",
      "Liang Zongxia",
      "Wang Sheng",
      "Xia Jianming"
    ],
    "published": "2025-11-30",
    "categories": [
      "q-fin.MF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00830v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00830v1",
    "fetched_at": "2025-12-02T08:34:38.678642",
    "chinese_title": "éšæœºé£é™©åŒæ¶ä¸‹çš„å‡è¡¡æŠ•èµ„ï¼šï¼ˆéï¼‰å”¯ä¸€æ€§ã€æœ€ä¼˜æ€§ä¸æ¯”è¾ƒé™æ€åˆ†æ",
    "chinese_summary": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶éšæœºé£é™©åŒæ¶å‚æ•°ä¸€èˆ¬åˆ†å¸ƒä¸‹çš„æ— é™ç»´æŠ•èµ„ç»„åˆé€‰æ‹©é—®é¢˜ï¼Œåˆ»ç”»äº†ç¡®å®šæ€§å‡è¡¡æŠ•èµ„ç­–ç•¥çš„ç»“æ„ï¼ˆæœ‰é™æœŸæœ›é£é™©åŒæ¶æ—¶å‡è¡¡å”¯ä¸€ï¼Œæ— é™æœŸæœ›æ—¶å¯èƒ½å¤šå‡è¡¡æˆ–å¹³å‡¡è§£ï¼‰ï¼›å¼•å…¥ä¸‰ç±»æœ€ä¼˜æ€§å‡†åˆ™å¹¶æ˜ç¡®å…¶å‡è¡¡çš„å­˜åœ¨å”¯ä¸€æ€§ï¼Œè¿˜é€šè¿‡æ¯”è¾ƒé™æ€åˆ†æå‘ç°åå‘ hazard rate å ä¼˜çš„é£é™©åŒæ¶åˆ†å¸ƒå¯¹åº”æ›´ä¸æ¿€è¿›çš„å‡è¡¡ç­–ç•¥ã€‚",
    "tags": [
      "Portfolio Optimization",
      "Risk Management",
      "Behavioral Finance"
    ],
    "key_contributions": [
      "æ­ç¤ºéšæœºé£é™©åŒæ¶åˆ†å¸ƒçš„æœŸæœ›æ€§è´¨ï¼ˆæœ‰é™/æ— é™ï¼‰å¯¹å‡è¡¡æŠ•èµ„ç­–ç•¥å”¯ä¸€æ€§çš„å…³é”®å½±å“",
      "å¼•å…¥ä¸‰ç±»æœ€ä¼˜æ€§å‡†åˆ™å¹¶æ˜ç¡®å…¶å¯¹åº”çš„å‡è¡¡å­˜åœ¨å”¯ä¸€æ€§ï¼ŒåŒæ—¶é€šè¿‡æ¯”è¾ƒé™æ€åˆ†æé£é™©åŒæ¶åˆ†å¸ƒå¯¹ç­–ç•¥æ¿€è¿›æ€§çš„å½±å“"
    ],
    "processed_at": "2025-12-02T08:41:11.290819"
  },
  {
    "id": "2512.00630v1",
    "title": "Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model",
    "abstract": "Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.",
    "authors": [
      "Zhiming Lian"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00630v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00630v1",
    "fetched_at": "2025-12-02T08:34:38.678661",
    "chinese_title": "åŸºäºrLoRAå¾®è°ƒçš„Qwen3-8Bæ¨¡å‹çš„é‡‘èæ–‡æœ¬åˆ†ç±»",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹é‡‘èæ–‡æœ¬åˆ†ç±»ï¼ˆå«æƒ…æ„Ÿåˆ†æä¸æ–°é—»åˆ†ç±»ï¼‰ä»»åŠ¡ï¼Œé‡‡ç”¨Qwen3-8Bå¤§æ¨¡å‹ï¼Œç»“åˆå¸¦å™ªå£°åµŒå…¥çš„æŒ‡ä»¤å¾®è°ƒã€ç§©ç¨³å®šä½ç§©é€‚åº”ï¼ˆrLoRAï¼‰åŠFlashAttentionä¼˜åŒ–æ–¹æ³•ï¼Œæå‡è®­ç»ƒæ•ˆç‡ä¸é²æ£’æ€§ï¼›å®éªŒè¡¨æ˜Qwen3-8Båœ¨å‡†ç¡®ç‡å’Œè®­ç»ƒepochéœ€æ±‚ä¸Šå‡ä¼˜äºä¼ ç»ŸTransformeråŠå…¶ä»–å¤§æ¨¡å‹åŸºçº¿ï¼ŒéªŒè¯å…¶é‡‘èåº”ç”¨æ½œåŠ›ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Sentiment Analysis",
      "Transformer"
    ],
    "key_contributions": [
      "æå‡ºç»“åˆå¸¦å™ªå£°åµŒå…¥æŒ‡ä»¤å¾®è°ƒã€rLoRAåŠFlashAttentionçš„Qwen3-8Bä¼˜åŒ–æ–¹æ³•ï¼Œæå‡é‡‘èæ–‡æœ¬åˆ†ç±»çš„è®­ç»ƒæ•ˆç‡ä¸é²æ£’æ€§",
      "åŸºå‡†å®éªŒéªŒè¯Qwen3-8Båœ¨é‡‘èæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»ŸTransformeråŠå…¶ä»–å¤§æ¨¡å‹åŸºçº¿ï¼Œå‡†ç¡®ç‡æ›´é«˜ä¸”è®­ç»ƒepochæ›´å°‘"
    ],
    "processed_at": "2025-12-02T08:41:35.978241"
  },
  {
    "id": "2512.00448v1",
    "title": "Efficient Calibration in the rough Bergomi model by Wasserstein distance",
    "abstract": "Despite the empirical success in modeling volatility of the rough Bergomi (rBergomi) model, it suffers from pricing and calibration difficulties stemming from its non-Markovian structure. To address this, we propose a comprehensive computational framework that enhances both simulation and calibration. First, we develop a modified Sum-of-Exponentials (mSOE) Monte Carlo scheme which hybridizes an exact simulation of the singular kernel near the origin with a multi-factor approximation for the remainder. This method achieves high accuracy, particularly for out-of-the-money options, with an $\\mathcal{O}(n)$ computational cost. Second, based on this efficient pricing engine, we then propose a distribution-matching calibration scheme by using Wasserstein distance as the optimization objective. This leverages a minimax formulation against Lipschitz payoffs, which effectively distributes pricing errors and improving robustness. Our numerical results confirm the mSOE scheme's convergence and demonstrate that the calibration algorithm reliably identifies model parameters and generalizes well to path-dependent options, which offers a powerful and generic tool for practical model fitting.",
    "authors": [
      "Changqing Teng",
      "Guanglian Li"
    ],
    "published": "2025-11-29",
    "categories": [
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00448v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00448v1",
    "fetched_at": "2025-12-02T08:34:38.678681",
    "chinese_title": "åŸºäºWassersteinè·ç¦»çš„ç²—ç³™Bergomiæ¨¡å‹é«˜æ•ˆæ ¡å‡†",
    "chinese_summary": "é’ˆå¯¹ç²—ç³™Bergomiæ¨¡å‹å› éé©¬å°”å¯å¤«ç»“æ„å¯¼è‡´çš„å®šä»·ä¸æ ¡å‡†éš¾é¢˜ï¼Œè®ºæ–‡æå‡ºæ”¹è¿›çš„æŒ‡æ•°å’Œï¼ˆmSOEï¼‰è’™ç‰¹å¡æ´›æ–¹æ¡ˆï¼Œæ··åˆåŸç‚¹é™„è¿‘å¥‡å¼‚æ ¸ç²¾ç¡®æ¨¡æ‹Ÿä¸å‰©ä½™éƒ¨åˆ†å¤šå› å­è¿‘ä¼¼ï¼Œå®ç°O(n)æˆæœ¬ä¸‹çš„é«˜ç²¾åº¦å®šä»·ï¼ˆå°¤å…¶è™šå€¼æœŸæƒï¼‰ï¼›å¹¶åŸºäºè¯¥å®šä»·å¼•æ“ï¼Œä»¥Wassersteinè·ç¦»ä¸ºä¼˜åŒ–ç›®æ ‡è®¾è®¡åˆ†å¸ƒåŒ¹é…æ ¡å‡†æ–¹æ¡ˆï¼Œåˆ©ç”¨Lipschitzæ”¶ç›Šçš„minimaxå…¬å¼æå‡é²æ£’æ€§ï¼Œæ•°å€¼å®éªŒéªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§åŠå¯¹è·¯å¾„ä¾èµ–æœŸæƒçš„æ³›åŒ–èƒ½åŠ›ã€‚",
    "tags": [
      "Volatility",
      "Options",
      "Asset Pricing"
    ],
    "key_contributions": [
      "æå‡ºæ”¹è¿›çš„æŒ‡æ•°å’Œï¼ˆmSOEï¼‰è’™ç‰¹å¡æ´›æ–¹æ¡ˆï¼Œæ··åˆå¥‡å¼‚æ ¸ç²¾ç¡®æ¨¡æ‹Ÿä¸å¤šå› å­è¿‘ä¼¼ï¼Œå®ç°O(n)æˆæœ¬ä¸‹çš„é«˜ç²¾åº¦å®šä»·ï¼ˆå°¤å…¶è™šå€¼æœŸæƒï¼‰",
      "åŸºäºè¯¥å®šä»·å¼•æ“ï¼Œä»¥Wassersteinè·ç¦»ä¸ºä¼˜åŒ–ç›®æ ‡è®¾è®¡åˆ†å¸ƒåŒ¹é…æ ¡å‡†æ–¹æ¡ˆï¼Œåˆ©ç”¨Lipschitzæ”¶ç›Šçš„minimaxå…¬å¼æå‡æ ¡å‡†é²æ£’æ€§å¹¶æ”¯æŒè·¯å¾„ä¾èµ–æœŸæƒæ³›åŒ–"
    ],
    "processed_at": "2025-12-02T08:41:58.947231"
  },
  {
    "id": "2512.00346v1",
    "title": "Convergence Rates of Turnpike Theorems for Portfolio Choice in Stochastic Factor Models",
    "abstract": "Turnpike theorems state that if an investor's utility is asymptotically equivalent to a power utility, then the optimal investment strategy converges to the CRRA strategy as the investment horizon tends to infinity. This paper aims to derive the convergence rates of the turnpike theorem for optimal feedback functions in stochastic factor models. In these models, optimal feedback functions can be decomposed into two terms: myopic portfolios and excess hedging demands. We obtain convergence rates for myopic portfolios in nonlinear stochastic factor models and for excess hedging demands in quadratic term structure models, where the interest rate is a quadratic function of a multivariate Ornstein-Uhlenbeck process. We show that the convergence rates are determined by (i) the decay speed of the price of a zero-coupon bond and (ii) how quickly the investor's utility becomes power-like at high levels of wealth. As an application, we consider optimal collective investment problems and show that sharing rules for terminal wealth affect convergence rates.",
    "authors": [
      "Hiroki Yamamichi"
    ],
    "published": "2025-11-29",
    "categories": [
      "q-fin.PM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00346v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00346v1",
    "fetched_at": "2025-12-02T08:34:38.678699",
    "chinese_title": "éšæœºå› å­æ¨¡å‹ä¸­æŠ•èµ„ç»„åˆé€‰æ‹©çš„å¤§é“å®šç†æ”¶æ•›é€Ÿç‡",
    "chinese_summary": "æœ¬æ–‡ç ”ç©¶éšæœºå› å­æ¨¡å‹ä¸­æŠ•èµ„ç»„åˆé€‰æ‹©å¤§é“å®šç†çš„æ”¶æ•›é€Ÿç‡ï¼Œå°†æœ€ä¼˜åé¦ˆå‡½æ•°åˆ†è§£ä¸ºè¿‘è§†æŠ•èµ„ç»„åˆä¸è¶…é¢å¯¹å†²éœ€æ±‚ï¼Œåˆ†åˆ«åœ¨éçº¿æ€§éšæœºå› å­æ¨¡å‹å’ŒäºŒæ¬¡æœŸé™ç»“æ„æ¨¡å‹ï¼ˆåˆ©ç‡ä¸ºå¤šå…ƒOUè¿‡ç¨‹äºŒæ¬¡å‡½æ•°ï¼‰ä¸­æ¨å¯¼æ”¶æ•›é€Ÿç‡ï¼Œå¹¶åº”ç”¨äºæœ€ä¼˜é›†ä½“æŠ•èµ„é—®é¢˜åˆ†æç»ˆç«¯è´¢å¯Œåˆ†äº«è§„åˆ™å¯¹æ”¶æ•›é€Ÿç‡çš„å½±å“ã€‚",
    "tags": [
      "Factor Model",
      "Portfolio Optimization"
    ],
    "key_contributions": [
      "æ¨å¯¼éšæœºå› å­æ¨¡å‹ä¸­æœ€ä¼˜åé¦ˆå‡½æ•°å¤§é“å®šç†çš„æ”¶æ•›é€Ÿç‡ï¼Œåˆ†è§£ä¸ºè¿‘è§†æŠ•èµ„ç»„åˆå’Œè¶…é¢å¯¹å†²éœ€æ±‚å¹¶åˆ†åˆ«åœ¨éçº¿æ€§æ¨¡å‹ä¸äºŒæ¬¡æœŸé™ç»“æ„æ¨¡å‹ä¸­å¾—åˆ°é€Ÿç‡",
      "å°†ç»“æœåº”ç”¨äºæœ€ä¼˜é›†ä½“æŠ•èµ„é—®é¢˜ï¼Œæ­ç¤ºç»ˆç«¯è´¢å¯Œåˆ†äº«è§„åˆ™å¯¹æ”¶æ•›é€Ÿç‡çš„å½±å“"
    ],
    "processed_at": "2025-12-02T08:42:15.967965"
  },
  {
    "id": "2512.00299v1",
    "title": "Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network",
    "abstract": "We investigate the static portfolio selection problem of S-shaped and non-concave utility maximization under first-order and second-order stochastic dominance (SD) constraints. In many S-shaped utility optimization problems, one should require a liquidation boundary to guarantee the existence of a finite concave envelope function. A first-order SD (FSD) constraint can replace this requirement and provide an alternative for risk management. We explicitly solve the optimal solution under a general S-shaped utility function with a first-order stochastic dominance constraint. However, the second-order SD (SSD) constrained problem under non-concave utilities is difficult to solve analytically due to the invalidity of Sion's maxmin theorem. For this sake, we propose a numerical algorithm to obtain a plausible and sub-optimal solution for general non-concave utilities. The key idea is to detect the poor performance region with respect to the SSD constraints, characterize its structure and modify the distribution on that region to obtain (sub-)optimality. A key financial insight is that the decision maker should follow the SD constraint on the poor performance scenario while conducting the unconstrained optimal strategy otherwise. We provide numerical experiments to show that our algorithm effectively finds a sub-optimal solution in many cases. Finally, we develop an algorithm-guided piecewise-neural-network framework to learn the solution of the SSD problem, which demonstrates accelerated convergence compared to standard neural network approaches.",
    "authors": [
      "Zeyun Hu",
      "Yang Liu"
    ],
    "published": "2025-11-29",
    "categories": [
      "q-fin.MF",
      "cs.LG",
      "q-fin.PM",
      "q-fin.RM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00299v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00299v1",
    "fetched_at": "2025-12-02T08:34:38.678721",
    "chinese_title": "å¸¦Så½¢æ•ˆç”¨çš„éšæœºå ä¼˜çº¦æŸä¼˜åŒ–ï¼šå·®è¡¨ç°åŒºåŸŸç®—æ³•ä¸ç¥ç»ç½‘ç»œ",
    "chinese_summary": "è®ºæ–‡ç ”ç©¶å¸¦ä¸€é˜¶å’ŒäºŒé˜¶éšæœºå ä¼˜ï¼ˆSDï¼‰çº¦æŸçš„Så½¢éå‡¹æ•ˆç”¨é™æ€æŠ•èµ„ç»„åˆé€‰æ‹©é—®é¢˜ï¼Œæ˜ç¡®æ±‚è§£ä¸€é˜¶SDçº¦æŸä¸‹ä¸€èˆ¬Så½¢æ•ˆç”¨çš„æœ€ä¼˜è§£ï¼›é’ˆå¯¹äºŒé˜¶SDçº¦æŸé—®é¢˜ï¼ˆå› Sionå®šç†æ— æ•ˆéš¾ä»¥è§£ææ±‚è§£ï¼‰ï¼Œæå‡ºå·®è¡¨ç°åŒºåŸŸç®—æ³•æ‰¾æ¬¡ä¼˜è§£ï¼Œå¹¶å¼€å‘ç®—æ³•å¼•å¯¼çš„åˆ†æ®µç¥ç»ç½‘ç»œæ¡†æ¶å­¦ä¹ è¯¥é—®é¢˜è§£ã€‚",
    "tags": [
      "Portfolio Optimization",
      "Risk Management",
      "Deep Learning",
      "Behavioral Finance"
    ],
    "key_contributions": [
      "æ˜ç¡®æ±‚è§£ä¸€é˜¶éšæœºå ä¼˜çº¦æŸä¸‹ä¸€èˆ¬Så½¢æ•ˆç”¨çš„æœ€ä¼˜æŠ•èµ„ç»„åˆè§£",
      "é’ˆå¯¹äºŒé˜¶éšæœºå ä¼˜çº¦æŸä¸‹éå‡¹æ•ˆç”¨é—®é¢˜ï¼Œæå‡ºå·®è¡¨ç°åŒºåŸŸç®—æ³•æ‰¾æ¬¡ä¼˜è§£å¹¶å¼€å‘ç®—æ³•å¼•å¯¼çš„åˆ†æ®µç¥ç»ç½‘ç»œæ¡†æ¶å­¦ä¹ å…¶è§£"
    ],
    "processed_at": "2025-12-02T08:42:28.456989"
  },
  {
    "id": "2512.00280v1",
    "title": "Retail Investor Horizon and Earnings Announcements",
    "abstract": "We examine whether retail investor investment horizons explain earnings-related return patterns. Using StockTwits posts (2010--2021), we classify stocks as long- or short-horizon prior to earnings. We find horizon composition strongly predicts price paths: long-horizon stocks exhibit larger immediate reactions and pronounced post-announcement drift compared to short-horizon stocks. A strategy buying long-horizon and shorting short-horizon stocks generates 0.43% monthly alpha. Additionally, elevated pre-event sentiment predicts weaker subsequent performance, particularly for short-horizon stocks. These results confirm that retail horizon composition provides a useful dimension for summarizing systematic variation in earnings returns and extracting information from retail activity.",
    "authors": [
      "Domonkos F. Vamossy"
    ],
    "published": "2025-11-29",
    "categories": [
      "q-fin.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00280v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00280v1",
    "fetched_at": "2025-12-02T08:34:38.678740",
    "chinese_title": "é›¶å”®æŠ•èµ„è€…æŠ•èµ„æœŸé™ä¸ç›ˆä½™å…¬å‘Š",
    "chinese_summary": "æœ¬æ–‡åˆ©ç”¨2010-2021å¹´StockTwitså¸–å­å°†è‚¡ç¥¨åˆ†ä¸ºé•¿/çŸ­æœŸé™ï¼Œå‘ç°æœŸé™ç»“æ„æ˜¾è‘—é¢„æµ‹ç›ˆä½™å…¬å‘Šç›¸å…³å›æŠ¥æ¨¡å¼ï¼ˆé•¿æœŸé™è‚¡ç¥¨å³æ—¶ååº”æ›´å¤§ã€å…¬å‘Šåæ¼‚ç§»æ›´æ˜æ˜¾ï¼‰ï¼›ç›¸å…³å¤šç©ºç­–ç•¥æœˆalphaè¾¾0.43%ï¼Œä¸”äº‹å‰æƒ…ç»ªå¯¹çŸ­æœŸé™è‚¡ç¥¨åç»­è¡¨ç°å½±å“æ›´æ˜¾è‘—ï¼Œè¯æ˜é›¶å”®æœŸé™ç»“æ„æ˜¯ç›ˆä½™å›æŠ¥å˜å¼‚çš„æœ‰ç”¨ç»´åº¦ã€‚",
    "tags": [
      "Behavioral Finance",
      "Investor Sentiment",
      "Asset Pricing",
      "Anomaly"
    ],
    "key_contributions": [
      "åˆ©ç”¨ç¤¾äº¤å¹³å°æ•°æ®åˆ†ç±»é›¶å”®æŠ•èµ„è€…æœŸé™ï¼Œé¦–æ¬¡æ­ç¤ºå…¶å¯¹ç›ˆä½™å…¬å‘Šå›æŠ¥è·¯å¾„çš„é¢„æµ‹ä½œç”¨",
      "æ„å»ºçš„å¤šç©ºç­–ç•¥äº§ç”Ÿæ˜¾è‘—æœˆalphaï¼Œä¸”æ˜ç¡®äº‹å‰æƒ…ç»ªå¯¹ä¸åŒæœŸé™è‚¡ç¥¨åç»­è¡¨ç°çš„å¼‚è´¨æ€§å½±å“"
    ],
    "processed_at": "2025-12-02T08:42:45.968466"
  },
  {
    "id": "2512.00142v1",
    "title": "DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions",
    "abstract": "This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.",
    "authors": [
      "Swati Sachan",
      "Dale S. Fickett"
    ],
    "published": "2025-11-28",
    "categories": [
      "cs.CR",
      "cs.AI",
      "q-fin.CP",
      "q-fin.GN"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00142v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00142v1",
    "fetched_at": "2025-12-02T08:34:38.678762",
    "chinese_title": "DeFi TrustBoostï¼šåŒºå—é“¾ä¸AIèµ‹èƒ½å¯ä¿¡å»ä¸­å¿ƒåŒ–é‡‘èå†³ç­–",
    "chinese_summary": "æœ¬æ–‡æå‡ºDeFi TrustBoostæ¡†æ¶ï¼Œç»“åˆåŒºå—é“¾ä¸å¯è§£é‡ŠAIè§£å†³ä½è´¢å¯Œå®¶åº­å°å¾®ä¼ä¸šè´·æ¬¾å®¡æ‰¹çš„å¯ä¿¡å†³ç­–é—®é¢˜ï¼›è¯¥æ¡†æ¶æ»¡è¶³æœºå¯†æ€§ã€æ•°æ®ä¿æŠ¤åˆè§„ç­‰4é¡¹å…³é”®è¦æ±‚ï¼Œè¿˜æå‡ºAIå†³ç­–é˜²ç¯¡æ”¹å®¡è®¡æŠ€æœ¯åŠé“¾ä¸Šé“¾ä¸‹æ•°æ®å­˜å‚¨ç­–ç•¥ä»¥æ”¯æŒé‡‘èæœºæ„åä½œã€‚",
    "tags": [
      "Risk Management",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºDeFi TrustBoostæ¡†æ¶ï¼Œç»“åˆåŒºå—é“¾ä¸å¯è§£é‡ŠAIè§£å†³ä½è´¢å¯Œå®¶åº­å°å¾®ä¼ä¸šè´·æ¬¾å®¡æ‰¹çš„å¯ä¿¡å†³ç­–é—®é¢˜ï¼Œæ»¡è¶³æœºå¯†æ€§ã€æ•°æ®åˆè§„ç­‰4é¡¹å…³é”®è¦æ±‚",
      "æå‡ºAIå†³ç­–é˜²ç¯¡æ”¹å®¡è®¡æŠ€æœ¯åŠé“¾ä¸Šé“¾ä¸‹æ•°æ®å­˜å‚¨ç­–ç•¥ï¼Œæ”¯æŒé‡‘èæœºæ„å†…å¤–éƒ¨åä½œ"
    ],
    "processed_at": "2025-12-02T08:43:08.679927"
  },
  {
    "id": "2512.00243v1",
    "title": "Optimizing Information Asset Investment Strategies in the Exploratory Phase of the Oil and Gas Industry: A Reinforcement Learning Approach",
    "abstract": "Our work investigates the economic efficiency of the prevailing \"ladder-step\" investment strategy in oil and gas exploration, which advocates for the incremental acquisition of geological information throughout the project lifecycle. By employing a multi-agent Deep Reinforcement Learning (DRL) framework, we model an alternative strategy that prioritizes the early acquisition of high-quality information assets. We simulate the entire upstream value chain-comprising competitive bidding, exploration, and development phases-to evaluate the economic impact of this approach relative to traditional methods. Our results demonstrate that front-loading information investment significantly reduces the costs associated with redundant data acquisition and enhances the precision of reserve valuation. Specifically, we find that the alternative strategy outperforms traditional methods in highly competitive environments by mitigating the \"winner's curse\" through more accurate bidding. Furthermore, the economic benefits are most pronounced during the development phase, where superior data quality minimizes capital misallocation. These findings suggest that optimal investment timing is structurally dependent on market competition rather than solely on price volatility, offering a new paradigm for capital allocation in extractive industries.",
    "authors": [
      "Paulo Roberto de Melo Barros Junior",
      "Monica Alexandra Vilar Ribeiro De Meireles",
      "Jose Luis Lima de Jesus Silva"
    ],
    "published": "2025-11-28",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00243v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00243v1",
    "fetched_at": "2025-12-02T08:34:42.048152",
    "chinese_title": "æ²¹æ°”è¡Œä¸šå‹˜æ¢é˜¶æ®µä¿¡æ¯èµ„äº§æŠ•èµ„ç­–ç•¥ä¼˜åŒ–ï¼šä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
    "chinese_summary": "æœ¬æ–‡é‡‡ç”¨å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¡†æ¶ï¼Œå¯¹æ¯”æ²¹æ°”å‹˜æ¢ä¸­ä¼ ç»Ÿé˜¶æ¢¯å¼ç­–ç•¥ä¸ä¼˜å…ˆæ—©æœŸé«˜è´¨é‡ä¿¡æ¯èµ„äº§çš„ç­–ç•¥ï¼Œå‘ç°å‰ç«¯åŠ è½½ä¿¡æ¯æŠ•èµ„å¯é™ä½å†—ä½™æ•°æ®æˆæœ¬ã€æå‡å‚¨é‡ä¼°å€¼ç²¾åº¦ï¼›ç ”ç©¶è¿˜æ­ç¤ºæœ€ä¼˜æŠ•èµ„æ—¶æœºä¾èµ–å¸‚åœºç«äº‰è€Œéä»…ä»·æ ¼æ³¢åŠ¨ï¼Œç«äº‰ç¯å¢ƒä¸­èƒ½ç¼“è§£èµ¢å®¶è¯…å’’ï¼Œå¼€å‘é˜¶æ®µæ”¶ç›Šæ›´æ˜¾è‘—ã€‚",
    "tags": [
      "Reinforcement Learning",
      "Portfolio Optimization",
      "Market Microstructure",
      "Asset Pricing"
    ],
    "key_contributions": [
      "æ­ç¤ºæœ€ä¼˜ä¿¡æ¯èµ„äº§æŠ•èµ„æ—¶æœºä¾èµ–å¸‚åœºç«äº‰è€Œéä»…ä»·æ ¼æ³¢åŠ¨ï¼Œç«äº‰ç¯å¢ƒä¸­å¯ç¼“è§£èµ¢å®¶è¯…å’’ï¼Œå¼€å‘é˜¶æ®µæ”¶ç›Šæ›´çªå‡º"
    ],
    "processed_at": "2025-12-02T08:43:27.829820"
  },
  {
    "id": "2512.02010v1",
    "title": "Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling",
    "abstract": "As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.",
    "authors": [
      "Jack Cook",
      "Junxian Guo",
      "Guangxuan Xiao",
      "Yujun Lin",
      "Song Han"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.02010v1",
    "arxiv_url": "https://arxiv.org/abs/2512.02010v1",
    "fetched_at": "2025-12-02T08:34:45.792678",
    "chinese_title": "å››æ¯”å…­ï¼šåŸºäºè‡ªé€‚åº”å—ç¼©æ”¾çš„æ›´ç²¾ç¡®NVFP4é‡åŒ–æ–¹æ³•",
    "chinese_summary": "é’ˆå¯¹NVFP4é‡åŒ–åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­æ˜“å¯¼è‡´å‘æ•£ã€æ¨ç†æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè®ºæ–‡æå‡º4/6ä¿®æ”¹æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªå€¼å—è¯„ä¼°ä¸¤ä¸ªæ½œåœ¨ç¼©æ”¾å› å­ï¼Œç¼“è§£FP4è¿‘æœ€å¤§å€¼çš„é‡åŒ–è¯¯å·®ï¼Œä½¿è¡¨ç¤ºæ›´å‡åŒ€ï¼›è¯¥æ–¹æ³•å¯é«˜æ•ˆåœ¨NVIDIA Blackwell GPUå®ç°ï¼Œé¢„è®­ç»ƒå®éªŒä¸­é¿å…å‘æ•£ï¼Œè®­ç»ƒæŸå¤±æ›´æ¥è¿‘BF16åŸºå‡†ã€‚",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡º4/6 NVFP4é‡åŒ–ä¿®æ”¹ï¼Œé€šè¿‡è¯„ä¼°åŒç¼©æ”¾å› å­ç¼“è§£è¿‘æœ€å¤§å€¼é‡åŒ–è¯¯å·®ï¼Œæå‡è¡¨ç¤ºç²¾åº¦"
    ],
    "processed_at": "2025-12-02T08:43:41.088631"
  },
  {
    "id": "2512.01870v1",
    "title": "Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees",
    "abstract": "We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \\mathbb{N}\\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\\mathbb{N}\\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.",
    "authors": [
      "Alessandro Breccia",
      "Federica Gerace",
      "Marco Lippi",
      "Gabriele Sicuro",
      "Pierluigi Contucci"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.AI",
      "cond-mat.dis-nn",
      "math-ph",
      "math.NT"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01870v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01870v1",
    "fetched_at": "2025-12-02T08:34:45.792718",
    "chinese_title": "æµ‹è¯•Transformeråœ¨æœ‰æ ¹æ ‘ç®—æœ¯åºåˆ—ä¸Šçš„å¯å­¦ä¹ æ€§",
    "chinese_summary": "è¯¥è®ºæ–‡å°†è‡ªç„¶æ•°é€šè¿‡è¿­ä»£ç´ å› æ•°åˆ†è§£æ˜ å°„ä¸ºæœ‰æ ¹å¹³é¢æ ‘å¾—åˆ°åºåˆ—â„•ğ’¯ï¼Œç”¨GPT-2æ¶æ„çš„Transformerä»å¤´è®­ç»ƒå‰10Â¹Â¹ä¸ªå…ƒç´ ï¼Œæµ‹è¯•ä¸‹ä¸€è¯å’Œæ©ç è¯é¢„æµ‹ä»»åŠ¡ï¼›ç»“æœæ˜¾ç¤ºæ¨¡å‹éƒ¨åˆ†å­¦ä¹ äº†â„•ğ’¯çš„å†…éƒ¨è¯­æ³•ï¼Œæ•æ‰åˆ°éå¹³å‡¡è§„å¾‹å’Œç›¸å…³æ€§ï¼Œè¡¨æ˜å¯å­¦ä¹ æ€§å¯èƒ½å»¶ä¼¸åˆ°ç®—æœ¯ç»“æ„æœ¬èº«ã€‚",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "æ„å»ºäº†è‡ªç„¶æ•°ç´ å› æ•°åˆ†è§£å¯¹åº”çš„æœ‰æ ¹æ ‘åºåˆ—â„•ğ’¯ï¼Œä½œä¸ºç®—æœ¯ç»“æ„çš„å¯å­¦ä¹ æµ‹è¯•å¯¹è±¡",
      "éªŒè¯äº†GPT-2æ¶æ„Transformerèƒ½éƒ¨åˆ†å­¦ä¹ â„•ğ’¯çš„å†…éƒ¨è¯­æ³•ï¼Œæ•æ‰éå¹³å‡¡è§„å¾‹ï¼Œè¡¨æ˜LLMå¯å­¦ä¹ æ€§å¯å»¶ä¼¸è‡³ç®—æœ¯ç»“æ„"
    ],
    "processed_at": "2025-12-02T08:43:57.166340"
  },
  {
    "id": "2512.01809v1",
    "title": "Much Ado About Noising: Dispelling the Myths of Generative Robotic Control",
    "abstract": "Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/",
    "authors": [
      "Chaoyi Pan",
      "Giri Anantharaman",
      "Nai-Chieh Huang",
      "Claire Jin",
      "Daniel Pfrommer",
      "Chenyang Yuan",
      "Frank Permenter",
      "Guannan Qu",
      "Nicholas Boffi",
      "Guanya Shi",
      "Max Simchowitz"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01809v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01809v1",
    "fetched_at": "2025-12-02T08:34:45.792760",
    "chinese_title": "å–§åš£çš„å™ªå£°ï¼šç ´é™¤ç”Ÿæˆå¼æœºå™¨äººæ§åˆ¶çš„è¿·æ€",
    "chinese_summary": "æœ¬æ–‡å¯¹æµè¡Œç”Ÿæˆå¼æ§åˆ¶ç­–ç•¥ï¼ˆGCPsï¼‰åœ¨è¡Œä¸ºå…‹éš†åŸºå‡†ä¸Šå¼€å±•å…¨é¢è¯„ä¼°ï¼Œå‘ç°å…¶æˆåŠŸå¹¶éæºäºå¤šæ¨¡æ€æ•æ‰æˆ–å¤æ‚è§‚æµ‹-åŠ¨ä½œæ˜ å°„ï¼Œè€Œæ˜¯ä¾èµ–å¸¦è®­ç»ƒç›‘ç£ä¸­é—´æ­¥éª¤ä¸é€‚å½“éšæœºæ€§çš„è¿­ä»£è®¡ç®—ï¼›è®¾è®¡çš„è½»é‡ä¸¤æ­¥å›å½’æœ€å°è¿­ä»£ç­–ç•¥ï¼ˆMIPï¼‰æ€§èƒ½åŒ¹é…æµGCPä¸”å¸¸ä¼˜äºè’¸é¦æ·å¾„æ¨¡å‹ï¼ŒæŒ‡å‡ºGCPåˆ†å¸ƒæ‹Ÿåˆç»„ä»¶çš„éæ ¸å¿ƒåœ°ä½ï¼ŒæŒ‡å‘ä»…å…³æ³¨æ§åˆ¶æ€§èƒ½çš„æ–°è®¾è®¡ç©ºé—´ã€‚",
    "tags": [
      "Deep Learning",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "æ­ç¤ºç”Ÿæˆå¼æ§åˆ¶ç­–ç•¥ï¼ˆGCPsï¼‰çš„æˆåŠŸæ ¸å¿ƒæ˜¯å¸¦ç›‘ç£ä¸­é—´æ­¥éª¤ä¸é€‚å½“éšæœºæ€§çš„è¿­ä»£è®¡ç®—ï¼Œè€Œéå¤šæ¨¡æ€æ•æ‰æˆ–å¤æ‚è§‚æµ‹-åŠ¨ä½œæ˜ å°„",
      "æå‡ºè½»é‡ä¸¤æ­¥å›å½’çš„æœ€å°è¿­ä»£ç­–ç•¥ï¼ˆMIPï¼‰ï¼Œæ€§èƒ½åŒ¹é…æµGCPä¸”å¸¸ä¼˜äºè’¸é¦æ·å¾„æ¨¡å‹ï¼ŒæŒ‡å‡ºåˆ†å¸ƒæ‹Ÿåˆç»„ä»¶çš„éæ ¸å¿ƒåœ°ä½å¹¶æ‹“å±•æ§åˆ¶ç­–ç•¥è®¾è®¡ç©ºé—´"
    ],
    "processed_at": "2025-12-02T08:44:18.248517"
  },
  {
    "id": "2512.01725v1",
    "title": "Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks",
    "abstract": "Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \\textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \\textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \\textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.",
    "authors": [
      "Jiannan Guan",
      "Qiguang Chen",
      "Libo Qin",
      "Dengyun Peng",
      "Jinhao Liu",
      "Liangyu Huo",
      "Jian Xie",
      "Wanxiang Che"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01725v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01725v1",
    "fetched_at": "2025-12-02T08:34:45.792795",
    "chinese_title": "è­¦æƒ•æ¨ç†è¿‡åº¦è‡ªä¿¡ï¼šå¤šè§£ä»»åŠ¡æ¨ç†è¿‡ç¨‹ä¸­çš„é™·é˜±",
    "chinese_summary": "è¯¥è®ºæ–‡æŒ‡å‡ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè§£ä»»åŠ¡ä¸­å› æ¨ç†è¿‡åº¦è‡ªä¿¡è¡¨ç°ä¸ä½³ï¼Œæå‡ºå¤šè§£ä»»åŠ¡åŸºå‡†MuSoBenchï¼›å®éªŒè¡¨æ˜é•¿é“¾æ€ç»´ï¼ˆLong-CoTï¼‰é€šè¿‡è¿­ä»£æ¢ç´¢å’Œè‡ªæˆ‘åæ€ç¼“è§£è¯¥é—®é¢˜ï¼Œè¿›ä¸€æ­¥æå‡ºè®¤çŸ¥åˆšæ€§å‡è®¾è§£é‡Šå…¶æˆå› ï¼Œå¹¶ç”¨æ³¨æ„åŠ›ç†µåˆ†ææä¾›åˆæ­¥æ”¯æŒï¼Œå¼ºè°ƒéœ€æ‹“å±•LLMæ¨ç†è¯„ä¼°ç»´åº¦ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºå¤šè§£ä»»åŠ¡åŸºå‡†MuSoBenchç”¨äºè¯„ä¼°LLMçš„å¤šè§£æ¨ç†èƒ½åŠ›",
      "æ­ç¤ºLong-CoTç¼“è§£LLMæ¨ç†è¿‡åº¦è‡ªä¿¡çš„æœºåˆ¶å¹¶æå‡ºè®¤çŸ¥åˆšæ€§å‡è®¾è§£é‡Šå…¶æˆå› "
    ],
    "processed_at": "2025-12-02T08:44:36.681994"
  },
  {
    "id": "2512.01723v1",
    "title": "Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation",
    "abstract": "Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.",
    "authors": [
      "Saba Kublashvili"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.AI",
      "cs.GT",
      "math.PR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01723v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01723v1",
    "fetched_at": "2025-12-02T08:34:45.792817",
    "chinese_title": "é¢å‘ç¨€ç–å†å²æ•°æ®çš„æ¦‚ç‡ç¥ç»ç¬¦å·æ¨ç†ï¼šèåˆè´å¶æ–¯æ¨æ–­ã€å› æœæ¨¡å‹ä¸åšå¼ˆè®ºåˆ†é…çš„æ¡†æ¶",
    "chinese_summary": "è®ºæ–‡é’ˆå¯¹å†å²äº‹ä»¶å»ºæ¨¡ä¸­æ•°æ®ç¨€ç–ï¼ˆN<<100ï¼‰ã€å¼‚è´¨å™ªå£°ã€ç¼ºå¤±åäº‹å®åŠéœ€å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºHistoricalMLæ¡†æ¶ï¼Œèåˆè´å¶æ–¯ä¸ç¡®å®šæ€§é‡åŒ–ã€ç»“æ„å› æœæ¨¡å‹ã€åˆä½œåšå¼ˆè®ºï¼ˆæ²™æ™®åˆ©å€¼ï¼‰ä¸æ³¨æ„åŠ›ç¥ç»æ¶æ„ï¼›ç†è®ºè¯æ˜ç¨€ç–æ•°æ®ä¸‹ç»“åˆé¢†åŸŸå¼ºå…ˆéªŒå¯ä¸€è‡´ä¼°è®¡ï¼Œæ²™æ™®åˆ©åˆ†é…æ»¡è¶³å…¬ç†å…¬å¹³æ€§ï¼Œä¼˜äºçº¯å›å½’æ–¹æ³•ï¼›å¹¶é€šè¿‡éæ´²ç“œåˆ†ã€å¸ƒåŒ¿æˆ˜äº‰æ¡ˆä¾‹éªŒè¯ã€‚",
    "tags": [
      "Deep Learning",
      "Factor Model",
      "Anomaly"
    ],
    "key_contributions": [
      "æå‡ºèåˆå¤šæ–¹æ³•çš„HistoricalMLæ¡†æ¶ï¼Œè§£å†³ç¨€ç–å†å²æ•°æ®å»ºæ¨¡çš„å…³é”®æŒ‘æˆ˜",
      "ç†è®ºè¯æ˜ç¨€ç–æ•°æ®ä¸‹ä¸€è‡´ä¼°è®¡æ€§åŠæ²™æ™®åˆ©åˆ†é…çš„å…¬ç†å…¬å¹³æ€§ï¼Œä¼˜äºçº¯å›å½’æ–¹æ³•"
    ],
    "processed_at": "2025-12-02T08:44:59.790341"
  },
  {
    "id": "2512.01716v1",
    "title": "Common Structure Discovery in Collections of Bipartite Networks: Application to Pollination Systems",
    "abstract": "Bipartite networks are widely used to encode the ecological interactions. Being able to compare the organization of bipartite networks is a first step toward a better understanding of how environmental factors shape community structure and resilience. Yet current methods for structure detection in bipartite networks overlook shared patterns across collections of networks. We introduce the \\emph{colBiSBM}, a family of probabilistic models for collections of bipartite networks that extends the classical Latent Block Model (LBM). The proposed framework assumes that networks are independent realizations of a shared mesoscale structure, encoded through common inter-block connectivity parameters. We establish identifiability conditions for the different variants of \\emph{colBiSBM} and develop a variational EM algorithm for parameter estimation, coupled with an adaptation of the Integrated Classification Likelihood (ICL) criterion for model selection. We demonstrate how our approach can be used to classify networks based on their topology or organization. Simulation studies highlight the ability of \\emph{colBiSBM} to recover common structures, improve clustering performance, and enhance link prediction by borrowing strength across networks. An application to plant--pollinator networks highlights how the method uncovers shared ecological roles and partitions networks into sub-collections with similar connectivity patterns. These results illustrate the methodological and practical advantages of joint modeling over separate network analyses in the study of bipartite systems.",
    "authors": [
      "Louis Lacoste",
      "Pierre Barbillon",
      "Sophie Donnet"
    ],
    "published": "2025-12-01",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01716v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01716v1",
    "fetched_at": "2025-12-02T08:34:45.792842",
    "chinese_title": "äºŒåˆ†ç½‘ç»œé›†åˆä¸­çš„å…±åŒç»“æ„å‘ç°ï¼šä¼ ç²‰ç³»ç»Ÿåº”ç”¨",
    "chinese_summary": "è®ºæ–‡æå‡ºcolBiSBMæ¨¡å‹ï¼Œæ‰©å±•ç»å…¸æ½œåœ¨å—æ¨¡å‹ï¼ˆLBMï¼‰ä»¥å¤„ç†äºŒåˆ†ç½‘ç»œé›†åˆï¼Œå‡è®¾ç½‘ç»œä¸ºå…±äº«ä¸­å°ºåº¦ç»“æ„çš„ç‹¬ç«‹å®ç°ï¼›é€šè¿‡å˜åˆ†EMç®—æ³•ä¼°è®¡å‚æ•°å¹¶ç»“åˆICLå‡†åˆ™é€‰æ‹©æ¨¡å‹ï¼Œæ¨¡æ‹Ÿä¸ä¼ ç²‰ç³»ç»Ÿåº”ç”¨éªŒè¯å…¶å¯æ¢å¤å…±åŒç»“æ„ã€æå‡èšç±»ä¸é“¾è·¯é¢„æµ‹æ€§èƒ½ã€‚",
    "tags": [
      "Graph Neural Network",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºcolBiSBMæ¨¡å‹ï¼Œæ‰©å±•LBMç”¨äºäºŒåˆ†ç½‘ç»œé›†åˆï¼Œæ•æ‰è·¨ç½‘ç»œçš„å…±äº«ä¸­å°ºåº¦ç»“æ„",
      "å»ºç«‹æ¨¡å‹å¯è¯†åˆ«æ€§æ¡ä»¶ï¼Œå¼€å‘å˜åˆ†EM+ICLæ–¹æ³•ï¼ŒéªŒè¯å…¶åœ¨ç»“æ„æ¢å¤ã€èšç±»åŠé“¾è·¯é¢„æµ‹çš„ä¼˜åŠ¿"
    ],
    "processed_at": "2025-12-02T08:45:24.662113"
  },
  {
    "id": "2512.01653v1",
    "title": "Cuffless Blood Pressure Estimation from Six Wearable Sensor Modalities in Multi-Motion-State Scenarios",
    "abstract": "Cardiovascular disease (CVD) is a leading cause of morbidity and mortality worldwide, and sustained hypertension is an often silent risk factor, making cuffless continuous blood pressure (BP) monitoring with wearable devices important for early screening and long-term management. Most existing cuffless BP estimation methods use only photoplethysmography (PPG) and electrocardiography (ECG) signals, alone or in combination. These models are typically developed under resting or quasi-static conditions and struggle to maintain robust accuracy in multi-motion-state scenarios. In this study, we propose a six-modal BP estimation framework that jointly leverages ECG, multi-channel PPG, attachment pressure, sensor temperature, and triaxial acceleration and angular velocity. Each modality is processed by a lightweight branch encoder, contrastive learning enforces cross-modal semantic alignment, and a mixture-of-experts (MoE) regression head adaptively maps the fused features to BP across motion states. Comprehensive experiments on the public Pulse Transit Time PPG Dataset, which includes running, walking, and sitting data from 22 subjects, show that the proposed method achieves mean absolute errors (MAE) of 3.60 mmHg for systolic BP (SBP) and 3.01 mmHg for diastolic BP (DBP). From a clinical perspective, it attains Grade A for SBP, DBP, and mean arterial pressure (MAP) according to the British Hypertension Society (BHS) protocol and meets the numerical criteria of the Association for the Advancement of Medical Instrumentation (AAMI) standard for mean error (ME) and standard deviation of error (SDE).",
    "authors": [
      "Yiqiao Chen",
      "Fazheng Xu",
      "Zijian Huang",
      "Juchi He",
      "Zhenghui Feng"
    ],
    "published": "2025-12-01",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01653v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01653v1",
    "fetched_at": "2025-12-02T08:34:45.792872",
    "chinese_title": "å¤šè¿åŠ¨çŠ¶æ€åœºæ™¯ä¸‹åŸºäºå…­ç§å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ— è¢–å¸¦è¡€å‹ä¼°è®¡",
    "chinese_summary": "è®ºæ–‡é’ˆå¯¹ç°æœ‰æ— è¢–å¸¦è¡€å‹ä¼°è®¡å¤šä¾èµ–å•ä¸€/åŒæ¨¡æ€ä¸”ä»…é€‚ç”¨äºé™æ€åœºæ™¯çš„å±€é™ï¼Œæå‡ºèåˆECGã€å¤šé€šé“PPGç­‰å…­ç§ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ¡†æ¶ï¼Œé€šè¿‡è½»é‡åˆ†æ”¯ç¼–ç å™¨ã€å¯¹æ¯”å­¦ä¹ å¯¹é½è·¨æ¨¡æ€è¯­ä¹‰åŠæ··åˆä¸“å®¶å›å½’å¤´é€‚é…å¤šè¿åŠ¨çŠ¶æ€ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šå®ç°é«˜ç²¾åº¦ä¸”æ»¡è¶³ä¸´åºŠæ ‡å‡†ã€‚",
    "tags": [
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "æå‡ºèåˆå…­ç§ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ— è¢–å¸¦è¡€å‹ä¼°è®¡æ¡†æ¶ï¼Œçªç ´å•ä¸€/åŒæ¨¡æ€åŠé™æ€åœºæ™¯çš„å±€é™",
      "é‡‡ç”¨å¯¹æ¯”å­¦ä¹ å¯¹é½è·¨æ¨¡æ€è¯­ä¹‰ã€æ··åˆä¸“å®¶å›å½’å¤´æå‡å¤šè¿åŠ¨çŠ¶æ€ç²¾åº¦ï¼Œä¸”æ»¡è¶³BHS Grade Aä¸´åºŠæ ‡å‡†"
    ],
    "processed_at": "2025-12-02T08:45:40.826291"
  },
  {
    "id": "2512.01591v1",
    "title": "Scaling and context steer LLMs along the same computational path as the human brain",
    "abstract": "Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether and why this alignment score arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 22 LLMs varying in size and architecture type. Our analyses confirm that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, this study sheds light on the sequential nature of computations and the factors underlying the partial convergence between biological and artificial neural networks.",
    "authors": [
      "JosÃ©phine Raugel",
      "StÃ©phane d'Ascoli",
      "JÃ©rÃ©my Rapin",
      "Valentin Wyart",
      "Jean-RÃ©mi King"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01591v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01591v1",
    "fetched_at": "2025-12-02T08:34:45.792911",
    "chinese_title": "ç¼©æ”¾ä¸ä¸Šä¸‹æ–‡å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹éµå¾ªä¸äººç±»å¤§è„‘ç›¸åŒçš„è®¡ç®—è·¯å¾„",
    "chinese_summary": "è¯¥ç ”ç©¶ç»“åˆå‚ä¸è€…å¬10å°æ—¶æœ‰å£°ä¹¦çš„æ—¶é—´åˆ†è¾¨è„‘ä¿¡å·ï¼Œä¸22ä¸ªä¸åŒè§„æ¨¡å’Œæ¶æ„çš„LLMåŸºå‡†è”åˆåˆ†æï¼Œå‘ç°LLMä¸å¤§è„‘çš„è¡¨å¾ç”Ÿæˆé¡ºåºç›¸ä¼¼ï¼ˆåˆå§‹å±‚å¯¹åº”æ—©æœŸè„‘å“åº”ã€æ·±å±‚å¯¹åº”åæœŸï¼‰ï¼Œä¸”å¯¹é½æ€§è·¨Transformerå’Œå¾ªç¯æ¶æ„ä¸€è‡´ä½†ä¾èµ–æ¨¡å‹è§„æ¨¡ä¸ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ­ç¤ºäº†ç”Ÿç‰©ä¸äººå·¥ç¥ç»ç½‘ç»œè®¡ç®—çš„åºåˆ—æ€§åŠæ”¶æ•›å› ç´ ã€‚",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "è¯å®å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»å¤§è„‘çš„è¡¨å¾ç”Ÿæˆé¡ºåºå…·æœ‰ç›¸ä¼¼æ€§ï¼ˆLLMåˆå§‹å±‚å¯¹åº”æ—©æœŸè„‘å“åº”ï¼Œæ·±å±‚å¯¹åº”åæœŸï¼‰",
      "æ­ç¤ºè¯¥è„‘-LLMå¯¹é½æ€§è·¨æ¶æ„ä¸€è‡´ï¼Œä½†ä¾èµ–æ¨¡å‹è§„æ¨¡å’Œä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé˜æ˜ç”Ÿç‰©ä¸äººå·¥ç¥ç»ç½‘ç»œè®¡ç®—çš„åºåˆ—æ€§åŠæ”¶æ•›å› ç´ "
    ],
    "processed_at": "2025-12-02T08:45:55.531700"
  },
  {
    "id": "2512.01465v1",
    "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data",
    "abstract": "Water quality monitoring is a core component of ecological environmental protection. However, due to sensor failure or other inevitable factors, data missing often exists in long-term monitoring, posing great challenges in water quality analysis. This paper proposes a Neural Tucker Convolutional Network (NTCN) model for water quality data imputation, which features the following key components: a) Encode different mode entities into respective embedding vectors, and construct a Tucker interaction tensor by outer product operations to capture the complex mode-wise feature interactions; b) Use 3D convolution to extract fine-grained spatiotemporal features from the interaction tensor. Experiments on three real-world water quality datasets show that the proposed NTCN model outperforms several state-of-the-art imputation models in terms of accuracy.",
    "authors": [
      "Hongnan Si",
      "Tong Li",
      "Yujie Chen",
      "Xin Liao"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01465v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01465v1",
    "fetched_at": "2025-12-02T08:34:45.792938",
    "chinese_title": "åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„éçº¿æ€§ä½ç§©è¡¨ç¤ºæ¨¡å‹ç”¨äºæ°´è´¨æ•°æ®æ’è¡¥",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹æ°´è´¨ç›‘æµ‹ä¸­æ•°æ®ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºç¥ç»å¡”å…‹å·ç§¯ç½‘ç»œï¼ˆNTCNï¼‰æ¨¡å‹ç”¨äºæ°´è´¨æ•°æ®æ’è¡¥ï¼›è¯¥æ¨¡å‹é€šè¿‡ç¼–ç å¤šæ¨¡æ€å®ä½“å¹¶æ„å»ºå¡”å…‹äº¤äº’å¼ é‡æ•æ‰å¤æ‚ç‰¹å¾äº¤äº’ï¼Œç»“åˆ3Då·ç§¯æå–ç»†ç²’åº¦æ—¶ç©ºç‰¹å¾ï¼Œåœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šå®éªŒè¡¨æ˜å…¶ä¼˜äºç°æœ‰å…ˆè¿›æ’è¡¥æ¨¡å‹ã€‚",
    "tags": [
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "æå‡ºç¥ç»å¡”å…‹å·ç§¯ç½‘ç»œï¼ˆNTCNï¼‰æ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€å®ä½“åµŒå…¥ä¸å¡”å…‹äº¤äº’å¼ é‡æ„å»ºæ•æ‰å¤æ‚æ¨¡æ€ç‰¹å¾äº¤äº’",
      "åˆ©ç”¨3Då·ç§¯æå–äº¤äº’å¼ é‡ä¸­çš„ç»†ç²’åº¦æ—¶ç©ºç‰¹å¾ï¼ŒéªŒè¯æ¨¡å‹åœ¨çœŸå®æ°´è´¨æ•°æ®é›†ä¸Šçš„æ’è¡¥ç²¾åº¦ä¼˜äºç°æœ‰SOTAæ–¹æ³•"
    ],
    "processed_at": "2025-12-02T08:46:25.373620"
  },
  {
    "id": "2512.01461v1",
    "title": "Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging",
    "abstract": "Model merging has emerged as a promising paradigm for enabling multi-task capabilities without additional training. However, existing methods often experience substantial performance degradation compared with individually fine-tuned models, even on similar tasks, underscoring the need to preserve task-specific information. This paper proposes Decomposition, Thresholding, and Scaling (DTS), an approximation-based personalized merging framework that preserves task-specific information with minimal storage overhead. DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors. It then introduces a novel thresholding strategy that partitions singular vector elements into groups and assigns a scaling factor to each group. To enable generalization to unseen tasks, we further extend DTS with a variant that fuses task-specific information in a data-free manner based on the semantic similarity of task characteristics. Extensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines while requiring only 1\\% additional storage per task. Furthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance. Our code is available at https://github.com/krumpguo/DTS.",
    "authors": [
      "Kuangpu Guo",
      "Yuhe Ding",
      "Jian Liang",
      "Zilei Wang",
      "Ran He"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01461v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01461v1",
    "fetched_at": "2025-12-02T08:34:45.792966",
    "chinese_title": "ä¿æŒç‹¬ç‰¹ï¼Œä¿æŒé«˜æ•ˆï¼šå¤šä»»åŠ¡åˆå¹¶ä¸­ä¿ç•™æ¨¡å‹ä¸ªæ€§",
    "chinese_summary": "ç°æœ‰æ¨¡å‹åˆå¹¶æ–¹æ³•å¸¸å› ä¸¢å¤±ä»»åŠ¡ç‰¹å®šä¿¡æ¯å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œæœ¬æ–‡æå‡ºDTSæ¡†æ¶ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ä¿ç•™å…³é”®ä»»åŠ¡ä¿¡æ¯ï¼Œç»“åˆåˆ†ç»„é˜ˆå€¼ä¸ç¼©æ”¾å› å­ç­–ç•¥ï¼Œä»…éœ€1%é¢å¤–å­˜å‚¨å³å¯æå‡åˆå¹¶æ€§èƒ½ï¼›è¿›ä¸€æ­¥æ‰©å±•å‡ºæ— æ•°æ®å˜ä½“ï¼ŒåŸºäºä»»åŠ¡è¯­ä¹‰ç›¸ä¼¼æ€§å®ç°å¯¹æœªè§è¿‡ä»»åŠ¡çš„æ³›åŒ–ï¼Œå®éªŒéªŒè¯å…¶ä¼˜äºç°æœ‰SOTAæ–¹æ³•ã€‚",
    "tags": [
      "Deep Learning",
      "LLM"
    ],
    "key_contributions": [
      "æå‡ºDTSæ¡†æ¶ï¼Œé€šè¿‡SVD+é˜ˆå€¼ç¼©æ”¾ç­–ç•¥é«˜æ•ˆä¿ç•™ä»»åŠ¡ç‰¹å®šä¿¡æ¯ï¼Œä»…éœ€1%é¢å¤–å­˜å‚¨ä¸”æ€§èƒ½ä¼˜äºSOTAï¼›",
      "æ‰©å±•æ— æ•°æ®DTSå˜ä½“ï¼ŒåŸºäºä»»åŠ¡è¯­ä¹‰ç›¸ä¼¼æ€§å®ç°å¯¹æœªè§è¿‡ä»»åŠ¡çš„æ³›åŒ–ï¼Œæ³›åŒ–æ€§èƒ½æ˜¾è‘—æå‡ã€‚"
    ],
    "processed_at": "2025-12-02T08:46:48.825513"
  },
  {
    "id": "2512.01252v1",
    "title": "Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe",
    "abstract": "Recent efforts on Diffusion Mixture-of-Experts (MoE) models have primarily focused on developing more sophisticated routing mechanisms. However, we observe that the underlying architectural configuration space remains markedly under-explored. Inspired by the MoE design paradigms established in large language models (LLMs), we identify a set of crucial architectural factors for building effective Diffusion MoE models--including DeepSeek-style expert modules, alternative intermediate widths, varying expert counts, and enhanced attention positional encodings. Our systematic study reveals that carefully tuning these configurations is essential for unlocking the full potential of Diffusion MoE models, often yielding gains that exceed those achieved by routing innovations alone. Through extensive experiments, we present novel architectures that can be efficiently applied to both latent and pixel-space diffusion frameworks, which provide a practical and efficient training recipe that enables Diffusion MoE models to surpass strong baselines while using equal or fewer activated parameters. All code and models are publicly available at: https://github.com/yhlleo/EfficientMoE.",
    "authors": [
      "Yahui Liu",
      "Yang Yue",
      "Jingyuan Zhang",
      "Chenxi Sun",
      "Yang Zhou",
      "Wencong Zeng",
      "Ruiming Tang",
      "Guorui Zhou"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01252v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01252v1",
    "fetched_at": "2025-12-02T08:34:45.793023",
    "chinese_title": "æ‰©æ•£æ··åˆä¸“å®¶æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒï¼šå®ç”¨æ–¹æ¡ˆ",
    "chinese_summary": "è¯¥è®ºæ–‡æŒ‡å‡ºç°æœ‰æ‰©æ•£æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹å¤šèšç„¦è·¯ç”±æœºåˆ¶ï¼Œå´æœªå……åˆ†æ¢ç´¢æ¶æ„é…ç½®ç©ºé—´ï¼›å€Ÿé‰´å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„MoEè®¾è®¡èŒƒå¼ï¼Œè¯†åˆ«DeepSeekå¼ä¸“å®¶æ¨¡å—ã€ä¸­é—´å®½åº¦ã€ä¸“å®¶æ•°é‡åŠå¢å¼ºæ³¨æ„åŠ›ä½ç½®ç¼–ç ç­‰å…³é”®æ¶æ„å› ç´ ï¼Œç³»ç»Ÿç ”ç©¶è¡¨æ˜è°ƒä¼˜è¿™äº›é…ç½®å¸¸æ¯”è·¯ç”±åˆ›æ–°å¸¦æ¥æ›´å¤§å¢ç›Šï¼›æå‡ºé€‚ç”¨äºæ½œåœ¨ç©ºé—´å’Œåƒç´ ç©ºé—´æ‰©æ•£æ¡†æ¶çš„æ–°æ¶æ„ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒä¸”æ€§èƒ½è¶…è¶Šå¼ºåŸºçº¿ï¼ŒåŒæ—¶æ¿€æ´»å‚æ•°ç›¸å½“æˆ–æ›´å°‘ã€‚",
    "tags": [
      "Deep Learning",
      "LLM",
      "Transformer"
    ],
    "key_contributions": [
      "ç³»ç»Ÿè¯†åˆ«å¹¶éªŒè¯æ‰©æ•£MoEæ¨¡å‹çš„å…³é”®æ¶æ„å› ç´ ï¼Œè¡¨æ˜è°ƒä¼˜é…ç½®æ¯”è·¯ç”±åˆ›æ–°æ›´èƒ½é‡Šæ”¾æ¨¡å‹æ½œåŠ›",
      "æå‡ºé€‚ç”¨äºæ½œåœ¨/åƒç´ ç©ºé—´æ‰©æ•£çš„é«˜æ•ˆæ–°æ¶æ„ï¼Œæ€§èƒ½è¶…åŸºçº¿ä¸”æ¿€æ´»å‚æ•°ç›¸å½“æˆ–æ›´å°‘"
    ],
    "processed_at": "2025-12-02T08:47:20.705818"
  },
  {
    "id": "2512.01212v1",
    "title": "A Comparative Study of Machine Learning Algorithms for Electricity Price Forecasting with LIME-Based Interpretability",
    "abstract": "With the rapid development of electricity markets, price volatility has significantly increased, making accurate forecasting crucial for power system operations and market decisions. Traditional linear models cannot capture the complex nonlinear characteristics of electricity pricing, necessitating advanced machine learning approaches. This study compares eight machine learning models using Spanish electricity market data, integrating consumption, generation, and meteorological variables. The models evaluated include linear regression, ridge regression, decision tree, KNN, random forest, gradient boosting, SVR, and XGBoost. Results show that KNN achieves the best performance with R^2 of 0.865, MAE of 3.556, and RMSE of 5.240. To enhance interpretability, LIME analysis reveals that meteorological factors and supply-demand indicators significantly influence price fluctuations through nonlinear relationships. This work demonstrates the effectiveness of machine learning models in electricity price forecasting while improving decision transparency through interpretability analysis.",
    "authors": [
      "Xuanyi Zhao",
      "Jiawen Ding",
      "Xueting Huang",
      "Yibo Zhang"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01212v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01212v1",
    "fetched_at": "2025-12-02T08:34:45.793050",
    "chinese_title": "åŸºäºLIMEå¯è§£é‡Šæ€§çš„ç”µåŠ›ä»·æ ¼é¢„æµ‹æœºå™¨å­¦ä¹ ç®—æ³•æ¯”è¾ƒç ”ç©¶",
    "chinese_summary": "æœ¬ç ”ç©¶é’ˆå¯¹ç”µåŠ›å¸‚åœºä»·æ ¼æ³¢åŠ¨åŠ å‰§çš„é—®é¢˜ï¼Œé‡‡ç”¨è¥¿ç­ç‰™ç”µåŠ›å¸‚åœºçš„æ¶ˆè´¹ã€å‘ç”µåŠæ°”è±¡å¤šå˜é‡æ•°æ®ï¼Œå¯¹æ¯”8ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå«çº¿æ€§å›å½’ã€éšæœºæ£®æ—ã€XGBoostç­‰ï¼‰çš„ç”µä»·é¢„æµ‹æ€§èƒ½ï¼Œå‘ç°KNNè¡¨ç°æœ€ä¼˜ï¼›åŒæ—¶é€šè¿‡LIMEå¯è§£é‡Šæ€§åˆ†æï¼Œæ­ç¤ºæ°”è±¡å› ç´ å’Œä¾›éœ€æŒ‡æ ‡å¯¹ç”µä»·æ³¢åŠ¨çš„éçº¿æ€§å½±å“æœºåˆ¶ï¼Œæå‡äº†é¢„æµ‹å†³ç­–çš„é€æ˜åº¦ã€‚",
    "tags": [
      "Time Series",
      "Volatility",
      "Factor Mining"
    ],
    "key_contributions": [
      "ç³»ç»Ÿå¯¹æ¯”8ç§æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¤šå˜é‡ç”µåŠ›ç”µä»·é¢„æµ‹ä¸­çš„æ€§èƒ½ï¼ŒKNNè¡¨ç°æœ€ä¼˜",
      "å¼•å…¥LIMEå¯è§£é‡Šæ€§åˆ†æï¼Œæ˜ç¡®æ°”è±¡ä¸ä¾›éœ€æŒ‡æ ‡å¯¹ç”µä»·æ³¢åŠ¨çš„éçº¿æ€§å½±å“æœºåˆ¶"
    ],
    "processed_at": "2025-12-02T08:48:02.979571"
  },
  {
    "id": "2512.01205v1",
    "title": "Research on Milling Machine Predictive Maintenance Based on Machine Learning and SHAP Analysis in Intelligent Manufacturing Environment",
    "abstract": "In the context of intelligent manufacturing, this paper conducts a series of experimental studies on the predictive maintenance of industrial milling machine equipment based on the AI4I 2020 dataset. This paper proposes a complete predictive maintenance experimental process combining artificial intelligence technology, including six main links: data preprocessing, model training, model evaluation, model selection, SHAP analysis, and result visualization. By comparing and analyzing the performance of eight machine learning models, it is found that integrated learning methods such as XGBoost and random forest perform well in milling machine fault prediction tasks. In addition, with the help of SHAP analysis technology, the influence mechanism of different features on equipment failure is deeply revealed, among which processing temperature, torque and speed are the key factors affecting failure. This study combines artificial intelligence and manufacturing technology, provides a methodological reference for predictive maintenance practice in an intelligent manufacturing environment, and has practical significance for promoting the digital transformation of the manufacturing industry, improving production efficiency and reducing maintenance costs.",
    "authors": [
      "Wen Zhao",
      "Jiawen Ding",
      "Xueting Huang",
      "Yibo Zhang"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01205v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01205v1",
    "fetched_at": "2025-12-02T08:34:45.793075",
    "chinese_title": "æ™ºèƒ½åˆ¶é€ ç¯å¢ƒä¸‹åŸºäºæœºå™¨å­¦ä¹ ä¸SHAPåˆ†æçš„é“£åˆ€é¢„æµ‹æ€§ç»´æŠ¤ç ”ç©¶",
    "chinese_summary": "è®ºæ–‡åŸºäºAI4I2020æ•°æ®é›†ï¼Œæå‡ºæ¶µç›–æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒç­‰å…­ç¯èŠ‚çš„é“£åˆ€é¢„æµ‹æ€§ç»´æŠ¤å®éªŒæµç¨‹ï¼Œå¯¹æ¯”8ç§æœºå™¨å­¦ä¹ æ¨¡å‹å‘ç°XGBoostä¸éšæœºæ£®æ—æ€§èƒ½çªå‡ºï¼Œç»“åˆSHAPåˆ†ææ­ç¤ºåŠ å·¥æ¸©åº¦ã€æ‰­çŸ©ã€è½¬é€Ÿä¸ºæ•…éšœå…³é”®å½±å“å› ç´ ï¼Œä¸ºæ™ºèƒ½åˆ¶é€ é¢„æµ‹æ€§ç»´æŠ¤æä¾›æ–¹æ³•å‚è€ƒã€‚",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "æå‡ºåŒ…å«å…­ç¯èŠ‚çš„é“£åˆ€é¢„æµ‹æ€§ç»´æŠ¤å®Œæ•´å®éªŒæµç¨‹",
      "ç»“åˆæ¨¡å‹å¯¹æ¯”ä¸SHAPåˆ†ææ­ç¤ºæ•…éšœå…³é”®å½±å“å› ç´ "
    ],
    "processed_at": "2025-12-02T08:48:25.442811"
  },
  {
    "id": "2512.01199v1",
    "title": "Know Thyself by Knowing Others: Learning Neuron Identity from Population Context",
    "abstract": "Neurons process information in ways that depend on their cell type, connectivity, and the brain region in which they are embedded. However, inferring these factors from neural activity remains a significant challenge. To build general-purpose representations that allow for resolving information about a neuron's identity, we introduce NuCLR, a self-supervised framework that aims to learn representations of neural activity that allow for differentiating one neuron from the rest. NuCLR brings together views of the same neuron observed at different times and across different stimuli and uses a contrastive objective to pull these representations together. To capture population context without assuming any fixed neuron ordering, we build a spatiotemporal transformer that integrates activity in a permutation-equivariant manner. Across multiple electrophysiology and calcium imaging datasets, a linear decoding evaluation on top of NuCLR representations achieves a new state-of-the-art for both cell type and brain region decoding tasks, and demonstrates strong zero-shot generalization to unseen animals. We present the first systematic scaling analysis for neuron-level representation learning, showing that increasing the number of animals used during pretraining consistently improves downstream performance. The learned representations are also label-efficient, requiring only a small fraction of labeled samples to achieve competitive performance. These results highlight how large, diverse neural datasets enable models to recover information about neuron identity that generalize across animals. Code is available at https://github.com/nerdslab/nuclr.",
    "authors": [
      "Vinam Arora",
      "Divyansha Lachi",
      "Ian J. Knight",
      "Mehdi Azabou",
      "Blake Richards",
      "Cole L. Hurwitz",
      "Josh Siegle",
      "Eva L. Dyer"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01199v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01199v1",
    "fetched_at": "2025-12-02T08:34:45.793109",
    "chinese_title": "çŸ¥å·±çŸ¥å½¼ï¼šä»ç¾¤ä½“ä¸Šä¸‹æ–‡å­¦ä¹ ç¥ç»å…ƒèº«ä»½",
    "chinese_summary": "è®ºæ–‡æå‡ºè‡ªç›‘ç£æ¡†æ¶NuCLRï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ•´åˆåŒä¸€ç¥ç»å…ƒä¸åŒæ—¶é—´/åˆºæ¿€ä¸‹çš„è§†å›¾ï¼Œç»“åˆæ’åˆ—ç­‰å˜æ—¶ç©ºTransformeræ•è·ç¾¤ä½“ä¸Šä¸‹æ–‡ï¼›åœ¨ç”µç”Ÿç†å’Œé’™æˆåƒæ•°æ®é›†ä¸Šï¼Œçº¿æ€§è§£ç ç»†èƒç±»å‹ä¸è„‘åŒºè¾¾SOTAï¼Œå®ç°å¯¹æœªè§è¿‡åŠ¨ç‰©çš„é›¶æ ·æœ¬æ³›åŒ–ï¼Œä¸”å…·æœ‰æ ‡ç­¾é«˜æ•ˆæ€§ä¸ç¼©æ”¾æ€§èƒ½æå‡ç‰¹æ€§ã€‚",
    "tags": [
      "Transformer",
      "Deep Learning",
      "Time Series"
    ],
    "key_contributions": [
      "æå‡ºNuCLRè‡ªç›‘ç£æ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä¸æ’åˆ—ç­‰å˜æ—¶ç©ºTransformerå­¦ä¹ ç¥ç»å…ƒæ´»åŠ¨çš„é€šç”¨è¡¨ç¤º",
      "å®ç°ç»†èƒç±»å‹/è„‘åŒºè§£ç SOTAï¼Œæ”¯æŒé›¶æ ·æœ¬æ³›åŒ–è‡³æœªè§è¿‡çš„åŠ¨ç‰©ï¼Œä¸”æ ‡ç­¾é«˜æ•ˆã€ç¼©æ”¾æ€§èƒ½æå‡"
    ],
    "processed_at": "2025-12-02T08:48:42.672401"
  },
  {
    "id": "2512.01672v1",
    "title": "ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models",
    "abstract": "Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.",
    "authors": [
      "Zhongyuan Wu",
      "Jingyuan Wang",
      "Zexuan Cheng",
      "Yilong Zhou",
      "Weizhi Wang",
      "Juhua Pu",
      "Chao Li",
      "Changqing Ma"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01672v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01672v1",
    "fetched_at": "2025-12-02T08:34:52.456487",
    "chinese_title": "ICAD-LLMï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡å­¦ä¹ çš„ä¸€åŠ³æ°¸é€¸å¼‚å¸¸æ£€æµ‹",
    "chinese_summary": "è®ºæ–‡é’ˆå¯¹ç°æœ‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•å¤šèšç„¦å•æ¨¡æ€ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºICADèŒƒå¼ï¼ˆå°†å¼‚å¸¸å®šä¹‰ä¸ºä¸æ­£å¸¸å‚è€ƒé›†çš„å·®å¼‚æ€§ï¼‰ï¼Œå¹¶åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ„å»ºICAD-LLMç»Ÿä¸€æ¡†æ¶ï¼Œå¯å¤„ç†å¼‚è´¨æ•°æ®ä¸”æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ï¼Œæ€§èƒ½ä¸ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸å½“ä¸”é™ä½éƒ¨ç½²æˆæœ¬ã€‚",
    "tags": [
      "LLM",
      "Anomaly",
      "Time Series",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºICADèŒƒå¼ï¼Œå°†å¼‚å¸¸å®šä¹‰ä¸ºä¸æ­£å¸¸å‚è€ƒé›†çš„å·®å¼‚æ€§",
      "æ„å»ºICAD-LLMç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨LLMä¸Šä¸‹æ–‡å­¦ä¹ å¤„ç†å¼‚è´¨æ•°æ®ï¼Œå…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›ä¸”æ€§èƒ½åª²ç¾ä»»åŠ¡ç‰¹å®šæ–¹æ³•"
    ],
    "processed_at": "2025-12-02T08:48:55.061605"
  },
  {
    "id": "2512.01534v1",
    "title": "Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis",
    "abstract": "Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.",
    "authors": [
      "Alexander Frotscher",
      "Christian F. Baumgartner",
      "Thomas Wolfers"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01534v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01534v1",
    "fetched_at": "2025-12-02T08:34:52.456522",
    "chinese_title": "è„‘æˆåƒä¸­çš„æ·±åº¦æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼šå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸åå·®åˆ†æ",
    "chinese_summary": "æœ¬æ–‡æ„å»ºäº†è„‘ç£å…±æŒ¯æˆåƒæ·±åº¦æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹çš„å¤§è§„æ¨¡å¤šä¸­å¿ƒåŸºå‡†ï¼ŒåŒ…å«å¤šæ‰«æä»ªå¥åº·æ•°æ®åŠå¤šæ ·ä¸´åºŠæµ‹è¯•é›†ï¼Œç³»ç»Ÿè¯„ä¼°ç®—æ³•æ€§èƒ½ä¸ç¨³å¥æ€§ï¼Œå‘ç°é‡å»ºç±»ï¼ˆæ‰©æ•£å¯å‘å¼ï¼‰åˆ†å‰²æ€§èƒ½è¾ƒå¼ºä½†å­˜åœ¨æ‰«æå™¨ã€ç—…ç¶å¤§å°ç­‰åå·®ï¼Œç‰¹å¾ç±»æŠ—åˆ†å¸ƒåç§»æ›´ä¼˜ã€‚",
    "tags": [
      "Anomaly",
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "æ„å»ºå«å¤šä¸­å¿ƒã€å¤šæ‰«æä»ªåŠå¹´é¾„è·¨åº¦å¥åº·æ•°æ®çš„å¤§è§„æ¨¡è„‘æˆåƒæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹åŸºå‡†ï¼Œè¦†ç›–å¤šæ ·ä¸´åºŠæµ‹è¯•é›†",
      "ç³»ç»Ÿè¯„ä¼°ç®—æ³•æ€§èƒ½ä¸ç¨³å¥æ€§ï¼Œæ­ç¤ºä¸åŒæ–¹æ³•ä¼˜åŠ£åŠæ‰«æå™¨ã€ç—…ç¶ç±»å‹/å¤§å°ã€äººå£ç»Ÿè®¡å­¦ç›¸å…³åå·®"
    ],
    "processed_at": "2025-12-02T08:49:07.202898"
  },
  {
    "id": "2512.01498v1",
    "title": "Winning Solutions for the Rayan AI Contest: Compositional Retrieval, Zero-Shot Anomaly Detection, and Backdoor Detection",
    "abstract": "This report presents solutions to three machine learning challenges: compositional image retrieval, zero-shot anomaly detection, and backdoored model detection. In compositional image retrieval, we developed a system that processes visual and textual inputs to retrieve relevant images, achieving 95.38\\% accuracy and ranking first with a clear margin over the second team. For zero-shot anomaly detection, we designed a model that identifies and localizes anomalies in images without prior exposure to abnormal examples, securing 1st place with 73.14\\% accuracy. In the backdoored model detection task, we proposed a method to detect hidden backdoor triggers in neural networks, reaching an accuracy of 78\\%, which placed our approach in second place. These results demonstrate the effectiveness of our methods in addressing key challenges related to retrieval, anomaly detection, and model security, with implications for real-world applications in industries such as healthcare, manufacturing, and cybersecurity. Code for all solutions is available online.",
    "authors": [
      "Ali Nafisi",
      "Sina Asghari",
      "Mohammad Saeed Arvenaghi",
      "Hossein Shakibania"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01498v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01498v1",
    "fetched_at": "2025-12-02T08:34:52.456550",
    "chinese_title": "Rayan AIç«èµ›è·å¥–æ–¹æ¡ˆï¼šç»„åˆå¼æ£€ç´¢ã€é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸åé—¨æ£€æµ‹",
    "chinese_summary": "è¯¥è®ºæ–‡æå‡ºRayan AIç«èµ›çš„ä¸‰ä¸ªè·å¥–æ–¹æ¡ˆï¼Œåˆ†åˆ«é’ˆå¯¹ç»„åˆå¼å›¾åƒæ£€ç´¢ï¼ˆè§†è§‰+æ–‡æœ¬è¾“å…¥ï¼Œ95.38%å‡†ç¡®ç‡è·ç¬¬ä¸€ï¼‰ã€é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆæ— å¼‚å¸¸æ ·æœ¬ä¸‹è¯†åˆ«å®šä½å›¾åƒå¼‚å¸¸ï¼Œ73.14%å‡†ç¡®ç‡è·ç¬¬ä¸€ï¼‰å’Œåé—¨æ¨¡å‹æ£€æµ‹ï¼ˆæ£€æµ‹ç¥ç»ç½‘ç»œéšè—åé—¨è§¦å‘ï¼Œ78%å‡†ç¡®ç‡è·ç¬¬äºŒï¼‰ï¼›æ–¹æ³•åœ¨æ£€ç´¢ã€å¼‚å¸¸æ£€æµ‹åŠæ¨¡å‹å®‰å…¨é¢†åŸŸæœ‰æ•ˆï¼Œå¯åº”ç”¨äºåŒ»ç–—ã€åˆ¶é€ ç­‰è¡Œä¸šï¼Œä»£ç å¼€æºã€‚",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºä¸‰ä¸ªç«èµ›è·å¥–æ–¹æ¡ˆï¼Œåœ¨ç»„åˆå¼å›¾åƒæ£€ç´¢ã€é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ã€åé—¨æ¨¡å‹æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚æˆç»©",
      "æ–¹æ³•åœ¨æ— å¼‚å¸¸æ ·æœ¬çš„å¼‚å¸¸æ£€æµ‹åŠæ¨¡å‹å®‰å…¨é¢†åŸŸå…·æœ‰å®é™…åº”ç”¨ä»·å€¼"
    ],
    "processed_at": "2025-12-02T08:49:37.535279"
  },
  {
    "id": "2512.01365v1",
    "title": "Modeling Wavelet Transformed Quantum Support Vector for Network Intrusion Detection",
    "abstract": "Network traffic anomaly detection is a critical cy- bersecurity challenge requiring robust solutions for complex Internet of Things (IoT) environments. We present a novel hybrid quantum-classical framework integrating an enhanced Quantum Support Vector Machine (QSVM) with the Quantum Haar Wavelet Packet Transform (QWPT) for superior anomaly classification under realistic noisy intermediate-scale Quantum conditions. Our methodology employs amplitude-encoded quan- tum state preparation, multi-level QWPT feature extraction, and behavioral analysis via Shannon Entropy profiling and Chi-square testing. Features are classified using QSVM with fidelity-based quantum kernels optimized through hybrid train- ing with simultaneous perturbation stochastic approximation (SPSA) optimizer. Evaluation under noiseless and depolarizing noise conditions demonstrates exceptional performance: 96.67% accuracy on BoT-IoT and 89.67% on IoT-23 datasets, surpassing quantum autoencoder approaches by over 7 percentage points.",
    "authors": [
      "Swati Kumari",
      "Shiva Raj Pokhrel",
      "Swathi Chandrasekhar",
      "Navneet Singh",
      "Hridoy Sankar Dutta",
      "Adnan Anwar",
      "Sutharshan Rajasegarar",
      "Robin Doss"
    ],
    "published": "2025-12-01",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01365v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01365v1",
    "fetched_at": "2025-12-02T08:34:52.456635",
    "chinese_title": "å°æ³¢å˜æ¢é‡å­æ”¯æŒå‘é‡æœºå»ºæ¨¡ç”¨äºç½‘ç»œå…¥ä¾µæ£€æµ‹",
    "chinese_summary": "è®ºæ–‡æå‡ºæ··åˆé‡å­-ç»å…¸æ¡†æ¶ï¼Œæ•´åˆå¢å¼ºå‹é‡å­æ”¯æŒå‘é‡æœºï¼ˆQSVMï¼‰ä¸é‡å­å“ˆå°”å°æ³¢åŒ…å˜æ¢ï¼ˆQWPTï¼‰ï¼Œé‡‡ç”¨æŒ¯å¹…ç¼–ç é‡å­æ€åˆ¶å¤‡ã€å¤šçº§QWPTç‰¹å¾æå–åŠåŸºäºä¿çœŸåº¦é‡å­æ ¸çš„QSVMç»“åˆSPSAä¼˜åŒ–å™¨è®­ç»ƒï¼›åœ¨å«å™ªä¸­ç­‰è§„æ¨¡é‡å­æ¡ä»¶ä¸‹ï¼ŒBoT-IoTå’ŒIoT-23æ•°æ®é›†å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡åˆ†åˆ«è¾¾96.67%å’Œ89.67%ï¼Œä¼˜äºé‡å­è‡ªåŠ¨ç¼–ç å™¨è¶…7ä¸ªç™¾åˆ†ç‚¹ã€‚",
    "tags": [
      "Anomaly"
    ],
    "key_contributions": [
      "æå‡ºæ•´åˆå¢å¼ºå‹QSVMä¸é‡å­å“ˆå°”å°æ³¢åŒ…å˜æ¢çš„æ··åˆé‡å­-ç»å…¸æ¡†æ¶ï¼Œé€‚é…ç‰©è”ç½‘ç¯å¢ƒç½‘ç»œæµé‡å¼‚å¸¸æ£€æµ‹",
      "åœ¨å«å™ªä¸­ç­‰è§„æ¨¡é‡å­æ¡ä»¶ä¸‹ï¼Œæ‰€ææ–¹æ³•åœ¨ä¸¤ç±»ç‰©è”ç½‘å…¥ä¾µæ£€æµ‹æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºé‡å­è‡ªåŠ¨ç¼–ç å™¨"
    ],
    "processed_at": "2025-12-02T08:50:07.809223"
  },
  {
    "id": "2512.00844v1",
    "title": "FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity",
    "abstract": "Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.",
    "authors": [
      "Giles Winchester",
      "George Parisis",
      "Luc Berthouze"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.SE",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00844v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00844v1",
    "fetched_at": "2025-12-02T08:34:52.456660",
    "chinese_title": "FC-ADLï¼šåŸºäºåŠŸèƒ½è¿æ¥çš„é«˜æ•ˆå¾®æœåŠ¡å¼‚å¸¸æ£€æµ‹ä¸å®šä½",
    "chinese_summary": "é’ˆå¯¹å¾®æœåŠ¡æ¶æ„ä¸­å¼‚å¸¸æ£€æµ‹ä¸å®šä½çš„æŒ‘æˆ˜ï¼ˆä¼ ç»Ÿæ–¹æ³•æœªè€ƒè™‘æ—¶å˜ä¾èµ–æˆ–å› æœæ¨ç†è®¡ç®—å¼€é”€å¤§ï¼‰ï¼Œè®ºæ–‡æå‡ºFC-ADLæ–¹æ³•ï¼ŒåŸºäºç¥ç»ç§‘å­¦çš„åŠŸèƒ½è¿æ¥é«˜æ•ˆåˆ»ç”»å¾®æœåŠ¡æŒ‡æ ‡é—´çš„æ—¶å˜ä¾èµ–å˜åŒ–ï¼Œå®ç°ç«¯åˆ°ç«¯å¯æ‰©å±•çš„å¼‚å¸¸æ£€æµ‹ä¸å®šä½ï¼Œæ— éœ€é«˜å¼€é”€çš„å› æœæˆ–å¤šå˜é‡æ–¹æ³•ï¼Œåœ¨å¤šç§æ•…éšœåœºæ™¯ä¸‹æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ä¸”å¯åº”ç”¨äºå¤§è§„æ¨¡éƒ¨ç½²ã€‚",
    "tags": [
      "Anomaly",
      "Time Series",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "æå‡ºFC-ADLç«¯åˆ°ç«¯æ–¹æ³•ï¼ŒåŸºäºåŠŸèƒ½è¿æ¥é«˜æ•ˆåˆ»ç”»å¾®æœåŠ¡æŒ‡æ ‡é—´çš„æ—¶å˜ä¾èµ–å˜åŒ–ï¼Œè§£å†³ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹æœªè€ƒè™‘æ—¶å˜ä¾èµ–ã€æ•…éšœå®šä½è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜",
      "å®ç°å¯æ‰©å±•çš„å¼‚å¸¸æ£€æµ‹ä¸å®šä½ï¼Œåœ¨å¤šç§æ•…éšœåœºæ™¯ä¸‹æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”èƒ½åº”ç”¨äºå¤§è§„æ¨¡å¾®æœåŠ¡éƒ¨ç½²ï¼ˆå¦‚é˜¿é‡Œåœºæ™¯ï¼‰"
    ],
    "processed_at": "2025-12-02T08:50:41.859716"
  },
  {
    "id": "2512.00321v1",
    "title": "Introducing AI-Driven IoT Energy Management Framework",
    "abstract": "Power consumption has become a critical aspect of modern life due to the consistent reliance on technological advancements. Reducing power consumption or following power usage predictions can lead to lower monthly costs and improved electrical reliability. The proposal of a holistic framework to establish a foundation for IoT systems with a focus on contextual decision making, proactive adaptation, and scalable structure. A structured process for IoT systems with accuracy and interconnected development would support reducing power consumption and support grid stability. This study presents the feasibility of this proposal through the application of each aspect of the framework. This system would have long term forecasting, short term forecasting, anomaly detection, and consideration of qualitative data with any energy management decisions taken. Performance was evaluated on Power Consumption Time Series data to display the direct application of the framework.",
    "authors": [
      "Shivani Mruthyunjaya",
      "Anandi Dutta",
      "Kazi Sifatul Islam"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00321v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00321v1",
    "fetched_at": "2025-12-02T08:34:52.456684",
    "chinese_title": "å¼•å…¥AIé©±åŠ¨çš„ç‰©è”ç½‘èƒ½æºç®¡ç†æ¡†æ¶",
    "chinese_summary": "è®ºæ–‡æå‡ºä¸€ç§AIé©±åŠ¨çš„ç‰©è”ç½‘èƒ½æºç®¡ç†æ¡†æ¶ï¼Œé›†æˆé•¿æœŸ/çŸ­æœŸèƒ½è€—é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹åŠå®šæ€§æ•°æ®è€ƒé‡ï¼Œæ—¨åœ¨é™ä½èƒ½è€—å¹¶æå‡ç”µç½‘ç¨³å®šæ€§ï¼›é€šè¿‡ç”µåŠ›æ¶ˆè´¹æ—¶é—´åºåˆ—æ•°æ®éªŒè¯äº†è¯¥æ¡†æ¶çš„å¯è¡Œæ€§ã€‚",
    "tags": [
      "Time Series",
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºAIé©±åŠ¨çš„ç‰©è”ç½‘èƒ½æºç®¡ç†æ¡†æ¶ï¼Œæ•´åˆå¤šæ¨¡å—æ”¯æŒèƒ½è€—ä¼˜åŒ–ä¸ç”µç½‘ç¨³å®š",
      "åŸºäºç”µåŠ›æ¶ˆè´¹æ—¶é—´åºåˆ—æ•°æ®éªŒè¯æ¡†æ¶å¯è¡Œæ€§"
    ],
    "processed_at": "2025-12-02T08:51:03.253470"
  },
  {
    "id": "2512.00251v1",
    "title": "SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks",
    "abstract": "The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.",
    "authors": [
      "Henry Onyeka",
      "Emmanuel Samson",
      "Liang Hong",
      "Tariqul Islam",
      "Imtiaz Ahmed",
      "Kamrul Hasan"
    ],
    "published": "2025-11-28",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00251v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00251v1",
    "fetched_at": "2025-12-02T08:34:52.456714",
    "chinese_title": "SD-CGANï¼šç”¨äºç‰©è”ç½‘ç½‘ç»œDDoSå¼‚å¸¸æ£€æµ‹çš„æ¡ä»¶Sinkhornæ•£åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ",
    "chinese_summary": "é’ˆå¯¹ç‰©è”ç½‘è¾¹ç¼˜ç½‘ç»œä¸­åŠ¨æ€ä¸å¹³è¡¡æµé‡ä¸‹çš„DDoSåŠé›¶æ—¥æ”»å‡»æ£€æµ‹éš¾é¢˜ï¼Œæå‡ºSD-CGANæ¡†æ¶ï¼ˆæ¡ä»¶GANå¢å¼ºSinkhornæ•£åº¦ï¼‰ï¼Œç»“åˆCTGANæ•°æ®å¢å¼ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼Œç”¨Sinkhornæ•£åº¦æå‡è®­ç»ƒç¨³å®šæ€§ä¸å‡å°‘æ¨¡å¼å´©æºƒï¼›åœ¨CICDDoS2019æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ä¸”è®¡ç®—é«˜æ•ˆé€‚é…è¾¹ç¼˜éƒ¨ç½²ã€‚",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºSD-CGANï¼ˆæ¡ä»¶Sinkhornæ•£åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰ï¼Œé€‚é…ç‰©è”ç½‘è¾¹ç¼˜ç½‘ç»œåŠ¨æ€ä¸å¹³è¡¡æµé‡ä¸‹çš„DDoSå¼‚å¸¸æ£€æµ‹åœºæ™¯ï¼›",
      "èåˆCTGANæ•°æ®å¢å¼ºä¸Sinkhornæ•£åº¦å‡ ä½•æ„ŸçŸ¥æŸå¤±ï¼Œæå‡æ£€æµ‹æ€§èƒ½å¹¶ä¿è¯è®¡ç®—æ•ˆç‡ï¼Œä¼˜äºåŸºçº¿æ·±åº¦å­¦ä¹ åŠGANæ–¹æ³•ã€‚"
    ],
    "processed_at": "2025-12-02T08:51:28.620608"
  },
  {
    "id": "2512.00229v1",
    "title": "TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection",
    "abstract": "Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \\textit{uncertainty} and detect \\textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \\textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \\textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \\textit{AUROC}, \\textit{AUPR}, and \\textit{FPR@95\\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \\textbf{\\(\\!\\approx\\!\\) 0 FPR@95\\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.",
    "authors": [
      "Pirzada Suhail",
      "Rehna Afroz",
      "Amit Sethi"
    ],
    "published": "2025-11-28",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00229v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00229v1",
    "fetched_at": "2025-12-02T08:34:52.456739",
    "chinese_title": "TIEï¼šç”¨äºè§†è§‰å¯è§£é‡Šå’Œä¸ç¡®å®šæ€§å¼•å¯¼çš„åˆ†å¸ƒå¤–æ£€æµ‹çš„è®­ç»ƒ-åè½¬-æ’é™¤æ¡†æ¶",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œéš¾ä»¥è¯†åˆ«åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ ·æœ¬çš„é—®é¢˜ï¼Œæå‡ºTIEæ¡†æ¶ï¼Œå°†æ ‡å‡†nç±»åˆ†ç±»å™¨æ‰©å±•ä¸ºå«åƒåœ¾ç±»çš„n+1ç±»æ¨¡å‹ï¼Œé€šè¿‡è®­ç»ƒ-åè½¬-æ’é™¤çš„é—­ç¯è¿‡ç¨‹è¿­ä»£ä¼˜åŒ–ï¼›æ¨ç†æ—¶æ— éœ€å¤–éƒ¨OODæ•°æ®é›†å³å¯æ£€æµ‹OODï¼Œå¹¶æä¾›è§†è§‰å¯è§£é‡Šæ€§ä¸ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚",
    "tags": [
      "Anomaly",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºè®­ç»ƒ-åè½¬-æ’é™¤ï¼ˆTIEï¼‰æ¡†æ¶ï¼Œå°†æ ‡å‡†nç±»åˆ†ç±»å™¨æ‰©å±•ä¸ºå«åƒåœ¾ç±»çš„n+1ç±»æ¨¡å‹ï¼Œé€šè¿‡é—­ç¯è¿­ä»£ä¼˜åŒ–å®ç°æ— éœ€å¤–éƒ¨OODæ•°æ®é›†çš„åˆ†å¸ƒå¤–æ£€æµ‹",
      "åŒæ—¶å®ç°è§†è§‰å¯è§£é‡Šæ€§ï¼ˆåè½¬æ ·æœ¬å½¢æˆè§†è§‰è¿è´¯ç±»åŸå‹ï¼‰ä¸ä¸ç¡®å®šæ€§å¼•å¯¼çš„OODæ£€æµ‹ï¼Œæå‡æ¨¡å‹å¯¹å¼‚å¸¸è¾“å…¥çš„è¯†åˆ«èƒ½åŠ›"
    ],
    "processed_at": "2025-12-02T08:51:58.815848"
  },
  {
    "id": "2512.00841v1",
    "title": "Prediction-space knowledge markets for communication-efficient federated learning on multimedia tasks",
    "abstract": "Federated learning (FL) enables collaborative training over distributed multimedia data but suffers acutely from statistical heterogeneity and communication constraints, especially when clients deploy large models. Classic parameter-averaging methods such as FedAvg transmit full model weights and can diverge under nonindependent and identically distributed (non-IID) data. We propose KTA v2, a prediction-space knowledge trading market for FL. Each round, clients locally train on their private data, then share only logits on a small public reference set. The server constructs a client-client similarity graph in prediction space, combines it with reference-set accuracy to form per-client teacher ensembles, and sends back personalized soft targets for a second-stage distillation update. This two-stage procedure can be interpreted as approximate block-coordinate descent on a unified objective with prediction-space regularization. Experiments on FEMNIST, CIFAR-10 and AG News show that, under comparable or much lower communication budgets, KTA v2 consistently outperforms a local-only baseline and strong parameter-based methods (FedAvg, FedProx), and substantially improves over a FedMD-style global teacher. On CIFAR-10 with ResNet-18, KTA v2 reaches 57.7% test accuracy using approximately 1/1100 of FedAvg's communication, while on AG News it attains 89.3% accuracy with approximately 1/300 of FedAvg's traffic.",
    "authors": [
      "Wenzhang Du"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00841v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00841v1",
    "fetched_at": "2025-12-02T08:35:02.449084",
    "chinese_title": "é¢å‘å¤šåª’ä½“ä»»åŠ¡çš„é€šä¿¡é«˜æ•ˆè”é‚¦å­¦ä¹ çš„é¢„æµ‹ç©ºé—´çŸ¥è¯†å¸‚åœº",
    "chinese_summary": "é’ˆå¯¹è”é‚¦å­¦ä¹ åœ¨ç»Ÿè®¡å¼‚è´¨æ€§å’Œé€šä¿¡çº¦æŸä¸‹çš„é—®é¢˜ï¼Œæå‡ºKTA v2æ–¹æ³•ï¼šå®¢æˆ·ç«¯ä»…å…±äº«å°å…¬å…±å‚è€ƒé›†çš„logitsï¼ŒæœåŠ¡å™¨æ„å»ºé¢„æµ‹ç©ºé—´ç›¸ä¼¼åº¦å›¾å¹¶ç»“åˆå‡†ç¡®ç‡å½¢æˆæ•™å¸ˆé›†æˆï¼Œè¿”å›ä¸ªæ€§åŒ–è½¯ç›®æ ‡è¿›è¡Œè’¸é¦æ›´æ–°ï¼›å®éªŒåœ¨FEMNISTã€CIFAR-10ç­‰æ•°æ®é›†ä¸Šè¯æ˜ï¼Œå…¶åœ¨ä½é€šä¿¡é¢„ç®—ä¸‹ä¼˜äºFedAvgç­‰ç°æœ‰æ–¹æ³•ã€‚",
    "tags": [
      "Deep Learning",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "æå‡ºKTA v2é¢„æµ‹ç©ºé—´çŸ¥è¯†äº¤æ˜“æ–¹æ³•ï¼Œé€šè¿‡å…±äº«å…¬å…±å‚è€ƒé›†logitså®ç°é€šä¿¡é«˜æ•ˆçš„è”é‚¦å­¦ä¹ ï¼Œç¼“è§£ç»Ÿè®¡å¼‚è´¨æ€§ä¸é€šä¿¡çº¦æŸ",
      "å®éªŒéªŒè¯åœ¨å¤šæ•°æ®é›†ä¸Šï¼ŒKTA v2åœ¨ä½é€šä¿¡é¢„ç®—ä¸‹æ˜¾è‘—ä¼˜äºFedAvgç­‰ç°æœ‰å‚æ•°å¹³å‡æ–¹æ³•åŠFedMDç±»æ–¹æ³•"
    ],
    "processed_at": "2025-12-02T08:52:28.433280"
  },
  {
    "id": "2512.01362v1",
    "title": "Directed evolution algorithm drives neural prediction",
    "abstract": "Neural prediction offers a promising approach to forecasting the individual variability of neurocognitive functions and disorders and providing prognostic indicators for personalized invention. However, it is challenging to translate neural predictive models into medical artificial intelligent applications due to the limitations of domain shift and label scarcity. Here, we propose the directed evolution model (DEM), a novel computational model that mimics the trial-and-error processes of biological directed evolution to approximate optimal solutions for predictive modeling tasks. We demonstrated that the directed evolution algorithm is an effective strategy for uncertainty exploration, enhancing generalization in reinforcement learning. Furthermore, by incorporating replay buffer and continual backpropagate methods into DEM, we provide evidence of achieving better trade-off between exploitation and exploration in continuous learning settings. We conducted experiments on four different datasets for children with cochlear implants whose spoken language developmental outcomes vary considerably on the individual-child level. Preoperative neural MRI data has shown to accurately predict the post-operative outcome of these children within but not across datasets. Our results show that DEM can efficiently improve the performance of cross-domain pre-implantation neural predictions while addressing the challenge of label scarcity in target domain.",
    "authors": [
      "Yanlin Wang",
      "Nancy M Young",
      "Patrick C M Wong"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01362v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01362v1",
    "fetched_at": "2025-12-02T08:35:12.568758",
    "chinese_title": "å®šå‘è¿›åŒ–ç®—æ³•é©±åŠ¨ç¥ç»é¢„æµ‹",
    "chinese_summary": "è®ºæ–‡æå‡ºæ¨¡æ‹Ÿç”Ÿç‰©å®šå‘è¿›åŒ–è¯•é”™è¿‡ç¨‹çš„å®šå‘è¿›åŒ–æ¨¡å‹ï¼ˆDEMï¼‰ï¼Œç»“åˆå›æ”¾ç¼“å†²åŒºä¸æŒç»­åå‘ä¼ æ’­æ–¹æ³•ä¼˜åŒ–è¿ç»­å­¦ä¹ ä¸­æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼›åœ¨äººå·¥è€³èœ—æ¤å…¥å„¿ç«¥çš„å››ä¸ªç¥ç»é¢„æµ‹æ•°æ®é›†å®éªŒä¸­ï¼ŒDEMæœ‰æ•ˆæå‡è·¨åŸŸæœ¯å‰ç¥ç»é¢„æµ‹æ€§èƒ½å¹¶åº”å¯¹ç›®æ ‡åŸŸæ ‡ç­¾ç¨€ç¼ºæŒ‘æˆ˜ã€‚",
    "tags": [
      "Deep Learning",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "æå‡ºå®šå‘è¿›åŒ–æ¨¡å‹ï¼ˆDEMï¼‰ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©å®šå‘è¿›åŒ–è¯•é”™è¿‡ç¨‹ï¼Œè§£å†³ç¥ç»é¢„æµ‹ä¸­çš„é¢†åŸŸåç§»ä¸æ ‡ç­¾ç¨€ç¼ºé—®é¢˜",
      "ç»“åˆå›æ”¾ç¼“å†²åŒºå’ŒæŒç»­åå‘ä¼ æ’­æ–¹æ³•ï¼Œä¼˜åŒ–è¿ç»­å­¦ä¹ ä¸­æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼Œæå‡è·¨åŸŸç¥ç»é¢„æµ‹æ€§èƒ½"
    ],
    "processed_at": "2025-12-02T08:52:51.053757"
  },
  {
    "id": "2512.01316v1",
    "title": "Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding",
    "abstract": "Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\\leftrightarrow$De translation tasks.",
    "authors": [
      "Koki Natsumi",
      "Hiroyuki Deguchi",
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01316v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01316v1",
    "fetched_at": "2025-12-02T08:35:12.568797",
    "chinese_title": "çº¦æŸä¸€è‡´æ€§çš„æ¦‚ç‡æœ€å°è´å¶æ–¯é£é™©è§£ç ",
    "chinese_summary": "æœ€å°è´å¶æ–¯é£é™©ï¼ˆMBRï¼‰è§£ç å› éœ€è®¡ç®—å€™é€‰å¯¹æ‰€æœ‰åˆ†æ•°å¯¼è‡´äºŒæ¬¡æ—¶é—´å¤æ‚åº¦ï¼Œæ¦‚ç‡MBRï¼ˆPMBRï¼‰è™½å‡å°‘è®¡ç®—ä½†è´¨é‡ä¸‹é™ï¼›æœ¬æ–‡æå‡ºAC-PMBRæ–¹æ³•ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦æ¨¡å‹æŒ‡å¯¼åˆ†æ•°çŸ©é˜µè¡¥å…¨ï¼Œåœ¨WMT'23è‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­ä½¿çŸ©é˜µè¡¥å…¨è¿‘ä¼¼è¯¯å·®æå‡è‡³å¤š3å€ï¼Œä¸”è®¡ç®—æˆæœ¬ç›¸å½“çš„æƒ…å†µä¸‹ç¿»è¯‘è´¨é‡ä¼˜äºPMBRã€‚",
    "tags": [
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºçº¦æŸä¸€è‡´æ€§çš„æ¦‚ç‡æœ€å°è´å¶æ–¯é£é™©ï¼ˆAC-PMBRï¼‰è§£ç æ–¹æ³•ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æ¨¡å‹æŒ‡å¯¼åˆ†æ•°çŸ©é˜µè¡¥å…¨ä¼˜åŒ–è´¨é‡-æˆæœ¬æƒè¡¡",
      "åœ¨WMT'23è‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­ï¼ŒAC-PMBRä½¿çŸ©é˜µè¡¥å…¨è¿‘ä¼¼è¯¯å·®æå‡è‡³å¤š3å€ï¼Œä¸”è®¡ç®—æˆæœ¬ç›¸å½“çš„æƒ…å†µä¸‹ç¿»è¯‘è´¨é‡ä¼˜äºPMBR"
    ],
    "processed_at": "2025-12-02T08:53:12.824750"
  },
  {
    "id": "2512.01056v1",
    "title": "The Silence that Speaks: Neural Estimation via Communication Gaps",
    "abstract": "Accurate remote state estimation is a fundamental component of many autonomous and networked dynamical systems, where multiple decision-making agents interact and communicate over shared, bandwidth-constrained channels. These communication constraints introduce an additional layer of complexity, namely, the decision of when to communicate. This results in a fundamental trade-off between estimation accuracy and communication resource usage. Traditional extensions of classical estimation algorithms (e.g., the Kalman filter) treat the absence of communication as 'missing' information. However, silence itself can carry implicit information about the system's state, which, if properly interpreted, can enhance the estimation quality even in the absence of explicit communication. Leveraging this implicit structure, however, poses significant analytical challenges, even in relatively simple systems. In this paper, we propose CALM (Communication-Aware Learning and Monitoring), a novel learning-based framework that jointly addresses the dual challenges of communication scheduling and estimator design. Our approach entails learning not only when to communicate but also how to infer useful information from periods of communication silence. We perform comparative case studies on multiple benchmarks to demonstrate that CALM is able to decode the implicit coordination between the estimator and the scheduler to extract information from the instances of 'silence' and enhance the estimation accuracy.",
    "authors": [
      "Shubham Aggarwal",
      "Dipankar Maity",
      "Tamer BaÅŸar"
    ],
    "published": "2025-11-30",
    "categories": [
      "eess.SY",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01056v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01056v1",
    "fetched_at": "2025-12-02T08:35:12.568823",
    "chinese_title": "æ²‰é»˜çš„ä¿¡æ¯ï¼šåŸºäºé€šä¿¡é—´éš™çš„ç¥ç»ä¼°è®¡",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹ç½‘ç»œåŠ¨æ€ç³»ç»Ÿä¸­é€šä¿¡çº¦æŸä¸‹çš„è¿œç¨‹çŠ¶æ€ä¼°è®¡é—®é¢˜ï¼ŒæŒ‡å‡ºé€šä¿¡æ²‰é»˜å¯æºå¸¦ç³»ç»ŸçŠ¶æ€çš„éšå«ä¿¡æ¯ï¼ˆä¼ ç»Ÿæ–¹æ³•æœªå……åˆ†åˆ©ç”¨ï¼‰ï¼›æå‡ºCALMï¼ˆé€šä¿¡æ„ŸçŸ¥å­¦ä¹ ä¸ç›‘æµ‹ï¼‰æ¡†æ¶ï¼Œè”åˆä¼˜åŒ–é€šä¿¡è°ƒåº¦ä¸ä¼°è®¡å™¨è®¾è®¡ï¼Œå­¦ä¹ ä»æ²‰é»˜ä¸­æå–æœ‰ç”¨ä¿¡æ¯ä»¥æå‡ä¼°è®¡è´¨é‡ï¼Œå®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚",
    "tags": [
      "Deep Learning",
      "Reinforcement Learning",
      "Time Series"
    ],
    "key_contributions": [
      "æ­ç¤ºé€šä¿¡æ²‰é»˜æºå¸¦ç³»ç»ŸçŠ¶æ€éšå«ä¿¡æ¯ï¼Œçªç ´ä¼ ç»Ÿå°†æ²‰é»˜è§†ä¸ºç¼ºå¤±ä¿¡æ¯çš„è®¤çŸ¥å±€é™",
      "æå‡ºCALMæ¡†æ¶ï¼Œè”åˆä¼˜åŒ–é€šä¿¡è°ƒåº¦ä¸ä¼°è®¡å™¨è®¾è®¡ï¼Œå®ç°ä»æ²‰é»˜ä¸­æå–ä¿¡æ¯æå‡ä¼°è®¡ç²¾åº¦"
    ],
    "processed_at": "2025-12-02T08:53:43.003367"
  },
  {
    "id": "2512.00545v1",
    "title": "DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning",
    "abstract": "The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.",
    "authors": [
      "Akrati Saxena",
      "Harshith Kumar Yadav",
      "Bart Rutten",
      "Shashi Shekhar Jha"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00545v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00545v1",
    "fetched_at": "2025-12-02T08:35:12.568849",
    "chinese_title": "DQ4FairIMï¼šåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å…¬å¹³æ„ŸçŸ¥å½±å“åŠ›æœ€å¤§åŒ–",
    "chinese_summary": "è¯¥è®ºæ–‡é’ˆå¯¹å½±å“åŠ›æœ€å¤§åŒ–ï¼ˆIMï¼‰é—®é¢˜ä¸­å¿½ç•¥å…¬å¹³æ€§å¯¼è‡´åå‘å¤šæ•°ç¾¤ä½“çš„ç¼ºé™·ï¼Œæå‡ºå…¬å¹³æ„ŸçŸ¥æ–¹æ³•DQ4FairIMï¼›å…¶å°†IMå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œç»“åˆç»“æ„å‘é‡ç½‘ç»œåµŒå…¥ä¸æ·±åº¦Qå­¦ä¹ ï¼Œé‡‡ç”¨maximinå…¬å¹³ç›®æ ‡ä¼˜å…ˆæå‡æœ€å—å½±å“å°çš„ç¾¤ä½“ï¼Œå®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
    "tags": [
      "Reinforcement Learning",
      "Graph Neural Network",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºå…¬å¹³æ„ŸçŸ¥çš„å½±å“åŠ›æœ€å¤§åŒ–æ–¹æ³•DQ4FairIMï¼Œé‡‡ç”¨maximinå…¬å¹³ç›®æ ‡ç¡®ä¿å„ç¾¤ä½“å…¬å¹³å½±å“",
      "å°†IMå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œç»“åˆç»“æ„å‘é‡ç½‘ç»œåµŒå…¥ä¸æ·±åº¦Qå­¦ä¹ è§£å†³è¯¥é—®é¢˜ï¼Œå®éªŒä¼˜äºç°æœ‰åŸºçº¿"
    ],
    "processed_at": "2025-12-02T08:54:08.880669"
  },
  {
    "id": "2512.00357v1",
    "title": "Learning Causal States Under Partial Observability and Perturbation",
    "abstract": "A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \\textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.",
    "authors": [
      "Na Li",
      "Hangguan Shan",
      "Wei Ni",
      "Wenjie Zhang",
      "Xinyu Li",
      "Yamin Wang"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00357v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00357v1",
    "fetched_at": "2025-12-02T08:35:12.568879",
    "chinese_title": "éƒ¨åˆ†å¯è§‚æµ‹ä¸æ‰°åŠ¨ä¸‹çš„å› æœçŠ¶æ€å­¦ä¹ ",
    "chinese_summary": "é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä¸­éƒ¨åˆ†å¯è§‚æµ‹ä¸”å—æ‰°åŠ¨çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPÂ²OMDPï¼‰çš„å†³ç­–éš¾é¢˜ï¼Œè®ºæ–‡æå‡ºCaDiffæ¡†æ¶ï¼Œç»“åˆå¼‚æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆADMï¼‰å’ŒåŒæ¨¡æ‹Ÿåº¦é‡ï¼ŒåŒæ—¶ç¼“è§£æ‰°åŠ¨å¹¶å¤„ç†éƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼›ç†è®ºä¸Šæ¨å¯¼ä»·å€¼å‡½æ•°è¿‘ä¼¼è¯¯å·®ä¸Šç•Œï¼Œå®éªŒè¯æ˜å…¶ä½¿Roboschoolä»»åŠ¡å›æŠ¥æå‡è‡³å°‘14.18%ï¼Œæ˜¯é¦–ä¸ªç”¨æ‰©æ•£æ¨¡å‹è¿‘ä¼¼å› æœçŠ¶æ€ä¸”å…¼å…·ç†è®ºä¸å®ç”¨ä»·å€¼çš„æ¡†æ¶ã€‚",
    "tags": [
      "Reinforcement Learning",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºCaDiffæ¡†æ¶ï¼Œç»“åˆå¼‚æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆADMï¼‰å’ŒåŒæ¨¡æ‹Ÿåº¦é‡ï¼Œè§£å†³PÂ²OMDPä¸­ç°æœ‰æ–¹æ³•æ— æ³•åŒæ—¶ç¼“è§£æ‰°åŠ¨ä¸å¤„ç†éƒ¨åˆ†å¯è§‚æµ‹æ€§çš„é—®é¢˜",
      "å»ºç«‹ç†è®ºä¿è¯ï¼ˆæ¨å¯¼ä»·å€¼å‡½æ•°è¿‘ä¼¼è¯¯å·®ä¸Šç•Œï¼‰ï¼Œå®éªŒéªŒè¯æ•ˆæœï¼Œæ˜¯é¦–ä¸ªç”¨æ‰©æ•£æ¨¡å‹è¿‘ä¼¼å› æœçŠ¶æ€ä¸”å…¼å…·ç†è®ºä¸¥è°¨æ€§å’Œå®ç”¨æ€§çš„æ¡†æ¶"
    ],
    "processed_at": "2025-12-02T08:54:33.124562"
  },
  {
    "id": "2512.01831v1",
    "title": "Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models",
    "abstract": "Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: \"Diversity-Prioritized\" (MIM), \"Compression-Prioritized\" (AR), and \"Decoupled\" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.",
    "authors": [
      "Yudi Wu",
      "Wenhao Zhao",
      "Dianbo Liu"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01831v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01831v1",
    "fetched_at": "2025-12-02T08:35:22.484627",
    "chinese_title": "è§£æ„ç”Ÿæˆå¤šæ ·æ€§ï¼šç¦»æ•£æ½œåœ¨ç”Ÿæˆæ¨¡å‹çš„ä¿¡æ¯ç“¶é¢ˆåˆ†æ",
    "chinese_summary": "æœ¬æ–‡åŸºäºä¿¡æ¯ç“¶é¢ˆç†è®ºæå‡ºè¯Šæ–­æ¡†æ¶ï¼Œåˆ†æç¦»æ•£æ½œåœ¨ç”Ÿæˆæ¨¡å‹ï¼ˆARã€MIMã€Diffusionï¼‰çš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œå°†å…¶åˆ†è§£ä¸ºè·¯å¾„å¤šæ ·æ€§ä¸æ‰§è¡Œå¤šæ ·æ€§ï¼›é€šè¿‡é›¶æ ·æœ¬å¹²é¢„æ–¹æ³•æ­ç¤ºä¸‰ç±»æ¨¡å‹çš„ä¸åŒç­–ç•¥ï¼Œå¹¶æå‡ºæ¨ç†æ—¶å¤šæ ·æ€§å¢å¼ºæŠ€æœ¯ã€‚",
    "tags": [
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "æ„å»ºåŸºäºä¿¡æ¯ç“¶é¢ˆçš„è¯Šæ–­æ¡†æ¶ï¼Œå¯æ“ä½œåŒ–åˆ†è§£ç”Ÿæˆå¤šæ ·æ€§ä¸ºè·¯å¾„å¤šæ ·æ€§ä¸æ‰§è¡Œå¤šæ ·æ€§",
      "æ­ç¤ºARã€MIMã€Diffusionä¸‰ç±»æ¨¡å‹çš„å¤šæ ·æ€§ç­–ç•¥å·®å¼‚ï¼Œæå‡ºæ¨ç†æ—¶å¤šæ ·æ€§å¢å¼ºæŠ€æœ¯"
    ],
    "processed_at": "2025-12-02T08:54:49.674456"
  },
  {
    "id": "2512.01822v1",
    "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
    "abstract": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
    "authors": [
      "Jintian Zhang",
      "Kewei Xu",
      "Jingsheng Zheng",
      "Zhuoyun Yu",
      "Yuqi Zhu",
      "Yujie Luo",
      "Lanning Wei",
      "Shuofei Qiao",
      "Lun Du",
      "Da Zheng",
      "Shumin Deng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01822v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01822v1",
    "fetched_at": "2025-12-02T08:35:22.484684",
    "chinese_title": "InnoGymï¼šAIæ™ºèƒ½ä½“åˆ›æ–°æ½œåŠ›çš„åŸºå‡†è¯„ä¼°æ¡†æ¶",
    "chinese_summary": "ç°æœ‰AIæ™ºèƒ½ä½“åŸºå‡†å¤šèšç„¦è¾“å‡ºæ­£ç¡®æ€§ï¼Œå¿½ç•¥æ–¹æ³•åŸåˆ›æ€§ï¼›è®ºæ–‡æå‡ºé¦–ä¸ªç³»ç»Ÿè¯„ä¼°AIæ™ºèƒ½ä½“åˆ›æ–°æ½œåŠ›çš„åŸºå‡†æ¡†æ¶InnoGymï¼ŒåŒ…å«æ€§èƒ½å¢ç›Šï¼ˆå¯¹æ¯”å·²çŸ¥æœ€ä¼˜è§£çš„æå‡ï¼‰å’Œæ–°é¢–æ€§ï¼ˆæ–¹æ³•ä¸å…ˆéªŒå·®å¼‚ï¼‰ä¸¤ä¸ªäº’è¡¥æŒ‡æ ‡ï¼Œè¿˜æ„å»º18ä¸ªçœŸå®é¢†åŸŸä»»åŠ¡åŠç»Ÿä¸€æ‰§è¡Œç¯å¢ƒiGymï¼Œå®éªŒæ­ç¤ºåˆ›æ„ä¸æœ‰æ•ˆæ€§çš„å·®è·ã€‚",
    "tags": [
      "LLM",
      "Benchmark",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºé¦–ä¸ªç³»ç»Ÿè¯„ä¼°AIæ™ºèƒ½ä½“åˆ›æ–°æ½œåŠ›çš„åŸºå‡†æ¡†æ¶InnoGymï¼Œå«æ€§èƒ½å¢ç›Šä¸æ–°é¢–æ€§ä¸¤ä¸ªäº’è¡¥æŒ‡æ ‡",
      "æ„å»ºå«18ä¸ªçœŸå®é¢†åŸŸä»»åŠ¡çš„åŸºå‡†åŠç»Ÿä¸€æ‰§è¡Œç¯å¢ƒiGymï¼Œæ­ç¤ºAIæ™ºèƒ½ä½“åˆ›æ„ä¸æœ‰æ•ˆæ€§çš„æ ¸å¿ƒå·®è·"
    ],
    "processed_at": "2025-12-02T08:55:09.520755"
  },
  {
    "id": "2512.01678v1",
    "title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale",
    "abstract": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.",
    "authors": [
      " Anubhab",
      "Rupesh Nasre"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.DC",
      "cs.PL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01678v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01678v1",
    "fetched_at": "2025-12-02T08:35:22.484708",
    "chinese_title": "Morphlingï¼šå¤§è§„æ¨¡ä¸‹å¿«é€Ÿã€èåˆä¸”çµæ´»çš„å›¾ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "chinese_summary": "GNNè®­ç»ƒå› èåˆä¸è§„åˆ™å›¾éå†ä¸è§„åˆ™çŸ©é˜µè¿ç®—é¢ä¸´ç¡¬ä»¶æŒ‘æˆ˜ï¼Œç°æœ‰PyGã€DGLç­‰æ¡†æ¶ä¾èµ–é€šç”¨å†…æ ¸å¯¼è‡´æ€§èƒ½ä¸ä½³ï¼›Morphlingä½œä¸ºé¢†åŸŸç‰¹å®šä»£ç åˆæˆå™¨ï¼Œå¯å°†é«˜å±‚GNNè§„èŒƒç¼–è¯‘ä¸ºå¤šåç«¯ä¼˜åŒ–å®ç°ï¼Œå¹¶é›†æˆç¨€ç–æ„ŸçŸ¥å¼•æ“åŠ¨æ€é€‰æ‰§è¡Œè·¯å¾„ï¼Œè¯„ä¼°æ˜¾ç¤ºå…¶è®­ç»ƒååé‡å¹³å‡æå‡20Xï¼ˆCPUï¼‰/19Xï¼ˆGPUï¼‰ï¼Œå†…å­˜æ¶ˆè€—æœ€å¤šå‡å°‘15Xã€‚",
    "tags": [
      "Graph Neural Network",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºé¢†åŸŸç‰¹å®šä»£ç åˆæˆå™¨Morphlingï¼Œé€šè¿‡æ¶æ„æ„ŸçŸ¥åŸè¯­åº“ç¼–è¯‘é«˜å±‚GNNè§„èŒƒåˆ°å¤šåç«¯ï¼ˆOpenMPã€CUDAã€MPIï¼‰ï¼Œè§£å†³ç°æœ‰æ¡†æ¶é€šç”¨å†…æ ¸çš„æ€§èƒ½ç“¶é¢ˆ",
      "é›†æˆè¿è¡Œæ—¶ç¨€ç–æ„ŸçŸ¥æ‰§è¡Œå¼•æ“ä¸å†…å­˜é«˜æ•ˆå¸ƒå±€ï¼ŒåŠ¨æ€ä¼˜åŒ–æ‰§è¡Œè·¯å¾„å¹¶é™ä½å³°å€¼å†…å­˜æ¶ˆè€—ï¼Œæ˜¾è‘—æå‡GNNè®­ç»ƒååé‡ä¸èµ„æºæ•ˆç‡"
    ],
    "processed_at": "2025-12-02T08:55:27.445008"
  },
  {
    "id": "2512.01603v1",
    "title": "MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark",
    "abstract": "Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\\footnote{https://github.com/Gatsby-web/MAC\\_SLU} and datasets\\footnote{huggingface.co/datasets/Gatsby1984/MAC\\_SLU} are released publicly.",
    "authors": [
      "Yuezhang Peng",
      "Chonghao Cai",
      "Ziang Liu",
      "Shuai Fan",
      "Sheng Jiang",
      "Hua Xu",
      "Yuxin Liu",
      "Qiguang Chen",
      "Kele Xu",
      "Yao Li",
      "Sheng Wang",
      "Libo Qin",
      "Xie Chen"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CL",
      "cs.MM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01603v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01603v1",
    "fetched_at": "2025-12-02T08:35:22.484753",
    "chinese_title": "MAC-SLUï¼šå¤šæ„å›¾æ±½è½¦åº§èˆ±å£è¯­ç†è§£åŸºå‡†",
    "chinese_summary": "è®ºæ–‡æå‡ºMAC-SLUå¤šæ„å›¾æ±½è½¦åº§èˆ±å£è¯­ç†è§£æ•°æ®é›†ï¼Œé€šè¿‡çœŸå®å¤æ‚å¤šæ„å›¾æ•°æ®æå‡ä»»åŠ¡éš¾åº¦ï¼›åŸºäºè¯¥æ•°æ®é›†å¯¹é¢†å…ˆå¼€æºLLMå’ŒLALMå¼€å±•å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œè¦†ç›–ä¸Šä¸‹æ–‡å­¦ä¹ ã€ç›‘ç£å¾®è°ƒç­‰èŒƒå¼ï¼Œå‘ç°ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½è½åäºç›‘ç£å¾®è°ƒï¼Œç«¯åˆ°ç«¯LALMæ€§èƒ½å¯åª²ç¾pipelineä¸”é¿å…è¯­éŸ³è¯†åˆ«è¯¯å·®ä¼ æ’­ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºMAC-SLUå¤šæ„å›¾æ±½è½¦åº§èˆ±å£è¯­ç†è§£æ•°æ®é›†ï¼Œå¼•å…¥çœŸå®å¤æ‚å¤šæ„å›¾æ•°æ®æå‡SLUä»»åŠ¡éš¾åº¦",
      "åŸºäºMAC-SLUå¯¹é¢†å…ˆå¼€æºLLMå’ŒLALMè¿›è¡Œå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºä¸åŒæ–¹æ³•çš„æ€§èƒ½å·®å¼‚åŠç«¯åˆ°ç«¯LALMçš„ä¼˜åŠ¿"
    ],
    "processed_at": "2025-12-02T08:55:36.563312"
  },
  {
    "id": "2512.01502v1",
    "title": "Formal Verification of Noisy Quantum Reinforcement Learning Policies",
    "abstract": "Quantum reinforcement learning (QRL) aims to use quantum effects to create sequential decision-making policies that achieve tasks more effectively than their classical counterparts. However, QRL policies face uncertainty from quantum measurements and hardware noise, such as bit-flip, phase-flip, and depolarizing errors, which can lead to unsafe behavior. Existing work offers no systematic way to verify whether trained QRL policies meet safety requirements under specific noise conditions.   We introduce QVerifier, a formal verification method that applies probabilistic model checking to analyze trained QRL policies with and without modeled quantum noise. QVerifier builds a complete model of the policy-environment interaction, incorporates quantum uncertainty directly into the transition probabilities, and then checks safety properties using the Storm model checker.   Experiments across multiple QRL environments show that QVerifier precisely measures how different noise models influence safety, revealing both performance degradation and cases where noise can help. By enabling rigorous safety verification before deployment, QVerifier addresses a critical need: because access to quantum hardware is expensive, pre-deployment verification is essential for any safety-critical use of QRL. QVerifier targets a potential classical-quantum sweet spot: trained QRL policies that execute efficiently on quantum hardware, yet remain tractable for classical probabilistic model checking despite being too slow for real-time classical deployment.",
    "authors": [
      "Dennis Gross"
    ],
    "published": "2025-12-01",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.FL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01502v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01502v1",
    "fetched_at": "2025-12-02T08:35:22.484776",
    "chinese_title": "å¸¦å™ªå£°é‡å­å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å½¢å¼åŒ–éªŒè¯",
    "chinese_summary": "é‡å­å¼ºåŒ–å­¦ä¹ ï¼ˆQRLï¼‰å› é‡å­æµ‹é‡ä¸ç¡¬ä»¶å™ªå£°æ˜“å¼•å‘ä¸å®‰å…¨è¡Œä¸ºï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç³»ç»ŸéªŒè¯æ‰‹æ®µï¼›è®ºæ–‡æå‡ºQVerifierå½¢å¼åŒ–éªŒè¯æ–¹æ³•ï¼Œé€šè¿‡æ¦‚ç‡æ¨¡å‹æ£€æŸ¥å»ºæ¨¡ç­–ç•¥-ç¯å¢ƒäº¤äº’å¹¶çº³å…¥é‡å­ä¸ç¡®å®šæ€§ï¼Œç»“åˆStormå·¥å…·éªŒè¯å®‰å…¨å±æ€§ï¼›å®éªŒè¡¨æ˜QVerifierå¯ç²¾å‡†åº¦é‡ä¸åŒå™ªå£°æ¨¡å‹å¯¹å®‰å…¨çš„å½±å“ï¼Œè§£å†³äº†QRLéƒ¨ç½²å‰çš„å®‰å…¨éªŒè¯å…³é”®éœ€æ±‚ã€‚",
    "tags": [
      "Reinforcement Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "æå‡ºQVerifierå½¢å¼åŒ–éªŒè¯æ–¹æ³•ï¼Œé¦–æ¬¡ç³»ç»Ÿå®ç°å¸¦å™ªå£°QRLç­–ç•¥çš„å®‰å…¨éªŒè¯",
      "é€šè¿‡å®éªŒç²¾å‡†åº¦é‡ä¸åŒé‡å­å™ªå£°æ¨¡å‹å¯¹QRLç­–ç•¥å®‰å…¨çš„å½±å“ï¼Œæ”¯æ’‘å®‰å…¨å…³é”®åœºæ™¯éƒ¨ç½²"
    ],
    "processed_at": "2025-12-02T08:55:48.473510"
  },
  {
    "id": "2512.01452v1",
    "title": "Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework",
    "abstract": "Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.",
    "authors": [
      "Lingbo Li",
      "Anuradha Mathrani",
      "Teo Susnjak"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01452v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01452v1",
    "fetched_at": "2025-12-02T08:35:22.484801",
    "chinese_title": "éšæœºå¯¹ç…§è¯•éªŒçš„è‡ªåŠ¨åŒ–åå€šé£é™©è¯„ä¼°ï¼šGEPAè®­ç»ƒçš„ç¨‹åºåŒ–æç¤ºæ¡†æ¶åˆæ¢",
    "chinese_summary": "è¯¥ç ”ç©¶æå‡ºåŸºäºDSPyçš„GEPAæ¨¡å—æ„å»ºå¯ç¼–ç¨‹åå€šé£é™©ï¼ˆRoBï¼‰è¯„ä¼° pipelineï¼Œæ›¿ä»£æ‰‹åŠ¨æç¤ºè®¾è®¡ï¼Œé€šè¿‡å¸•ç´¯æ‰˜å¼•å¯¼æœç´¢ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å¹¶ç”Ÿæˆå¯æ£€æŸ¥è½¨è¿¹ï¼›åœ¨7ä¸ªRoBé¢†åŸŸçš„100ä¸ªéšæœºå¯¹ç…§è¯•éªŒï¼ˆRCTï¼‰ä¸ŠéªŒè¯ï¼ŒGEPAæç¤ºæ˜¾è‘—æå‡éƒ¨åˆ†é¢†åŸŸï¼ˆå¦‚éšæœºåºåˆ—ç”Ÿæˆï¼‰è¯„ä¼°å‡†ç¡®æ€§ï¼Œæ•´ä½“æ€§èƒ½ä¼˜äºæ‰‹åŠ¨è®¾è®¡æç¤ºã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Risk Management",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºåŸºäºDSPy-GEPAçš„å¯ç¼–ç¨‹RoBè¯„ä¼° pipelineï¼Œå®ç°LLMæ¨ç†çš„å¯å¤ç°ã€å¯ä¼˜åŒ–ï¼Œæ›¿ä»£æ‰‹åŠ¨æç¤ºè®¾è®¡",
      "åœ¨100ä¸ªRCTçš„7ä¸ªRoBé¢†åŸŸéªŒè¯ï¼ŒGEPAæç¤ºæ˜¾è‘—æå‡éƒ¨åˆ†é¢†åŸŸè¯„ä¼°å‡†ç¡®æ€§ï¼Œæ•´ä½“æ€§èƒ½ä¼˜äºæ‰‹åŠ¨è®¾è®¡æç¤º"
    ],
    "processed_at": "2025-12-02T08:56:07.221422"
  },
  {
    "id": "2512.01396v1",
    "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches",
    "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.",
    "authors": [
      "Zhiqing Zhong",
      "Jiaming Huang",
      "Pinjia He"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.CR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01396v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01396v1",
    "fetched_at": "2025-12-02T08:35:22.484831",
    "chinese_title": "BackportBenchï¼šè¡¥ä¸è‡ªåŠ¨åå‘ç§»æ¤çš„å¤šè¯­è¨€åŸºå‡†",
    "chinese_summary": "é’ˆå¯¹ç°æœ‰è‡ªåŠ¨è¡¥ä¸åå‘ç§»æ¤æŠ€æœ¯æ•ˆæœè¯„ä¼°ä¸è¶³çš„é—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥é¦–ä¸ªç»¼åˆåŸºå‡†BackportBenchï¼ŒåŒ…å«å¤šè¯­è¨€ï¼ˆPyPIã€Mavenã€npmï¼‰çš„202ä¸ªè¡¥ä¸åå‘ç§»æ¤é—®é¢˜åŠå¯æ‰§è¡ŒDockerç¯å¢ƒä¸æµ‹è¯•ç”¨ä¾‹ï¼›é€šè¿‡è¯¥åŸºå‡†è¯„ä¼°ä¼ ç»Ÿæ–¹æ³•ä¸LLMæŠ€æœ¯ï¼Œå‘ç°æ™ºèƒ½æ–¹æ³•åœ¨éœ€é€»è¾‘/ç»“æ„å˜æ›´çš„åœºæ™¯ä¸­è¡¨ç°æ›´ä¼˜ï¼Œè·¨è¯­è¨€æ€§èƒ½å­˜åœ¨å·®å¼‚å¹¶ç»™å‡ºç›¸å…³å¯ç¤ºã€‚",
    "tags": [
      "LLM",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºé¦–ä¸ªå¤šè¯­è¨€è¡¥ä¸åå‘ç§»æ¤ç»¼åˆåŸºå‡†BackportBenchï¼ŒåŒ…å«202ä¸ªé—®é¢˜ã€Dockerç¯å¢ƒåŠæµ‹è¯•ç”¨ä¾‹",
      "åŸºäºè¯¥åŸºå‡†è¯„ä¼°ä¼ ç»Ÿæ–¹æ³•ä¸LLMæŠ€æœ¯ï¼Œæ­ç¤ºæ™ºèƒ½æ–¹æ³•ä¼˜åŠ¿åŠè·¨è¯­è¨€æ€§èƒ½å·®å¼‚ï¼Œç»™å‡ºç ”ç©¶ä¸å®è·µå¯ç¤º"
    ],
    "processed_at": "2025-12-02T08:56:16.438744"
  },
  {
    "id": "2512.01311v1",
    "title": "CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL",
    "abstract": "Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.",
    "authors": [
      "Shinji Mai",
      "Yunpeng Zhai",
      "Ziqian Chen",
      "Cheng Chen",
      "Anni Zou",
      "Shuchang Tao",
      "Zhaoyang Liu",
      "Bolin Ding"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01311v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01311v1",
    "fetched_at": "2025-12-02T08:35:22.484865",
    "chinese_title": "CuESï¼šä¸€ç§å¥½å¥‡å¿ƒé©±åŠ¨ä¸”ç¯å¢ƒæ¥åœ°çš„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åˆæˆæ¡†æ¶",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­æ— é¢„å®šä¹‰ä»»åŠ¡çš„ä»»åŠ¡ç¨€ç¼ºç“¶é¢ˆï¼Œæå‡ºCuESæ¡†æ¶â€”â€”æ— éœ€æ‰‹å·¥ç§å­æˆ–å¤–éƒ¨è¯­æ–™ï¼Œé€šè¿‡å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢ã€æŠ½è±¡äº¤äº’æ¨¡å¼ä¸ºå¯å¤ç”¨ä»»åŠ¡schemaï¼Œç»“åˆè½»é‡è‡ªä¸Šè€Œä¸‹æŒ‡å¯¼ä¸è®°å¿†è´¨é‡æ§åˆ¶ï¼Œè‡ªä¸»ä»ç¯å¢ƒç»“æ„ä¸æ•ˆç”¨ä¸­ç”Ÿæˆå¤šæ ·ã€å¯æ‰§è¡Œä¸”æœ‰æ„ä¹‰çš„ä»»åŠ¡ï¼›è¯¥æ¡†æ¶åœ¨AppWorldç­‰ç¯å¢ƒä¸­ç”Ÿæˆçš„ä»»åŠ¡åˆ†å¸ƒåŒ¹é…æˆ–è¶…è¶Šäººå·¥æ•°æ®é›†ï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸ç­–ç•¥æ€§èƒ½ã€‚",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Deep Learning"
    ],
    "key_contributions": [
      "å½¢å¼åŒ–æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„ä»»åŠ¡ç”Ÿæˆé—®é¢˜ï¼Œè§£å†³æ— é¢„å®šä¹‰ä»»åŠ¡çš„ä»»åŠ¡ç¨€ç¼ºç“¶é¢ˆ",
      "æå‡ºCuESæ¡†æ¶ï¼Œè‡ªä¸»ä»ç¯å¢ƒä¸­ç”Ÿæˆå¤šæ ·å¯æ‰§è¡Œä»»åŠ¡ï¼Œæ— éœ€æ‰‹å·¥ç§å­ï¼Œå¤šç¯å¢ƒä¸‹ä»»åŠ¡è´¨é‡ä¼˜äºäººå·¥æ•°æ®é›†å¹¶æå‡ä¸‹æ¸¸ç­–ç•¥æ€§èƒ½"
    ],
    "processed_at": "2025-12-02T08:56:34.777469"
  },
  {
    "id": "2512.01187v1",
    "title": "Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement",
    "abstract": "Transformer models often exhibit brittle extrapolation, failing on inputs that are longer or structurally more complex than those seen during training. We introduce Counter-Example-Driven Curricula (CEDC), an automated framework that improves model robustness by iteratively focusing on its own failures. At each step, CEDC uses the current model to generate a diverse set of candidate problems, employs a fast, executable verifier to identify incorrect predictions (counter-examples), and then fine-tunes the model on a dataset enriched with these discovered failures. We evaluate CEDC on a suite of algorithmic and natural language tasks, including integer addition, sorting, Dyck-2 language recognition, and three text classification benchmarks. Compared to static training and standard curriculum learning baselines, CEDC achieves up to 30x greater length extrapolation, is 3.75x more computationally efficient than uniform data augmentation, and requires no manual difficulty heuristics. We provide a detailed analysis of the counter-examples, showing how the curriculum naturally adapts to target progressively more complex error modes. Our findings establish verifier-guided, failure-driven learning as a simple, powerful, and efficient paradigm for enhancing the generalization capabilities of Transformer models.",
    "authors": [
      "Harshil Vejendla"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01187v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01187v1",
    "fetched_at": "2025-12-02T08:35:22.484884",
    "chinese_title": "ä»¥å¤±è´¥æ•™å­¦ï¼šç”¨äºTransformerè‡ªæˆ‘æå‡çš„åä¾‹é©±åŠ¨è¯¾ç¨‹",
    "chinese_summary": "é’ˆå¯¹Transformeræ¨¡å‹å¤–æ¨è„†æ€§é—®é¢˜ï¼Œè®ºæ–‡æå‡ºåä¾‹é©±åŠ¨è¯¾ç¨‹ï¼ˆCEDCï¼‰æ¡†æ¶ï¼šè¿­ä»£ç”Ÿæˆå€™é€‰é—®é¢˜ã€ç”¨å¿«é€ŸéªŒè¯å™¨è¯†åˆ«é”™è¯¯é¢„æµ‹ï¼ˆåä¾‹ï¼‰å¹¶å¾®è°ƒæ¨¡å‹ï¼Œåœ¨ç®—æ³•ä¸è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­æ˜¾è‘—æå‡é•¿åº¦å¤–æ¨èƒ½åŠ›ä¸è®¡ç®—æ•ˆç‡ï¼Œæ— éœ€æ‰‹åŠ¨éš¾åº¦å¯å‘å¼ã€‚",
    "tags": [
      "Transformer",
      "Deep Learning",
      "LLM",
      "NLP"
    ],
    "key_contributions": [
      "æå‡ºåä¾‹é©±åŠ¨è¯¾ç¨‹ï¼ˆCEDCï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£å¤±è´¥å‘ç°ä¸å¾®è°ƒæå‡Transformeré²æ£’æ€§",
      "ç›¸æ¯”é™æ€è®­ç»ƒä¸æ ‡å‡†è¯¾ç¨‹å­¦ä¹ ï¼Œåœ¨é•¿åº¦å¤–æ¨ã€è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ï¼Œæ— éœ€æ‰‹åŠ¨éš¾åº¦å¯å‘å¼"
    ],
    "processed_at": "2025-12-02T08:56:42.912314"
  },
  {
    "id": "2512.01181v1",
    "title": "First On-Orbit Demonstration of a Geospatial Foundation Model",
    "abstract": "Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.",
    "authors": [
      "Andrew Du",
      "Roberto Del Prete",
      "Alejandro Mousist",
      "Nick Manser",
      "Fabrice Marre",
      "Andrew Barton",
      "Carl Seubert",
      "Gabriele Meoni",
      "Tat-Jun Chin"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01181v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01181v1",
    "fetched_at": "2025-12-02T08:35:22.484921",
    "chinese_title": "åœ°å­¦åŸºç¡€æ¨¡å‹çš„é¦–æ¬¡åœ¨è½¨æ¼”ç¤º",
    "chinese_summary": "é’ˆå¯¹åœ°å­¦åŸºç¡€æ¨¡å‹ï¼ˆGeoFMï¼‰ä½“ç§¯å¤§éš¾ä»¥åœ¨èµ„æºå—é™å¤ªç©ºç¡¬ä»¶éƒ¨ç½²çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåŸºäºè§†è§‰Transformerï¼ˆViTï¼‰çš„ç´§å‡‘GeoFMå˜ä½“ï¼Œé€šè¿‡æ¨¡å‹å‹ç¼©ä¸é¢†åŸŸè‡ªé€‚åº”ä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å¹¶æ”¯æŒæ˜Ÿä¸Šæ‰§è¡Œï¼›è¿˜åœ¨å›½é™…ç©ºé—´ç«™IMAGIN-eè½½è·ä¸Šæ¼”ç¤ºäº†å¯é åœ¨è½¨æ¨ç†ï¼Œå»ºç«‹äº†ä»å¤§å‹GeoFMåˆ°é€‚èˆªé«˜æ•ˆéƒ¨ç½²çš„è·¯å¾„ã€‚",
    "tags": [
      "Deep Learning",
      "Transformer"
    ],
    "key_contributions": [
      "æå‡ºåŸºäºViTçš„ç´§å‡‘GeoFMå˜ä½“ï¼Œé€šè¿‡æ¨¡å‹å‹ç¼©ä¸é¢†åŸŸè‡ªé€‚åº”å¹³è¡¡æ€§èƒ½ä¸èµ„æºéœ€æ±‚ï¼Œé€‚é…å¤ªç©ºç¡¬ä»¶éƒ¨ç½²",
      "åœ¨å›½é™…ç©ºé—´ç«™å®ç°GeoFMé¦–æ¬¡å¯é åœ¨è½¨æ¨ç†ï¼Œå»ºç«‹å¤§å‹GeoFMåˆ°é€‚èˆªéƒ¨ç½²çš„æŠ€æœ¯è·¯å¾„"
    ],
    "processed_at": "2025-12-02T08:56:51.967770"
  },
  {
    "id": "2512.01113v1",
    "title": "Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning",
    "abstract": "Algorithmic reasoning -- the ability to perform step-by-step logical inference -- has become a core benchmark for evaluating reasoning in graph neural networks (GNNs) and large language models (LLMs). Ideally, one would like to design a single model capable of performing well on multiple algorithmic reasoning tasks simultaneously. However, this is challenging when the execution steps of algorithms differ from one another, causing negative interference when they are trained together.   We propose branching neural networks, a principled architecture for multitask algorithmic reasoning. Searching for the optimal $k$-ary tree with $L$ layers over $n$ algorithmic tasks is combinatorial, requiring exploration of up to $k^{nL}$ possible structures. We develop AutoBRANE, an efficient algorithm that reduces this search to $O(nL)$ time by solving a convex relaxation at each layer to approximate an optimal task partition. The method clusters tasks using gradient-based affinity scores and can be used on top of any base model, including GNNs and LLMs.   We validate AutoBRANE on a broad suite of graph-algorithmic and text-based reasoning benchmarks. We show that gradient features estimate true task performance within 5% error across four GNNs and four LLMs (up to 34B parameters). On the CLRS benchmark, it outperforms the strongest single multitask GNN by 3.7% and the best baseline by 1.2%, while reducing runtime by 48% and memory usage by 26%. The learned branching structures reveal an intuitively reasonable hierarchical clustering of related algorithms. On three text-based graph reasoning benchmarks, AutoBRANE improves over the best non-branching multitask baseline by 3.2%. Finally, on a large graph dataset with 21M edges and 500 tasks, AutoBRANE achieves a 28% accuracy gain over existing multitask and branching architectures, along with a 4.5$\\times$ reduction in runtime.",
    "authors": [
      "Dongyue Li",
      "Zhenshuo Zhang",
      "Minxuan Duan",
      "Edgar Dobriban",
      "Hongyang R. Zhang"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01113v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01113v1",
    "fetched_at": "2025-12-02T08:35:22.484950",
    "chinese_title": "ç”¨äºå¤šä»»åŠ¡ç®—æ³•æ¨ç†çš„é«˜æ•ˆåˆ†æ”¯ç½‘ç»œå­¦ä¹ ",
    "chinese_summary": "æœ¬æ–‡æå‡ºåˆ†æ”¯ç¥ç»ç½‘ç»œæ¶æ„ä»¥è§£å†³å¤šä»»åŠ¡ç®—æ³•æ¨ç†ä¸­ä»»åŠ¡æ‰§è¡Œæ­¥éª¤å·®å¼‚å¯¼è‡´çš„è´Ÿå¹²æ‰°é—®é¢˜ï¼Œå¼€å‘AutoBRANEç®—æ³•é€šè¿‡å‡¸æ¾å¼›ä¸æ¢¯åº¦äº²å’Œåº¦èšç±»å°†ç»„åˆç»“æ„æœç´¢å¤æ‚åº¦é™è‡³O(nL)ï¼Œåœ¨CLRSç­‰åŸºå‡†ä¸ŠéªŒè¯å…¶ä¼˜äºç°æœ‰æ–¹æ³•ä¸”é™ä½è¿è¡Œæ—¶ä¸å†…å­˜æ¶ˆè€—ã€‚",
    "tags": [
      "LLM",
      "Graph Neural Network",
      "Benchmark",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºåˆ†æ”¯ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä¸ºå¤šä»»åŠ¡ç®—æ³•æ¨ç†æä¾›ç¼“è§£ä»»åŠ¡é—´è´Ÿå¹²æ‰°çš„åŸåˆ™æ€§æ¶æ„",
      "å¼€å‘AutoBRANEç®—æ³•ï¼Œå°†æœ€ä¼˜åˆ†æ”¯ç»“æ„æœç´¢å¤æ‚åº¦ä»O(k^(nL))é™è‡³O(nL)ï¼Œæ”¯æŒGNNã€LLMç­‰ä»»æ„åŸºç¡€æ¨¡å‹"
    ],
    "processed_at": "2025-12-02T08:57:06.770592"
  },
  {
    "id": "2512.01093v1",
    "title": "Bayesian dynamic scheduling of multipurpose batch processes under incomplete look-ahead information",
    "abstract": "Multipurpose batch processes become increasingly popular in manufacturing industries since they adapt to low-volume, high-value products and shifting demands. These processes often operate in a dynamic environment, which faces disturbances such as processing delays and demand changes. To minimise long-term cost and system nervousness (i.e., disruptive changes to schedules), schedulers must design rescheduling strategies to address such disturbances effectively. Existing methods often assume complete look-ahead information over the scheduling horizon. This assumption contrasts with realistic situations where schedulers can only access incomplete look-ahead information. Sticking with existing methods may lead to suboptimal long-term costs and high-level system nervousness. In this work we propose a Bayesian dynamic scheduling method. Our method relies on learning a Bayesian Network from the probability distribution of disturbances. Specifically, the Bayesian Network represents how likely each operation will be impacted by disturbances. During the online execution, when new disturbances become observed, this method updates the posterior distribution and therefore guides the rescheduling strategy. We compare our method with the existing periodic rescheduling strategy (which generates new schedules from scratch at fixed intervals) on four benchmark problems. Computational results show that our method achieves statistically better long-term costs and system nervousness. In the theoretical aspect, we prove that if disturbances are mutually independent, the impact-quantifying variables inherently satisfy the independence assumptions required by Bayesian Networks. As an implication, practitioners can extend the method to other scheduling problems (such as job shop scheduling and continuous processes), given that they define the problem-specific dependencies between operations.",
    "authors": [
      "Taicheng Zheng",
      "Dan Li",
      "Jie Li"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.LG",
      "eess.SY",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01093v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01093v1",
    "fetched_at": "2025-12-02T08:35:22.484975",
    "chinese_title": "ä¸å®Œå…¨å‰ç»ä¿¡æ¯ä¸‹å¤šç”¨é€”æ‰¹å¤„ç†è¿‡ç¨‹çš„è´å¶æ–¯åŠ¨æ€è°ƒåº¦",
    "chinese_summary": "é’ˆå¯¹å¤šç”¨é€”æ‰¹å¤„ç†è¿‡ç¨‹ä¸­ç°æœ‰è°ƒåº¦æ–¹æ³•å‡è®¾å®Œå…¨å‰ç»ä¿¡æ¯ä¸ç¬¦åˆå®é™…çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§è´å¶æ–¯åŠ¨æ€è°ƒåº¦æ–¹æ³•ï¼Œé€šè¿‡è´å¶æ–¯ç½‘ç»œå­¦ä¹ æ‰°åŠ¨çš„æ¦‚ç‡åˆ†å¸ƒï¼Œåœ¨çº¿è§‚æµ‹æ–°æ‰°åŠ¨æ—¶æ›´æ–°åéªŒåˆ†å¸ƒä»¥æŒ‡å¯¼é‡è°ƒåº¦ï¼›å®éªŒå¯¹æ¯”æ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨é•¿æœŸæˆæœ¬å’Œç³»ç»Ÿæ‰°åŠ¨æ–¹é¢ä¼˜äºç°æœ‰å‘¨æœŸæ€§é‡è°ƒåº¦ç­–ç•¥ã€‚",
    "tags": [
      "Benchmark",
      "Risk Management"
    ],
    "key_contributions": [
      "æå‡ºä¸å®Œå…¨å‰ç»ä¿¡æ¯ä¸‹çš„è´å¶æ–¯åŠ¨æ€è°ƒåº¦æ–¹æ³•ï¼Œçªç ´ç°æœ‰æ–¹æ³•å®Œå…¨å‰ç»ä¿¡æ¯çš„å‡è®¾å±€é™",
      "å®éªŒéªŒè¯è¯¥æ–¹æ³•åœ¨é•¿æœŸæˆæœ¬å’Œç³»ç»Ÿæ‰°åŠ¨æ§åˆ¶ä¸Šä¼˜äºç°æœ‰å‘¨æœŸæ€§é‡è°ƒåº¦ç­–ç•¥"
    ],
    "processed_at": "2025-12-02T08:57:25.857279"
  },
  {
    "id": "2512.01031v1",
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "abstract": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
    "authors": [
      "Jiaming Tang",
      "Yufei Sun",
      "Yilong Zhao",
      "Shang Yang",
      "Yujun Lin",
      "Zhuoyang Zhang",
      "James Hou",
      "Yao Lu",
      "Zhijian Liu",
      "Song Han"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01031v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01031v1",
    "fetched_at": "2025-12-02T08:35:22.485013",
    "chinese_title": "VLASHï¼šåŸºäºæœªæ¥çŠ¶æ€æ„ŸçŸ¥å¼‚æ­¥æ¨ç†çš„å®æ—¶è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰éƒ¨ç½²æ•ˆç‡ä½ã€å¼‚æ­¥æ¨ç†å­˜åœ¨é¢„æµ‹ä¸æ‰§è¡Œæ—¶é—´é”™ä½çš„é—®é¢˜ï¼Œæå‡ºVLASHæ¡†æ¶â€”â€”é€šè¿‡ç”¨å…ˆå‰åŠ¨ä½œå—æ»šåŠ¨æœºå™¨äººçŠ¶æ€ä¼°è®¡æœªæ¥æ‰§è¡Œæ—¶çš„çŠ¶æ€ï¼Œæ— éœ€é¢å¤–å¼€é”€æˆ–æ¶æ„ä¿®æ”¹ï¼›å®éªŒè¡¨æ˜å…¶é€Ÿåº¦æå‡è¾¾2.03xã€ååº”å»¶è¿Ÿé™ä½17.4xä¸”å®Œå…¨ä¿ç•™åŸç²¾åº¦ï¼Œæ”¯æŒä¹’ä¹“ã€æ‰“åœ°é¼ ç­‰ä¼ ç»ŸåŒæ­¥æ¨ç†æ— æ³•å®Œæˆçš„å¿«ååº”é«˜ç²¾åº¦ä»»åŠ¡ã€‚",
    "tags": [
      "Deep Learning",
      "Transformer",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "æå‡ºVLASHé€šç”¨å¼‚æ­¥æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æœªæ¥çŠ¶æ€ä¼°è®¡è§£å†³VLAå¼‚æ­¥æ¨ç†ä¸­é¢„æµ‹ä¸æ‰§è¡Œçš„æ—¶é—´é”™ä½é—®é¢˜ï¼Œæ— éœ€é¢å¤–è®¡ç®—å¼€é”€æˆ–æ¶æ„ä¿®æ”¹",
      "å®éªŒéªŒè¯VLASHå¯ä½¿VLAé€Ÿåº¦æå‡2.03xã€ååº”å»¶è¿Ÿé™ä½17.4xä¸”å®Œå…¨ä¿ç•™åŸç²¾åº¦ï¼Œæ”¯æŒä¼ ç»ŸåŒæ­¥æ¨ç†æ— æ³•å®Œæˆçš„å¿«ååº”é«˜ç²¾åº¦ä»»åŠ¡"
    ],
    "processed_at": "2025-12-02T08:57:46.789814"
  },
  {
    "id": "2512.01017v1",
    "title": "ChartAnchor: Chart Grounding with Structural-Semantic Fidelity",
    "abstract": "Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.",
    "authors": [
      "Xinhang Li",
      "Jingbo Zhou",
      "Pengfei Luo",
      "Yixiong Xiao",
      "Tong Xu"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01017v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01017v1",
    "fetched_at": "2025-12-02T08:35:22.485040",
    "chinese_title": "ChartAnchorï¼šå…·æœ‰ç»“æ„è¯­ä¹‰ä¿çœŸåº¦çš„å›¾è¡¨å®šä½",
    "chinese_summary": "ç°æœ‰å›¾è¡¨ç†è§£åŸºå‡†å› å¤šæ ·æ€§çª„ã€ä»»åŠ¡å­¤ç«‹ã€è¯„ä¼°æ¡†æ¶ä¸å®Œæ•´éš¾ä»¥å…¨é¢è¯„ä¼°å›¾è¡¨å®šä½èƒ½åŠ›ï¼Œè®ºæ–‡æå‡ºChartAnchoråŸºå‡†ï¼ŒåŒ…å«8k+è¦†ç›–30ç§å›¾è¡¨ç±»å‹çš„å›¾è¡¨-è¡¨æ ¼-ä»£ç ä¸‰å…ƒç»„ï¼›å¼•å…¥å›¾è¡¨è½¬ä»£ç ç”Ÿæˆä¸å—æ§å›¾è¡¨è½¬è¡¨æ ¼é‡å»ºä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œç»“åˆå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œå…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å›¾è¡¨å®šä½èƒ½åŠ›ã€‚",
    "tags": [
      "LLM",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºåŒ…å«8k+è¦†ç›–30ç§å›¾è¡¨ç±»å‹çš„å›¾è¡¨-è¡¨æ ¼-ä»£ç ä¸‰å…ƒç»„çš„ChartAnchoråŸºå‡†ï¼Œæ¥æºæ¶µç›–å¤šæ ·çœŸå®ä¸å¢å¼ºåœºæ™¯",
      "è®¾è®¡å›¾è¡¨è½¬ä»£ç ç”Ÿæˆã€å—æ§å›¾è¡¨è½¬è¡¨æ ¼é‡å»ºä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œæ„å»ºå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œå…¨é¢è¯„ä¼°å›¾è¡¨å®šä½çš„ç»“æ„ä¸è¯­ä¹‰ä¿çœŸåº¦"
    ],
    "processed_at": "2025-12-02T08:58:00.921923"
  },
  {
    "id": "2512.00939v1",
    "title": "Constant-Time Motion Planning with Manipulation Behaviors",
    "abstract": "Recent progress in contact-rich robotic manipulation has been striking, yet most deployed systems remain confined to simple, scripted routines. One of the key barriers is the lack of motion planning algorithms that can provide verifiable guarantees for safety, efficiency and reliability. To address this, a family of algorithms called Constant-Time Motion Planning (CTMP) was introduced, which leverages a preprocessing phase to enable collision-free motion queries in a fixed, user-specified time budget (e.g., 10 milliseconds). However, existing CTMP methods do not explicitly incorporate the manipulation behaviors essential for object handling. To bridge this gap, we introduce the \\textit{Behavioral Constant-Time Motion Planner} (B-CTMP), an algorithm that extends CTMP to solve a broad class of two-step manipulation tasks: (1) a collision-free motion to a behavior initiation state, followed by (2) execution of a manipulation behavior (such as grasping or insertion) to reach the goal. By precomputing compact data structures, B-CTMP guarantees constant-time query in mere milliseconds while ensuring completeness and successful task execution over a specified set of states. We evaluate B-CTMP on two canonical manipulation tasks in simulation, shelf picking and plug insertion,and demonstrate its effectiveness on a real robot. Our results show that B-CTMP unifies collision-free planning and object manipulation within a single constant-time framework, providing provable guarantees of speed and success for manipulation in semi-structured environments.",
    "authors": [
      "Nayesha Gandotra",
      "Itamar Mishani",
      "Maxim Likhachev"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00939v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00939v1",
    "fetched_at": "2025-12-02T08:35:22.485064",
    "chinese_title": "å¸¦æ“çºµè¡Œä¸ºçš„å¸¸æ•°æ—¶é—´è¿åŠ¨è§„åˆ’",
    "chinese_summary": "é’ˆå¯¹ç°æœ‰å¸¸æ•°æ—¶é—´è¿åŠ¨è§„åˆ’ï¼ˆCTMPï¼‰æœªæ˜¾å¼èå…¥å¯¹è±¡æ“çºµè¡Œä¸ºçš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºè¡Œä¸ºå¸¸æ•°æ—¶é—´è¿åŠ¨è§„åˆ’å™¨ï¼ˆB-CTMPï¼‰ï¼Œé€šè¿‡é¢„è®¡ç®—ç´§å‡‘æ•°æ®ç»“æ„ï¼Œåœ¨æ¯«ç§’çº§å¸¸æ•°æ—¶é—´å†…ç»Ÿä¸€è§£å†³æ— ç¢°æ’è¿åŠ¨åˆ°è¡Œä¸ºå¯åŠ¨çŠ¶æ€ã€æ‰§è¡Œæ“çºµè¡Œä¸ºçš„ä¸¤æ­¥ä»»åŠ¡ï¼Œä¿è¯ä»»åŠ¡å®Œå¤‡æ€§ä¸æ‰§è¡ŒæˆåŠŸï¼Œç»ä»¿çœŸå’Œå®æœºéªŒè¯æœ‰æ•ˆã€‚",
    "tags": [
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºB-CTMPç®—æ³•ï¼Œé¦–æ¬¡å°†æ“çºµè¡Œä¸ºæ˜¾å¼èå…¥å¸¸æ•°æ—¶é—´è¿åŠ¨è§„åˆ’æ¡†æ¶ï¼Œæ”¯æŒä¸¤æ­¥æ“çºµä»»åŠ¡",
      "é¢„è®¡ç®—ç´§å‡‘æ•°æ®ç»“æ„ï¼Œä¿è¯æ¯«ç§’çº§å¸¸æ•°æ—¶é—´æŸ¥è¯¢ï¼ŒåŒæ—¶ç¡®ä¿ä»»åŠ¡å®Œå¤‡æ€§ä¸æ‰§è¡ŒæˆåŠŸ"
    ],
    "processed_at": "2025-12-02T08:58:14.145252"
  },
  {
    "id": "2512.00553v1",
    "title": "List Replicable Reinforcement Learning",
    "abstract": "Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \\emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \\emph{small list} of policies across different runs, with high probability. The size of this list defines the \\emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \\emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.",
    "authors": [
      "Bohan Zhang",
      "Michael Chen",
      "A. Pavan",
      "N. V. Vinodchandran",
      "Lin F. Yang",
      "Ruosong Wang"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00553v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00553v1",
    "fetched_at": "2025-12-02T08:35:29.466953",
    "chinese_title": "åˆ—è¡¨å¯å¤åˆ¶å¼ºåŒ–å­¦ä¹ ",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ä¸ç¨³å®šã€éš¾ä»¥å¤åˆ¶çš„é—®é¢˜ï¼Œåœ¨PAC RLæ¡†æ¶ä¸‹ç ”ç©¶åˆ—è¡¨å¯å¤åˆ¶æ€§ï¼ˆå«å¼±/å¼ºå½¢å¼ï¼‰ï¼›æå‡ºå¯è¯æ˜é«˜æ•ˆçš„è¡¨æ ¼RLç®—æ³•ï¼Œä¿è¯åˆ—è¡¨å¤æ‚åº¦å¤šé¡¹å¼äºçŠ¶æ€ã€åŠ¨ä½œåŠ horizoné•¿åº¦ï¼Œè¿˜æ‰©å±•å®ç°å¼ºåˆ—è¡¨å¯å¤åˆ¶æ€§ï¼Œå…³é”®åˆ›æ–°åŒ…æ‹¬åŸºäºå­—å…¸åºçš„è¿‘ä¼˜åŠ¨ä½œé€‰æ‹©ç­–ç•¥ç­‰ã€‚",
    "tags": [
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "æå‡ºåˆ—è¡¨å¯å¤åˆ¶æ€§çš„å¼±/å¼ºå½¢å¼å®šä¹‰ï¼Œåˆ†æç°æœ‰RLç®—æ³•çš„æŒ‡æ•°çº§åˆ—è¡¨å¤æ‚åº¦é—®é¢˜ï¼›è®¾è®¡å¯è¯æ˜é«˜æ•ˆçš„è¡¨æ ¼RLç®—æ³•ï¼Œä¿è¯åˆ—è¡¨å¤æ‚åº¦å¤šé¡¹å¼äºçŠ¶æ€ã€åŠ¨ä½œåŠ horizoné•¿åº¦ï¼Œæ‰©å±•å®ç°å¼ºåˆ—è¡¨å¯å¤åˆ¶æ€§"
    ],
    "processed_at": "2025-12-02T08:58:31.323146"
  },
  {
    "id": "2512.00403v1",
    "title": "SelfAI: Building a Self-Training AI System with LLM Agents",
    "abstract": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.",
    "authors": [
      "Xiao Wu",
      "Ting-Zhu Huang",
      "Liang-Jian Deng",
      "Xiaobing Yu",
      "Yu Zhong",
      "Shangqi Deng",
      "Ufaq Khan",
      "Jianghao Wu",
      "Xiaofeng Liu",
      "Imran Razzak",
      "Xiaojun Chang",
      "Yutong Xie"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00403v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00403v1",
    "fetched_at": "2025-12-02T08:35:29.466997",
    "chinese_title": "SelfAIï¼šæ„å»ºåŸºäºLLMæ™ºèƒ½ä½“çš„è‡ªè®­ç»ƒAIç³»ç»Ÿ",
    "chinese_summary": "é’ˆå¯¹ç°æœ‰åŸºäºLLMæ™ºèƒ½ä½“çš„è‡ªä¸»ç§‘å­¦å‘ç°æ¡†æ¶å­˜åœ¨é¢†åŸŸç‹­çª„ã€å®æ—¶äº¤äº’æœ‰é™ã€æ— åŸåˆ™åœæ­¢æœºåˆ¶ç­‰é—®é¢˜ï¼Œè®ºæ–‡æå‡ºSelfAIé€šç”¨å¤šæ™ºèƒ½ä½“å¹³å°ï¼Œæ•´åˆç”¨æˆ·æ™ºèƒ½ä½“ã€å¸¦æœ€ä¼˜åœæ­¢å‡†åˆ™çš„LLMè®¤çŸ¥æ™ºèƒ½ä½“åŠå®éªŒç®¡ç†å™¨ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªæ–°è¯„ä¼°æŒ‡æ ‡ï¼›è¯¥å¹³å°åœ¨å¤šé¢†åŸŸåŸºå‡†ä¸Šè¡¨ç°ä¼˜äºç»å…¸æ–¹æ³•ï¼Œå‡å°‘å†—ä½™è¯•éªŒä¸”æ”¯æŒäººæœºæ— ç¼äº¤äº’ã€‚",
    "tags": [
      "LLM",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºSelfAIé€šç”¨å¤šæ™ºèƒ½ä½“å¹³å°ï¼Œæ•´åˆç”¨æˆ·æ™ºèƒ½ä½“ï¼ˆç¿»è¯‘ç ”ç©¶ç›®æ ‡ä¸ºå®éªŒé…ç½®ï¼‰ã€å¸¦æœ€ä¼˜åœæ­¢å‡†åˆ™çš„LLMè®¤çŸ¥æ™ºèƒ½ä½“ï¼ˆè¿­ä»£ä¼˜åŒ–è¶…å‚æ•°æœç´¢ï¼‰åŠå®éªŒç®¡ç†å™¨ï¼ˆå¹¶è¡Œå®¹é”™è®­ç»ƒ+ç»“æ„åŒ–çŸ¥è¯†åº“åé¦ˆï¼‰",
      "å¼•å…¥Scoreå’ŒAUP_Dä¸¤ä¸ªæ–°æŒ‡æ ‡é‡åŒ–å‘ç°æ•ˆç‡ä¸æœç´¢å¤šæ ·æ€§ï¼Œåœ¨å¤šé¢†åŸŸåŸºå‡†ä¸Šä¼˜äºç»å…¸æ–¹æ³•ä¸”å‡å°‘å†—ä½™è¯•éªŒã€æ”¯æŒäººæœºæ— ç¼äº¤äº’"
    ],
    "processed_at": "2025-12-02T08:58:49.710401"
  },
  {
    "id": "2511.21669v2",
    "title": "DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving",
    "abstract": "Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.",
    "authors": [
      "Fengze Yu",
      "Leshu Li",
      "Brad McDanel",
      "Sai Qian Zhang"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.21669v2",
    "arxiv_url": "https://arxiv.org/abs/2511.21669v2",
    "fetched_at": "2025-12-02T08:35:29.467301",
    "chinese_title": "DSDï¼šä¸€ç§ç”¨äºè¾¹äº‘æ•æ·å¤§æ¨¡å‹æœåŠ¡çš„åˆ†å¸ƒå¼æ¨æµ‹è§£ç æ–¹æ¡ˆ",
    "chinese_summary": "é’ˆå¯¹ç°æœ‰æ¨æµ‹è§£ç ï¼ˆSDï¼‰ä»…æ”¯æŒå•èŠ‚ç‚¹æ‰§è¡Œã€æ— æ³•é€‚é…è¾¹äº‘å¼‚æ„ç¯å¢ƒçš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºåˆ†å¸ƒå¼æ¨æµ‹è§£ç æ¡†æ¶DSDï¼Œé€šè¿‡åè°ƒdraft-targetæ¨¡å‹æ‰§è¡Œæ‰©å±•è‡³å¤šè®¾å¤‡éƒ¨ç½²ï¼›åŒæ—¶è®¾è®¡DSD-Simç¦»æ•£äº‹ä»¶æ¨¡æ‹Ÿå™¨æ•æ‰ç½‘ç»œç­‰åŠ¨æ€ï¼Œå¹¶æå‡ºè‡ªé€‚åº”çª—å£æ§åˆ¶ï¼ˆAWCï¼‰ç­–ç•¥ä¼˜åŒ–ååé‡ï¼Œå®éªŒæ˜¾ç¤ºDSDè¾ƒç°æœ‰SDåŸºçº¿æœ€å¤šæå‡1.1å€é€Ÿåº¦å’Œ9.7%ååé‡ã€‚",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºåˆ†å¸ƒå¼æ¨æµ‹è§£ç æ¡†æ¶DSDï¼Œå°†SDæ‰©å±•è‡³è¾¹äº‘å¤šè®¾å¤‡å¼‚æ„ç¯å¢ƒçš„LLMæœåŠ¡",
      "è®¾è®¡DSD-Simæ¨¡æ‹Ÿå™¨ä¸AWCç­–ç•¥ï¼Œä¼˜åŒ–LLMæ¨ç†çš„å»¶è¿Ÿå’Œååé‡"
    ],
    "processed_at": "2025-12-02T08:59:02.787622"
  },
  {
    "id": "2512.00396v1",
    "title": "Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement",
    "abstract": "We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.",
    "authors": [
      "Andrea Procopio",
      "Marco Esposito",
      "Sara Raggiunto",
      "Andrey Gizdov",
      "Alberto Belli",
      "Paola Pierleoni"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00396v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00396v1",
    "fetched_at": "2025-12-02T08:35:46.172061",
    "chinese_title": "è¾¹ç¼˜æ—¶é—´åºåˆ—ï¼šç”¨äºå¯ç©¿æˆ´æ­¥æ€æ£€æµ‹å’Œæœ€ä¼˜ä¼ æ„Ÿå™¨æ”¾ç½®çš„å¾®å‹å¯åˆ†ç¦»CNN",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ­¥æ€æ£€æµ‹ï¼Œåœ¨èµ„æºå—é™çš„å¯ç©¿æˆ´è®¾å¤‡ä¸Šå¯¹æ¯”é˜ˆå€¼æ³•ä¸ä¸‰ç§1D CNNï¼Œæå‡ºçš„è¶…è½»é‡å¯åˆ†ç¦»CNNï¼ˆå¸¦æ®‹å·®çš„Model2å‚æ•°ä»…533ï¼Œæ¯”åŸºçº¿å°‘10å€ä½†æ€§èƒ½ç›¸å½“ï¼‰è¡¨ç°ä¼˜å¼‚ï¼›åŒæ—¶åˆ†æä¼ æ„Ÿå™¨ä½ç½®å‘ç°èƒ¸å’Œå¤§è…¿æœ€å¯é ï¼Œæ¨¡å‹å¯åœ¨STM32 MCUä¸Šä½å»¶è¿Ÿè¿è¡Œï¼Œå®ç°è¾¹ç¼˜ç«¯é«˜æ•ˆæ­¥æ€æ£€æµ‹ã€‚",
    "tags": [
      "Time Series",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºè¶…è½»é‡å¯åˆ†ç¦»CNNï¼ˆå¸¦æ®‹å·®çš„Model2å‚æ•°ä»…533ï¼‰ï¼Œåœ¨PDæ­¥æ€æ£€æµ‹ä¸­æ€§èƒ½åŒ¹é…æˆ–è¶…è¶ŠåŸºçº¿ï¼ˆå‚æ•°å°‘10å€ï¼‰ï¼Œä¸”å¯åœ¨STM32ç±»MCUä¸Šäºš10mså»¶è¿Ÿè¿è¡Œ",
      "åˆ†æä¼ æ„Ÿå™¨ä½ç½®å¾—å‡ºèƒ¸å’Œå¤§è…¿ä¸ºæœ€ä¼˜æ”¾ç½®éƒ¨ä½ï¼Œå¤šéƒ¨ä½èåˆæœªä¼˜äºæœ€ä¼˜å•éƒ¨ä½ï¼Œä¸ºå¯ç©¿æˆ´æ­¥æ€æ£€æµ‹çš„ä¼ æ„Ÿå™¨éƒ¨ç½²æä¾›å‚è€ƒ"
    ],
    "processed_at": "2025-12-02T08:59:16.034013"
  },
  {
    "id": "2512.01439v1",
    "title": "Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech",
    "abstract": "India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\\%). This work contributes to bridging the language gap in digital financial services for emerging markets.",
    "authors": [
      "Bharatdeep Hazarika",
      "Arya Suneesh",
      "Prasanna Devadiga",
      "Pawan Kumar Rajpoot",
      "Anshuman B Suresh",
      "Ahmed Ifthaquar Hussain"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01439v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01439v1",
    "fetched_at": "2025-12-02T08:36:02.937812",
    "chinese_title": "é‡‘èæ´åŠ©å¤šè¯­è¨€å¯¹è¯AIï¼šå¼¥åˆå°åº¦é‡‘èç§‘æŠ€çš„è¯­è¨€éšœç¢",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹å°åº¦è¯­è¨€å¤šæ ·æ€§å¯¼è‡´çš„é‡‘èç§‘æŠ€è¯­è¨€éšœç¢é—®é¢˜ï¼Œæå‡ºæ”¯æŒå°åœ°è‹±è¯­ç­‰æ··åˆè¯­è¨€çš„å¤šæ™ºèƒ½ä½“æ¶æ„å¯¹è¯AIç³»ç»Ÿï¼ŒåŒ…å«è¯­è¨€åˆ†ç±»ã€åŠŸèƒ½ç®¡ç†åŠå¤šè¯­è¨€å“åº”ç”Ÿæˆæ¨¡å—ï¼›é€šè¿‡å¤šè¯­è¨€æ¨¡å‹å¯¹æ¯”ä¸çœŸå®éƒ¨ç½²éªŒè¯ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æå‡ç”¨æˆ·å‚ä¸åº¦ä¸”ä»…å¢åŠ 4-8%çš„ä½å»¶è¿Ÿå¼€é”€ï¼Œä¸ºæ–°å…´å¸‚åœºæ•°å­—é‡‘èæœåŠ¡çš„è¯­è¨€é¸¿æ²Ÿå¼¥åˆæä¾›æœ‰æ•ˆæ–¹æ¡ˆã€‚",
    "tags": [
      "NLP",
      "Financial Agent",
      "LLM"
    ],
    "key_contributions": [
      "æå‡ºé€‚é…å°åº¦è¯­è¨€å¤šæ ·æ€§çš„å¤šæ™ºèƒ½ä½“æ¶æ„é‡‘èå¯¹è¯AIç³»ç»Ÿï¼Œæ”¯æŒå°åœ°è‹±è¯­ç­‰æ··åˆè¯­è¨€äº¤äº’",
      "ç»çœŸå®éƒ¨ç½²éªŒè¯ï¼Œç³»ç»Ÿæ˜¾è‘—æå‡ç”¨æˆ·å‚ä¸åº¦ä¸”ä»…å¢åŠ 4-8%ä½å»¶è¿Ÿå¼€é”€ï¼ŒåŠ©åŠ›æ–°å…´å¸‚åœºæ•°å­—é‡‘èæœåŠ¡è¯­è¨€é¸¿æ²Ÿå¼¥åˆ"
    ],
    "processed_at": "2025-12-02T08:59:32.801378"
  },
  {
    "id": "2512.00332v1",
    "title": "Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents",
    "abstract": "Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.",
    "authors": [
      "Daud Waqas",
      "Aaryamaan Golthi",
      "Erika Hayashida",
      "Huanzhi Mao"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00332v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00332v1",
    "fetched_at": "2025-12-02T08:36:02.937848",
    "chinese_title": "æ–­è¨€æ¡ä»¶åˆè§„ï¼šå¤šè½®å·¥å…·è°ƒç”¨Agentä¸­çš„æº¯æºæ„ŸçŸ¥æ¼æ´",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹å¤šè½®å·¥å…·è°ƒç”¨å¤§æ¨¡å‹çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºæ–­è¨€æ¡ä»¶åˆè§„ï¼ˆA-CCï¼‰è¯„ä¼°èŒƒå¼ï¼Œä»ç”¨æˆ·æºæ–­è¨€ï¼ˆUSAï¼‰å’Œå·¥å…·æºæ–­è¨€ï¼ˆFSAï¼‰ä¸¤ä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹åº”å¯¹è¯¯å¯¼æ€§æ–­è¨€çš„è¡Œä¸ºï¼›å®éªŒè¡¨æ˜æ¨¡å‹å¯¹è¿™ä¸¤ç±»è¯¯å¯¼æ€§æ–­è¨€å‡é«˜åº¦è„†å¼±ï¼Œè¯å®A-CCæ˜¯éƒ¨ç½²Agentçš„å…³é”®æ½œåœ¨æ¼æ´ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Financial Agent",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºæ–­è¨€æ¡ä»¶åˆè§„ï¼ˆA-CCï¼‰è¯„ä¼°èŒƒå¼ï¼Œä»ç”¨æˆ·æºæ–­è¨€å’Œå·¥å…·æºæ–­è¨€ä¸¤ä¸ªç»´åº¦è¯„ä¼°å¤šè½®å·¥å…·è°ƒç”¨å¯¹è¯çš„æ¨¡å‹è¡Œä¸º",
      "é€šè¿‡å®éªŒæ­ç¤ºå¤šè½®å·¥å…·è°ƒç”¨Agentå¯¹è¯¯å¯¼æ€§æ–­è¨€çš„é«˜åº¦è„†å¼±æ€§ï¼Œè¯å®A-CCæ˜¯éƒ¨ç½²Agentçš„å…³é”®æ½œåœ¨æ¼æ´"
    ],
    "processed_at": "2025-12-02T08:59:45.942340"
  },
  {
    "id": "2512.01149v1",
    "title": "A Benchmark of Causal vs Correlation AI for Predictive Maintenance",
    "abstract": "Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.",
    "authors": [
      "Krishna Taduri",
      "Shaunak Dhande",
      "Giacinto Paolo",
      " Saggese",
      "Paul Smith"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01149v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01149v1",
    "fetched_at": "2025-12-02T08:36:16.367802",
    "chinese_title": "å› æœAIä¸ç›¸å…³AIåœ¨é¢„æµ‹æ€§ç»´æŠ¤ä¸­çš„åŸºå‡†æ¯”è¾ƒ",
    "chinese_summary": "è®ºæ–‡é’ˆå¯¹åˆ¶é€ ä¸šé¢„æµ‹æ€§ç»´æŠ¤ä¸­æ¼æ£€æˆæœ¬ä¸ºè¯¯æŠ¥50å€çš„æˆæœ¬ä¸å¯¹ç§°é—®é¢˜ï¼Œè¯„ä¼°äº†8ç§æ¨¡å‹ï¼ˆå«åŸºçº¿ç»Ÿè®¡ä¸å½¢å¼å› æœæ¨ç†æ–¹æ³•ï¼‰åœ¨1ä¸‡å°CNCæœºåºŠæ•°æ®ä¸Šçš„è¡¨ç°ï¼›å…¶ä¸­å½¢å¼å› æœæ¨ç†æ¨¡å‹å¹´æˆæœ¬èŠ‚çœ116ä¸‡ç¾å…ƒï¼ˆé™70.2%ï¼‰ï¼Œä¼˜äºæœ€ä½³ç›¸å…³æ¨¡å‹çº¦8ä¸‡ç¾å…ƒï¼Œä¸”è¯¯æŠ¥å‡å°‘97%ã€å¯è§£é‡Šæ€§æ›´å¥½ã€‚",
    "tags": [
      "Benchmark",
      "Anomaly"
    ],
    "key_contributions": [
      "è¯æ˜å½¢å¼å› æœæ¨ç†æ¨¡å‹å¯æ˜¾è‘—é™ä½è¿è¥æˆæœ¬ï¼ŒåŒæ—¶æå‡æ•…éšœæ£€æµ‹å¬å›ç‡å¹¶å¤§å¹…å‡å°‘è¯¯æŠ¥ï¼Œä¸”å¯è§£é‡Šæ€§æ›´ä¼˜"
    ],
    "processed_at": "2025-12-02T09:00:00.062126"
  },
  {
    "id": "2512.00946v1",
    "title": "Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data",
    "abstract": "Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.",
    "authors": [
      "Alvaro Paredes Amorin",
      "Andre Python",
      "Christoph Weisser"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00946v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00946v1",
    "fetched_at": "2025-12-02T08:36:16.367868",
    "chinese_title": "è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹åœ¨å¼‚æ„é‡‘èæ–‡æœ¬æ•°æ®æƒ…æ„Ÿåˆ†ç±»ä¸­çš„å¾®è°ƒç ”ç©¶",
    "chinese_summary": "è¯¥è®ºæ–‡å¯¹æ¯”äº†è½»é‡çº§å¼€æºLLMsï¼ˆDeepSeek-LLM 7Bã€Llama3 8B Instructã€Qwen3 8Bï¼‰ä¸ä¼ ç»Ÿé‡‘èNLPæ¨¡å‹FinBERTåœ¨äº”ä¸ªå¼‚æ„é‡‘èæ–‡æœ¬æ•°æ®é›†ä¸Šçš„æƒ…æ„Ÿåˆ†ç±»æ€§èƒ½ï¼Œå‘ç°Qwen3 8Bå’ŒLlama3 8Bå³ä½¿ä»…ç”¨5%è®­ç»ƒæ•°æ®æˆ–é›¶/å°‘æ ·æœ¬ä¹Ÿèƒ½å–å¾—ç«äº‰åŠ›è¡¨ç°ï¼Œè¯æ˜è½»é‡çº§å¼€æºLLMsæ˜¯ç»æµé«˜æ•ˆçš„é‡‘èæƒ…æ„Ÿåˆ†æé€‰æ‹©ã€‚",
    "tags": [
      "LLM",
      "Sentiment Analysis",
      "NLP",
      "Transformer"
    ],
    "key_contributions": [
      "ç³»ç»Ÿå¯¹æ¯”è½»é‡çº§å¼€æºLLMsä¸ä¼ ç»Ÿé‡‘èNLPæ¨¡å‹åœ¨å¤šå¼‚æ„é‡‘èæ–‡æœ¬æ•°æ®é›†ä¸Šçš„æƒ…æ„Ÿåˆ†ç±»æ€§èƒ½",
      "éªŒè¯è½»é‡çº§å¼€æºLLMsä»…éœ€å°‘é‡è®­ç»ƒæ•°æ®æˆ–é›¶/å°‘æ ·æœ¬å³å¯å®ç°ç«äº‰åŠ›è¡¨ç°ï¼Œä¸ºèµ„æºæœ‰é™çš„ç ”ç©¶è€…æä¾›ç»æµé«˜æ•ˆæ–¹æ¡ˆ"
    ],
    "processed_at": "2025-12-02T09:00:10.546891"
  },
  {
    "id": "2512.00586v1",
    "title": "Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&D",
    "abstract": "This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.",
    "authors": [
      "Michael R. Doane"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-bio.QM"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00586v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00586v1",
    "fetched_at": "2025-12-02T08:36:16.367913",
    "chinese_title": "ç»Ÿè®¡è‡ªç„¶è¯­è¨€å¤„ç†ä¼˜åŒ–åˆ¶è¯ç ”å‘ä¸­ä¸´åºŠè¯•éªŒæˆåŠŸé¢„æµ‹",
    "chinese_summary": "è¯¥è®ºæ–‡é’ˆå¯¹ç¥ç»ç§‘å­¦é¢†åŸŸä¸´åºŠè¯•éªŒé«˜å¤±è´¥ç‡é—®é¢˜ï¼Œåˆ©ç”¨ç»Ÿè®¡NLPæå–ä¸´åºŠè¯•éªŒæ–‡æœ¬ç‰¹å¾ï¼Œå¯¹æ¯”éLLMæ¡†æ¶ï¼ˆé€»è¾‘å›å½’ç­‰ï¼‰ä¸BioBERTæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ï¼›BioBERTæ¨¡å‹ROC-AUCè¾¾0.74ï¼Œé¢„æµ‹è¯¯å·®æ¯”è¡Œä¸šåŸºå‡†ä½40%ï¼Œèƒ½è¾…åŠ©åˆ¶è¯ç ”å‘ä¼˜åŒ–èµ„æºåˆ†é…ã€‚",
    "tags": [
      "NLP",
      "LLM",
      "Transformer",
      "Benchmark"
    ],
    "key_contributions": [
      "BioBERTæ¨¡å‹ä¸´åºŠè¯•éªŒæˆåŠŸé¢„æµ‹ç²¾åº¦ä¼˜äºè¡Œä¸šåŸºå‡†ï¼Œå¯åŠ©åŠ›åˆ¶è¯ç ”å‘é™ä½é£é™©"
    ],
    "processed_at": "2025-12-02T09:00:20.246089"
  },
  {
    "id": "2511.20944v2",
    "title": "Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection",
    "abstract": "Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual losses, presenting a massive economic asymmetry: the financial cost of a False Negative (fraud loss) exceeds the operational cost of a False Positive (manual review) by a ratio of approximately 5,480:1. This paper contrasts two detection paradigms: a Forensic Psycholinguistic Stream (CatBoost), which analyzes linguistic cues like urgency and authority with high interpretability, and a Semantic Stream (DistilBERT), which utilizes deep learning for contextual understanding. We evaluated both streams on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud. Benchmarked on Tesla T4 infrastructure, DistilBERT achieved near-perfect detection on synthetic threats (AUC >0.99, F1 =0.998) with acceptable real-time latency (7.4 ms). CatBoost achieved competitive detection (AUC =0.991, F1 =0.949) at 8.4x lower latency (0.8 ms) with negligible resource consumption. We conclude that while DistilBERT offers maximum accuracy for GPU-equipped organizations, CatBoost provides a viable, cost-effective alternative for edge deployments. Both approaches demonstrate a theoretical ROI exceeding 99.9% when optimized via cost-sensitive learning.",
    "authors": [
      "Yaw Osei Adjei",
      "Frederick Ayivor"
    ],
    "published": "2025-11-26",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2511.20944v2",
    "arxiv_url": "https://arxiv.org/abs/2511.20944v2",
    "fetched_at": "2025-12-02T08:36:16.368049",
    "chinese_title": "è¯­ä¹‰ä¼˜åŠ¿ä¸å–è¯æ•ˆç‡ï¼šæ·±åº¦å­¦ä¹ ä¸å¿ƒç†è¯­è¨€å­¦åœ¨å•†åŠ¡é‚®ä»¶æ³„éœ²æ£€æµ‹ä¸­çš„æ¯”è¾ƒåˆ†æ",
    "chinese_summary": "è®ºæ–‡é’ˆå¯¹é€ æˆå·¨é¢ç»æµæŸå¤±çš„å•†åŠ¡é‚®ä»¶æ³„éœ²ï¼ˆBECï¼‰å¨èƒï¼Œå¯¹æ¯”äº†åŸºäºå¿ƒç†è¯­è¨€å­¦çš„CatBoostï¼ˆé«˜å¯è§£é‡Šã€ä½å»¶è¿Ÿï¼‰ä¸åŸºäºæ·±åº¦å­¦ä¹ çš„DistilBERTï¼ˆé«˜å‡†ç¡®ç‡ï¼‰ä¸¤ç§æ£€æµ‹èŒƒå¼ï¼Œåœ¨æ··åˆæ•°æ®é›†ä¸ŠéªŒè¯äº†ä¸¤è€…æ€§èƒ½å·®å¼‚åŠæˆæœ¬æ•ˆç›Šï¼Œä¸ºä¸åŒéƒ¨ç½²åœºæ™¯æä¾›å¯è¡Œé€‰æ‹©ã€‚",
    "tags": [
      "NLP",
      "Deep Learning",
      "Transformer",
      "Risk Management"
    ],
    "key_contributions": [
      "å¯¹æ¯”åˆ†æå¿ƒç†è¯­è¨€å­¦ä¸æ·±åº¦å­¦ä¹ åœ¨BECæ£€æµ‹ä¸­çš„æ€§èƒ½ã€æ•ˆç‡åŠæˆæœ¬æ•ˆç›Š",
      "éªŒè¯ä¸¤ç§æ–¹æ³•åœ¨å«äººç±»åˆæ³•ä¸AIåˆæˆæ¬ºè¯ˆé‚®ä»¶çš„æ··åˆæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œé€‚é…ä¸åŒéƒ¨ç½²åœºæ™¯"
    ],
    "processed_at": "2025-12-02T09:00:26.794095"
  },
  {
    "id": "2512.01412v1",
    "title": "A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns",
    "abstract": "Explainability is essential for neural networks that model long time series, yet most existing explainable AI methods only produce point-wise importance scores and fail to capture temporal structures such as trends, cycles, and regime changes. This limitation weakens human interpretability and trust in long-horizon models. To address these issues, we identify four key requirements for interpretable time-series modeling: temporal continuity, pattern-centric explanation, causal disentanglement, and faithfulness to the model's inference process. We propose EXCAP, a unified framework that satisfies all four requirements. EXCAP combines an attention-based segmenter that extracts coherent temporal patterns, a causally structured decoder guided by a pre-trained causal graph, and a latent aggregation mechanism that enforces representation stability. Our theoretical analysis shows that EXCAP provides smooth and stable explanations over time and is robust to perturbations in causal masks. Extensive experiments on classification and forecasting benchmarks demonstrate that EXCAP achieves strong predictive accuracy while generating coherent and causally grounded explanations. These results show that EXCAP offers a principled and scalable approach to interpretable modeling of long time series with relevance to high-stakes domains such as healthcare and finance.",
    "authors": [
      "Ziqian Wang",
      "Yuxiao Cheng",
      "Jinli Suo"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01412v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01412v1",
    "fetched_at": "2025-12-02T08:36:29.715107",
    "chinese_title": "é€šè¿‡æå–ä¿¡æ¯æ€§ç»“æ„åŒ–å› æœæ¨¡å¼å®ç°é•¿æ—¶åºè‡ªè§£é‡Šæ¨¡å‹",
    "chinese_summary": "é’ˆå¯¹é•¿æ—¶åºç¥ç»ç½‘ç»œå¯è§£é‡Šæ€§ä¸è¶³ï¼ˆç°æœ‰æ–¹æ³•ä»…è¾“å‡ºç‚¹å¼é‡è¦æ€§ï¼Œæ— æ³•æ•æ‰è¶‹åŠ¿ã€å‘¨æœŸç­‰æ—¶åºç»“æ„ï¼‰çš„é—®é¢˜ï¼Œæå‡ºEXCAPæ¡†æ¶ï¼Œç»“åˆæ³¨æ„åŠ›åˆ†æ®µå™¨ã€é¢„è®­ç»ƒå› æœå›¾å¼•å¯¼çš„è§£ç å™¨åŠæ½œåœ¨èšåˆæœºåˆ¶ï¼Œæ»¡è¶³æ—¶åºè¿ç»­æ€§ã€æ¨¡å¼ä¸­å¿ƒè§£é‡Šç­‰å››å¤§å¯è§£é‡Šè¦æ±‚ï¼›å®éªŒè¡¨æ˜å…¶é¢„æµ‹å‡†ç¡®ä¸”è§£é‡Šè¿è´¯å› æœåŒ–ï¼Œé€‚ç”¨äºé‡‘èç­‰é«˜é£é™©é¢†åŸŸã€‚",
    "tags": [
      "Time Series",
      "Deep Learning",
      "Transformer",
      "Graph Neural Network"
    ],
    "key_contributions": [
      "æå‡ºEXCAPè‡ªè§£é‡Šæ¡†æ¶ï¼Œæ»¡è¶³æ—¶åºè¿ç»­æ€§ã€æ¨¡å¼ä¸­å¿ƒè§£é‡Šç­‰å››å¤§å¯è§£é‡Šè¦æ±‚ï¼Œå¼¥è¡¥ç°æœ‰æ–¹æ³•æ— æ³•æ•æ‰æ—¶åºç»“æ„çš„ä¸è¶³",
      "å®éªŒè¯æ˜EXCAPé¢„æµ‹æ€§èƒ½ä¼˜å¼‚ä¸”ç”Ÿæˆè¿è´¯å› æœåŒ–è§£é‡Šï¼Œé€‚ç”¨äºé‡‘èç­‰é«˜é£é™©é¢†åŸŸ"
    ],
    "processed_at": "2025-12-02T09:00:34.839045"
  },
  {
    "id": "2512.01289v1",
    "title": "OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction",
    "abstract": "Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.",
    "authors": [
      "Mingqin Yu",
      "Fethi Rabhi",
      "Boming Xia",
      "Zhengyi Yang",
      "Felix Tan",
      "Qinghua Lu"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.AI",
      "cs.GR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01289v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01289v1",
    "fetched_at": "2025-12-02T08:36:29.715173",
    "chinese_title": "OntoMetricï¼šé¢å‘è‡ªåŠ¨åŒ–ESGçŸ¥è¯†å›¾è°±æ„å»ºçš„æœ¬ä½“å¼•å¯¼æ¡†æ¶",
    "chinese_summary": "é’ˆå¯¹ESGæŠ«éœ²æ–‡æ¡£éç»“æ„åŒ–å¯¼è‡´çš„æå–å›°éš¾é—®é¢˜ï¼Œè®ºæ–‡æå‡ºOntoMetricæœ¬ä½“å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥åˆ†å‰²ã€æœ¬ä½“çº¦æŸLLMæå–åŠä¸¤é˜¶æ®µéªŒè¯æ„å»ºESGçŸ¥è¯†å›¾è°±ï¼›è¯¥æ¡†æ¶è§£å†³äº†æ— çº¦æŸLLMæå–çš„ä¸€è‡´æ€§å·®ã€å¹»è§‰ç­‰é—®é¢˜ï¼Œè¯„ä¼°æ˜¾ç¤ºè¾ƒé«˜çš„è¯­ä¹‰å‡†ç¡®ç‡å’Œæ¨¡å¼åˆè§„æ€§ï¼Œä¸”ä¿ç•™æº¯æºä¿¡æ¯ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºOntoMetricæœ¬ä½“å¼•å¯¼æ¡†æ¶ï¼Œå®ç°ESGéç»“æ„åŒ–æ–‡æ¡£åˆ°å¯éªŒè¯çŸ¥è¯†å›¾è°±çš„è‡ªåŠ¨åŒ–è½¬æ¢",
      "è®¾è®¡ä¸‰é˜¶æ®µå¤„ç† pipeline åŠä¸¤é˜¶æ®µéªŒè¯æœºåˆ¶ï¼Œæ˜¾è‘—æå‡æå–çš„è¯­ä¹‰å‡†ç¡®ç‡ä¸æ¨¡å¼åˆè§„æ€§ï¼Œä¿ç•™æº¯æºä¿¡æ¯",
      "åœ¨å¤šESGæ ‡å‡†ï¼ˆSASBã€TCFDã€IFRS S2ç­‰ï¼‰ä¸ŠéªŒè¯æœ‰æ•ˆï¼Œæ€§èƒ½è¿œä¼˜äºæ— çº¦æŸLLMæå–åŸºçº¿"
    ],
    "processed_at": "2025-12-02T09:00:59.515292"
  },
  {
    "id": "2512.00417v1",
    "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency",
    "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.   Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.",
    "authors": [
      "Jiacheng Guo",
      "Suozhi Huang",
      "Zixin Yao",
      "Yifan Zhang",
      "Yifu Lu",
      "Jiashuo Liu",
      "Zihao Li",
      "Yanyan Deng",
      "Qixin Xiao",
      "Jia Tian",
      "Kanghong Zhan",
      "Tianyi Li",
      "Xiaochen Liu",
      "Jason Ge",
      "Chaoyang He",
      "Kaixuan Huang",
      "Lin Yang",
      "Wenhao Huang",
      "Mengdi Wang"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00417v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00417v1",
    "fetched_at": "2025-12-02T08:36:29.715267",
    "chinese_title": "CryptoBenchï¼šåŠ å¯†è´§å¸é¢†åŸŸLLMæ™ºèƒ½ä½“ä¸“å®¶çº§è¯„ä¼°çš„åŠ¨æ€åŸºå‡†",
    "chinese_summary": "æœ¬æ–‡å¼•å…¥é¦–ä¸ªä¸“å®¶è®¾è®¡çš„åŠ¨æ€åŸºå‡†CryptoBenchï¼Œé’ˆå¯¹åŠ å¯†è´§å¸é¢†åŸŸLLMæ™ºèƒ½ä½“çš„çœŸå®èƒ½åŠ›è¯„ä¼°ï¼Œè§£å†³è¯¥é¢†åŸŸæ—¶é—´æ•æ„Ÿã€ä¿¡æ¯å¯¹æŠ—æ€§å¼ºã€å¤šæºæ•°æ®åˆæˆç­‰ç‰¹æœ‰æŒ‘æˆ˜ï¼›åŸºå‡†æ¯æœˆåŒ…å«50é“ç”±åŠ å¯†ä¸“ä¸šäººå£«è®¾è®¡çš„ä»»åŠ¡ï¼ŒæŒ‰ç®€å•/å¤æ‚æ£€ç´¢ã€ç®€å•/å¤æ‚é¢„æµ‹å››è±¡é™åˆ†ç±»ï¼Œå¯ç²¾å‡†è¯„ä¼°LLMçš„æ•°æ®è·å–ä¸é«˜çº§åˆ†æé¢„æµ‹èƒ½åŠ›ï¼›å¯¹10ä¸ªLLMçš„è¯„ä¼°æ­ç¤ºäº†å…¶å­˜åœ¨æ£€ç´¢-é¢„æµ‹å¤±è¡¡çš„å¤±æ•ˆæ¨¡å¼ã€‚",
    "tags": [
      "LLM",
      "Financial Agent",
      "Benchmark"
    ],
    "key_contributions": [
      "æ„å»ºåŠ å¯†è´§å¸é¢†åŸŸé¦–ä¸ªä¸“å®¶çº§åŠ¨æ€åŸºå‡†CryptoBenchï¼Œé€‚é…è¯¥é¢†åŸŸæ—¶é—´æ•æ„Ÿã€ä¿¡æ¯å¯¹æŠ—æ€§å¼ºç­‰ç‰¹æœ‰æŒ‘æˆ˜",
      "è®¾è®¡å››è±¡é™ä»»åŠ¡åˆ†ç±»ä½“ç³»ï¼ˆç®€å•/å¤æ‚æ£€ç´¢ã€ç®€å•/å¤æ‚é¢„æµ‹ï¼‰ï¼Œå®ç°å¯¹LLMæ™ºèƒ½ä½“æ•°æ®è·å–ä¸é«˜çº§åˆ†æèƒ½åŠ›çš„ç²¾å‡†è¯„ä¼°",
      "é€šè¿‡è¯„ä¼°10ä¸ªLLMæ­ç¤ºå…¶æ£€ç´¢-é¢„æµ‹å¤±è¡¡çš„å¤±æ•ˆæ¨¡å¼"
    ],
    "processed_at": "2025-12-02T09:01:16.253269"
  },
  {
    "id": "2512.01054v1",
    "title": "Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs",
    "abstract": "Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.   We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.   We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.   Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.",
    "authors": [
      "MohammadParsa Dini",
      "Human Jafari",
      "Sajjad Amini",
      "MohammadMahdi Mojahedian"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01054v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01054v1",
    "fetched_at": "2025-12-02T08:36:33.340594",
    "chinese_title": "DDPMä¸VAEæœºå™¨é—å¿˜ä¸­çš„è‡ªé€‚åº”Î»å‡å»é‡è¦æ€§é‡‡æ ·åˆ†æ•°æ–¹æ³•",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDDPMï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„æœºå™¨é—å¿˜é—®é¢˜ï¼Œæå‡ºè‡ªé€‚åº”Î»å‡å»é‡è¦æ€§é‡‡æ ·åˆ†æ•°ï¼ˆAdaptive-lambda SISSï¼‰æ–¹æ³•ï¼Œå°†å›ºå®šÎ»æ”¹ä¸ºåŠ¨æ€æ¨æ–­çš„éšå˜é‡ï¼Œé€šè¿‡è½»é‡ç½‘ç»œåŸºäºSISSæŸå¤±é¡¹çš„ä¸Šä¸‹æ–‡ç‰¹å¾å‚æ•°åŒ–Î»åéªŒï¼Œè”åˆä¼˜åŒ–æ¨¡å‹ä¸Î»æ¨æ–­æœºåˆ¶ï¼›è¿˜æ‰©å±•è‡ªé€‚åº”Î»åˆ°åˆ†æ•°é—å¿˜ï¼Œå¼•å…¥å¤šç±»Score Forgetting DistillationåŠæ··åˆç›®æ ‡ã€å¼ºåŒ–å­¦ä¹ å…¬å¼åŒ–é—å¿˜è¿‡ç¨‹ã€‚",
    "tags": [
      "Deep Learning",
      "Reinforcement Learning"
    ],
    "key_contributions": [
      "æå‡ºAdaptive-lambda SISSï¼Œå°†Î»ä»å›ºå®šå€¼è½¬ä¸ºåŠ¨æ€æ¨æ–­çš„éšå˜é‡ï¼Œé€šè¿‡è½»é‡ç½‘ç»œåŸºäºSISSæŸå¤±ä¸Šä¸‹æ–‡ç‰¹å¾å‚æ•°åŒ–Î»åéªŒï¼Œè”åˆä¼˜åŒ–æ¨¡å‹ä¸Î»æ¨æ–­æœºåˆ¶ï¼Œä¼˜åŒ–é—å¿˜ä¸ä¿ç•™çš„æƒè¡¡",
      "æ‰©å±•è‡ªé€‚åº”Î»åˆ°åˆ†æ•°é—å¿˜ï¼Œå¼•å…¥å¤šç±»Score Forgetting Distillationï¼Œæå‡ºç»“åˆSFDä¸SISSçš„æ··åˆç›®æ ‡åŠå°†é—å¿˜è§†ä¸ºåºåˆ—å†³ç­–çš„å¼ºåŒ–å­¦ä¹ å…¬å¼åŒ–æ–¹æ³•"
    ],
    "processed_at": "2025-12-02T09:01:37.645030"
  },
  {
    "id": "2512.00499v1",
    "title": "ESPO: Entropy Importance Sampling Policy Optimization",
    "abstract": "Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.",
    "authors": [
      "Yuepeng Sheng",
      "Yuwei Huang",
      "Shuman Liu",
      "Haibo Zhang",
      "Anxiang Zeng"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00499v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00499v1",
    "fetched_at": "2025-12-02T08:36:33.340700",
    "chinese_title": "ESPOï¼šç†µé‡è¦æ€§é‡‡æ ·ç­–ç•¥ä¼˜åŒ–",
    "chinese_summary": "ç°æœ‰åŸºäºåˆ†ç»„çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚GRPOã€GSPOï¼‰å­˜åœ¨ä¼˜åŒ–ç²’åº¦ä¸è®­ç»ƒç¨³å®šæ€§çš„æƒè¡¡ï¼ŒGSPOå­˜åœ¨æ¢¯åº¦æœªå……åˆ†åˆ©ç”¨å’Œå‡åŒ€ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼›è®ºæ–‡æå‡ºESPOï¼ŒæŒ‰é¢„æµ‹ç†µåˆ†è§£åºåˆ—ï¼Œé‡‡ç”¨ç†µé©±åŠ¨é‡è¦æ€§é‡‡æ ·æ•æ‰åºåˆ—å†…å¼‚è´¨æ€§ã€ç†µè‡ªé€‚åº”è£å‰ªåŠ¨æ€åˆ†é…ä¿¡ä»»åŒºåŸŸï¼Œå®éªŒåœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸ŠåŠ é€Ÿæ”¶æ•›ä¸”è¾¾SOTAï¼ŒHMMTå‡†ç¡®ç‡ä»4.4%æå‡è‡³13.13%ã€‚",
    "tags": [
      "LLM",
      "Reinforcement Learning",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºESPOæ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹ç†µåˆ†è§£åºåˆ—å¹¶ç»“åˆç†µé©±åŠ¨é‡è¦æ€§é‡‡æ ·ä¸ç†µè‡ªé€‚åº”è£å‰ªï¼Œè§£å†³ç°æœ‰åˆ†ç»„RLæ¡†æ¶çš„æ¢¯åº¦æœªå……åˆ†åˆ©ç”¨å’Œå‡åŒ€ä¿¡ç”¨åˆ†é…é—®é¢˜",
      "å®éªŒéªŒè¯ESPOåœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸ŠåŠ é€Ÿæ”¶æ•›ä¸”è¾¾åˆ°SOTAï¼Œæ˜¾è‘—æå‡HMMTç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡çš„å‡†ç¡®ç‡"
    ],
    "processed_at": "2025-12-02T09:01:52.905041"
  },
  {
    "id": "2512.01953v1",
    "title": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference",
    "abstract": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.",
    "authors": [
      "Sai Gokhale",
      "Devleena Das",
      "Rajeev Patwari",
      "Ashish Sirasao",
      "Elliott Delaye"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01953v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01953v1",
    "fetched_at": "2025-12-02T08:36:57.166986",
    "chinese_title": "KV Paretoï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„KVç¼“å­˜ä¸æ¨¡å‹å‹ç¼©ç³»ç»Ÿçº§ä¼˜åŒ–",
    "chinese_summary": "è®ºæ–‡é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ—¶KVç¼“å­˜çš„å†…å­˜ç“¶é¢ˆï¼Œæå‡ºKV Paretoç³»ç»Ÿçº§æ¡†æ¶ï¼Œç³»ç»Ÿæ˜ å°„KVç¼“å­˜é‡åŒ–ã€åˆ†å—é¢„å¡«å……ã€æ¨¡å‹æƒé‡é‡åŒ–ä¸‰ç§ä¼˜åŒ–æŠ€æœ¯åœ¨å†…å­˜æ¶ˆè€—ä¸ä»»åŠ¡å‡†ç¡®ç‡é—´çš„æƒè¡¡å‰æ²¿ï¼›è¯†åˆ«å‡ºæ¨¡å‹ç‰¹å®šå¸•ç´¯æ‰˜æœ€ä¼˜é…ç½®ï¼Œå®ç°68-78%å†…å­˜å‡å°‘ä¸”ä»…1-3%å‡†ç¡®ç‡ä¸‹é™ã€‚",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºKV Paretoç³»ç»Ÿçº§æ¡†æ¶ï¼Œç³»ç»Ÿåˆ†æä¸‰ç§äº’è¡¥ä¼˜åŒ–æŠ€æœ¯çš„è”åˆæ•ˆåº”ä¸æƒè¡¡å…³ç³»",
      "è¯†åˆ«æ¨¡å‹ç‰¹å®šå¸•ç´¯æ‰˜æœ€ä¼˜é…ç½®ï¼Œå®ç°å¤§å¹…å†…å­˜å‹ç¼©ä¸”å‡†ç¡®ç‡æŸå¤±æå°"
    ],
    "processed_at": "2025-12-02T09:02:01.734598"
  },
  {
    "id": "2512.01748v1",
    "title": "SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models",
    "abstract": "Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.",
    "authors": [
      "Stella Etuk",
      "Ashraf Matrawy"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01748v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01748v1",
    "fetched_at": "2025-12-02T08:36:57.167018",
    "chinese_title": "SA-ADPï¼šå¤§è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿåº¦æ„ŸçŸ¥è‡ªé€‚åº”å·®åˆ†éšç§",
    "chinese_summary": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­éšç§æ³„éœ²é—®é¢˜ï¼Œä¼ ç»Ÿå·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆDP-SGDï¼‰å› å‡åŒ€åŠ å™ªç‰ºç‰²æ¨¡å‹æ•ˆç”¨ï¼Œæœ¬æ–‡æå‡ºSA-ADPæ–¹æ³•ï¼ŒåŸºäºä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰çš„æ•æ„Ÿåº¦è‡ªé€‚åº”åˆ†é…å™ªå£°ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨ä¿æŒå¼ºéšç§ä¿æŠ¤çš„åŒæ—¶ä¸é™ä½æ¨¡å‹æ•ˆç”¨ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºæ•æ„Ÿåº¦æ„ŸçŸ¥çš„è‡ªé€‚åº”å·®åˆ†éšç§æ–¹æ³•SA-ADPï¼ŒåŸºäºä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰çš„æ•æ„Ÿåº¦è‡ªé€‚åº”åˆ†é…å™ªå£°",
      "å®éªŒéªŒè¯SA-ADPåœ¨ä¿æŒå¼ºéšç§ä¿æŠ¤çš„åŒæ—¶ä¸é™ä½æ¨¡å‹æ•ˆç”¨ï¼Œç¼“è§£ä¼ ç»ŸDP-SGDçš„æ•ˆç”¨-éšç§æƒè¡¡"
    ],
    "processed_at": "2025-12-02T09:02:13.101861"
  },
  {
    "id": "2512.01392v1",
    "title": "RE-LLM: Integrating Large Language Models into Renewable Energy Systems",
    "abstract": "Energy system models are increasingly employed to guide long-term planning in multi-sectoral environments where decisions span electricity, heat, transport, land use, and industry. While these models provide rigorous quantitative insights, their outputs are often highly technical, making them difficult to interpret for non-expert stakeholders such as policymakers, planners, and the public. This communication gap limits the accessibility and practical impact of scenario-based modeling, particularly as energy transitions grow more complex with rising shares of renewables, sectoral integration, and deep uncertainties. To address this challenge, we propose the Renewable Energy Large Language Model (RE-LLM), a hybrid framework that integrates Large Language Models (LLMs) directly into the energy system modeling workflow. RE-LLM combines three core elements: (i) optimization-based scenario exploration, (ii) machine learning surrogates that accelerate computationally intensive simulations, and (iii) LLM-powered natural language generation that translates complex results into clear, stakeholder-oriented explanations. This integrated design not only reduces computational burden but also enhances inter-pretability, enabling real-time reasoning about trade-offs, sensitivities, and policy implications. The framework is adaptable across different optimization platforms and energy system models, ensuring broad applicability beyond the case study presented. By merging speed, rigor, and interpretability, RE-LLM advances a new paradigm of human-centric energy modeling. It enables interactive, multilingual, and accessible engagement with future energy pathways, ultimately bridging the final gap between data-driven analysis and actionable decision-making for sustainable transitions.",
    "authors": [
      "Ali Forootani",
      "Mohammad Sadr",
      "Danial Esmaeili Aliabadi",
      "Daniela Thraen"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01392v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01392v1",
    "fetched_at": "2025-12-02T08:36:57.167045",
    "chinese_title": "RE-LLMï¼šå°†å¤§è¯­è¨€æ¨¡å‹é›†æˆåˆ°å¯å†ç”Ÿèƒ½æºç³»ç»Ÿä¸­",
    "chinese_summary": "é’ˆå¯¹èƒ½æºç³»ç»Ÿæ¨¡å‹è¾“å‡ºæŠ€æœ¯åŒ–å¯¼è‡´éä¸“å®¶éš¾ç†è§£çš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºRE-LLMæ··åˆæ¡†æ¶ï¼Œæ•´åˆä¼˜åŒ–åœºæ™¯æ¢ç´¢ã€æœºå™¨å­¦ä¹ ä»£ç†åŠ é€Ÿæ¨¡æ‹ŸåŠLLMé©±åŠ¨çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œæ—¢é™ä½è®¡ç®—è´Ÿæ‹…åˆæå‡å¯è§£é‡Šæ€§ï¼Œæ”¯æŒå®æ—¶æ¨ç†æƒè¡¡ä¸æ”¿ç­– implicationsï¼Œä¸”è·¨å¹³å°é€‚ç”¨ã€‚",
    "tags": [
      "LLM",
      "NLP"
    ],
    "key_contributions": [
      "æå‡ºRE-LLMæ··åˆæ¡†æ¶ï¼Œæ•´åˆä¼˜åŒ–ã€æœºå™¨å­¦ä¹ ä»£ç†ä¸LLMï¼Œè§£å†³èƒ½æºç³»ç»Ÿæ¨¡å‹æŠ€æœ¯åŒ–è¾“å‡ºçš„æ²Ÿé€šéšœç¢",
      "æ¡†æ¶é™ä½è®¡ç®—è´Ÿæ‹…ã€å¢å¼ºå¯è§£é‡Šæ€§ï¼Œæ”¯æŒå®æ—¶æ¨ç†æƒè¡¡ä¸æ”¿ç­– implicationsï¼Œä¸”è·¨å¹³å°é€‚ç”¨"
    ],
    "processed_at": "2025-12-02T09:02:23.241721"
  },
  {
    "id": "2512.01326v1",
    "title": "Securing Large Language Models (LLMs) from Prompt Injection Attacks",
    "abstract": "Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.",
    "authors": [
      "Omar Farooq Khan Suri",
      "John McCrae"
    ],
    "published": "2025-12-01",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01326v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01326v1",
    "fetched_at": "2025-12-02T08:36:57.167067",
    "chinese_title": "ä¿æŠ¤å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…å—æç¤ºæ³¨å…¥æ”»å‡»",
    "chinese_summary": "æœ¬ç ”ç©¶è¯„ä¼°ä»»åŠ¡ç‰¹å®šå¾®è°ƒæ–¹æ³•JATMOå¯¹é—ä¼ æ”»å‡»æ¡†æ¶HOUYIçš„é²æ£’æ€§ï¼Œé€šè¿‡é€‚é…HOUYIï¼ˆå¼•å…¥è‡ªå®šä¹‰é€‚åº”åº¦è¯„åˆ†ã€ä¿®æ”¹å˜å¼‚é€»è¾‘ç­‰ï¼‰ï¼Œå¾®è°ƒLLaMA 2-7Bç­‰æ¨¡å‹å¹¶å¯¹æ¯”GPT-3.5-TurboåŸºçº¿ï¼Œå‘ç°JATMOå¯é™ä½æ”»å‡»æˆåŠŸç‡ä½†æ— æ³•å®Œå…¨é˜²å¾¡æç¤ºæ³¨å…¥ï¼Œä¸”å­˜åœ¨ç”Ÿæˆè´¨é‡ä¸æ³¨å…¥è„†å¼±æ€§çš„æƒè¡¡ï¼ŒæŒ‡å‡ºéœ€åˆ†å±‚å¯¹æŠ—æ€§ç¼“è§£ç­–ç•¥ã€‚",
    "tags": [
      "LLM",
      "NLP",
      "Deep Learning"
    ],
    "key_contributions": [
      "é€‚é…é—ä¼ æ”»å‡»æ¡†æ¶HOUYIï¼ˆè‡ªå®šä¹‰é€‚åº”åº¦è¯„åˆ†ã€ä¿®æ”¹å˜å¼‚é€»è¾‘ç­‰ï¼‰ï¼Œå®ç°å¯¹LLMé˜²å¾¡æ–¹æ³•æ›´å‡†ç¡®çš„è¯„ä¼°",
      "æ­ç¤ºä»»åŠ¡ç‰¹å®šå¾®è°ƒæ–¹æ³•JATMOçš„å±€é™æ€§ï¼ˆæ— æ³•å®Œå…¨æŠµå¾¡æç¤ºæ³¨å…¥ã€ç”Ÿæˆè´¨é‡ä¸è„†å¼±æ€§å­˜åœ¨æƒè¡¡ï¼‰ï¼ŒæŒ‡å‡ºéœ€åˆ†å±‚å¯¹æŠ—æ€§ç¼“è§£ç­–ç•¥"
    ],
    "processed_at": "2025-12-02T09:02:40.670672"
  },
  {
    "id": "2512.01099v1",
    "title": "Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems",
    "abstract": "As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.",
    "authors": [
      "Daria Smirnova",
      "Hamid Nasiri",
      "Marta Adamska",
      "Zhengxin Yu",
      "Peter Garraghan"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01099v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01099v1",
    "fetched_at": "2025-12-02T08:36:57.167094",
    "chinese_title": "LLMç¼–æ’çš„AIç³»ç»Ÿä¸­é¢å‘èƒ½è€—æ„ŸçŸ¥çš„æ•°æ®é©±åŠ¨æ¨¡å‹é€‰æ‹©",
    "chinese_summary": "æœ¬æ–‡é’ˆå¯¹LLMç¼–æ’AIç³»ç»Ÿæ—¶ä¾èµ–æ¨¡å‹å®šæ€§æè¿°å¯¼è‡´é€‰æ‹©æ¬¡ä¼˜ã€èƒ½è€—å¢åŠ çš„é—®é¢˜ï¼Œæå‡ºèƒ½è€—æ„ŸçŸ¥æ¨¡å‹é€‰æ‹©æ¡†æ¶GUIDEï¼Œé€šè¿‡çº³å…¥å®šé‡æ€§èƒ½ç‰¹å¾æƒè¡¡æ€§èƒ½ä¸èƒ½è€—ï¼›å®éªŒè¡¨æ˜è¯¥æ¡†æ¶å¯æå‡ä»»åŠ¡å‡†ç¡®ç‡0.90%-11.92%ã€æœ€é«˜æå‡54%èƒ½è€—æ•ˆç‡ï¼Œå¹¶å°†ç¼–æ’å»¶è¿Ÿä»4.51ç§’é™è‡³7.2æ¯«ç§’ã€‚",
    "tags": [
      "LLM",
      "Deep Learning"
    ],
    "key_contributions": [
      "å®è¯æ­ç¤ºLLM-basedæ¨¡å‹ç¼–æ’ä¾èµ–å®šæ€§æè¿°çš„å±€é™æ€§ï¼ˆæ¬¡ä¼˜é€‰æ‹©ã€å‡†ç¡®ç‡ä¸‹é™ã€èƒ½è€—å¢åŠ ï¼‰",
      "æå‡ºèƒ½è€—æ„ŸçŸ¥æ¨¡å‹é€‰æ‹©æ¡†æ¶GUIDEï¼Œç»“åˆå®šé‡æ€§èƒ½ç‰¹å¾å®ç°æ€§èƒ½-èƒ½è€—æƒè¡¡ï¼Œå®éªŒéªŒè¯å¤šç»´åº¦ä¼˜åŠ¿"
    ],
    "processed_at": "2025-12-02T09:02:56.456799"
  },
  {
    "id": "2512.00878v1",
    "title": "Less is More: Resource-Efficient Low-Rank Adaptation",
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.",
    "authors": [
      "Chunlin Tian",
      "Xuyang Wei",
      "Huanrong Liu",
      "Zhijiang Guo",
      "Li Li"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00878v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00878v1",
    "fetched_at": "2025-12-02T08:36:57.167126",
    "chinese_title": "å°‘å³æ˜¯å¤šï¼šèµ„æºé«˜æ•ˆçš„ä½ç§©è‡ªé€‚åº”",
    "chinese_summary": "é’ˆå¯¹ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å­˜åœ¨çš„èµ„æºå¼€é”€ä¸å‚æ•°å¹²æ‰°é—®é¢˜ï¼Œè®ºæ–‡ä»å±‚é—´åŠå±‚å†…å‚æ•°å†—ä½™è§†è§’æå‡ºèµ„æºé«˜æ•ˆçš„ä½ç§©è‡ªé€‚åº”æ–¹æ³•EffiLoRAï¼›è¯¥æ–¹æ³•é‡‡ç”¨ç»Ÿä¸€AçŸ©é˜µè·¨æ‰€æœ‰Transformerå±‚ï¼Œå¼•å…¥è¿è¡Œæ—¶é€‰æ‹©æ€§BçŸ©é˜µæ›´æ–°ä»¥åŠ¨æ€å¹³è¡¡ç³»ç»Ÿèµ„æºé¢„ç®—ä¸æ¨¡å‹æ€§èƒ½ï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ä¼˜äºLoRAã€‚",
    "tags": [
      "LLM",
      "Transformer",
      "Deep Learning"
    ],
    "key_contributions": [
      "æå‡ºèµ„æºé«˜æ•ˆçš„ä½ç§©è‡ªé€‚åº”æ–¹æ³•EffiLoRAï¼Œè§£å†³LoRAçš„èµ„æºå¼€é”€ä¸å‚æ•°å¹²æ‰°é—®é¢˜",
      "é‡‡ç”¨ç»Ÿä¸€AçŸ©é˜µè·¨å±‚+è¿è¡Œæ—¶é€‰æ‹©æ€§BçŸ©é˜µæ›´æ–°ï¼ŒåŠ¨æ€å¹³è¡¡èµ„æºé¢„ç®—ä¸æ€§èƒ½ï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šä¼˜äºLoRA"
    ],
    "processed_at": "2025-12-02T09:03:06.944615"
  },
  {
    "id": "2512.00837v1",
    "title": "WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models",
    "abstract": "Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.   In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\\% over state-of-the-art baselines at a watermark detectability strength of 95\\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\\% and 36.47\\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \\href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.",
    "authors": [
      "Yukang Lin",
      "Jiahao Shao",
      "Shuoran Jiang",
      "Wentao Zhu",
      "Bingjie Lu",
      "Xiangping Wu",
      "Joanna Siebert",
      "Qingcai Chen"
    ],
    "published": "2025-11-30",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00837v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00837v1",
    "fetched_at": "2025-12-02T08:36:57.167162",
    "chinese_title": "WaterSearchï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„è´¨é‡æ„ŸçŸ¥å‹åŸºäºæœç´¢çš„æ°´å°æ¡†æ¶",
    "chinese_summary": "è®ºæ–‡æå‡ºWaterSearchæ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶ç§å­æ± å®ç°æ°´å°æ–‡æœ¬çš„å¤šæ ·åŒ–å¹¶è¡Œç”Ÿæˆï¼Œè”åˆä¼˜åŒ–åˆ†å¸ƒä¿çœŸåº¦ä¸æ°´å°ä¿¡å·ç‰¹å¾ä»¥æå‡æ–‡æœ¬è´¨é‡ï¼›åŒæ—¶æå‡ºå…·æœ‰å¼ºæ”»å‡»é²æ£’æ€§çš„å¥å­çº§æ°´å°æ£€æµ‹æ–¹æ³•ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¤šLLMå’Œä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰SOTAæ–¹æ³•ã€‚",
    "tags": [
      "LLM",
      "NLP"
    ],
    "key_contributions": [
      "è®¾è®¡æ§åˆ¶ç§å­æ± çš„åµŒå…¥æ–¹æ¡ˆï¼Œæå‡ºå¥å­çº§åŸºäºæœç´¢çš„æ°´å°æ¡†æ¶WaterSearchï¼Œè”åˆä¼˜åŒ–åˆ†å¸ƒä¿çœŸåº¦ä¸æ°´å°ä¿¡å·ç‰¹å¾æå‡æ–‡æœ¬è´¨é‡",
      "æå‡ºå…·æœ‰å¼ºæ”»å‡»é²æ£’æ€§çš„å¥å­çº§æ°´å°æ£€æµ‹æ–¹æ³•ï¼Œå®éªŒéªŒè¯åœ¨å¤šLLMå’Œä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰SOTAåŸºçº¿"
    ],
    "processed_at": "2025-12-02T09:03:20.691027"
  },
  {
    "id": "2512.00134v1",
    "title": "Asm2SrcEval: Evaluating Large Language Models for Assembly-to-Source Code Translation",
    "abstract": "Assembly-to-source code translation is a critical task in reverse engineering, cybersecurity, and software maintenance, yet systematic benchmarks for evaluating large language models on this problem remain scarce. In this work, we present the first comprehensive evaluation of five state-of-the-art large language models on assembly-to-source translation. We assess model performance using a diverse set of metrics capturing lexical similarity (BLEU, ROUGE, and METEOR), semantic alignment (BERTScore), fluency (Perplexity), and efficiency (time prediction). Our results reveal clear trade-offs: while certain models excel in text similarity metrics, others demonstrate lower perplexity or faster inference times. We further provide qualitative analyses of typical model successes and failure cases, highlighting challenges such as control flow recovery and identifier reconstruction. Taken together, our benchmark offers actionable insights into the strengths and limitations of current large language models for program translation, establishing a foundation for future research in combining accuracy with efficiency for real-world applications.",
    "authors": [
      "Parisa Hamedi",
      "Hamed Jelodar",
      "Samita Bai",
      "Mohammad Meymani",
      "Roozbeh Razavi-Far",
      "Ali A. Ghorbani"
    ],
    "published": "2025-11-28",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00134v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00134v1",
    "fetched_at": "2025-12-02T08:36:57.167259",
    "chinese_title": "Asm2SrcEvalï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ±‡ç¼–åˆ°æºä»£ç ç¿»è¯‘ä¸­çš„è¡¨ç°",
    "chinese_summary": "è¯¥è®ºæ–‡é’ˆå¯¹æ±‡ç¼–åˆ°æºä»£ç ç¿»è¯‘ä»»åŠ¡ï¼Œé¦–æ¬¡å…¨é¢è¯„ä¼°5ä¸ªå½“å‰æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼›é‡‡ç”¨è¯æ³•ç›¸ä¼¼æ€§ã€è¯­ä¹‰å¯¹é½ã€æµç•…æ€§åŠæ•ˆç‡ç­‰å¤šç»´åº¦æŒ‡æ ‡è¯„ä¼°ï¼Œå¹¶ç»“åˆå®šæ€§åˆ†æå…¸å‹æ¡ˆä¾‹ï¼Œæ­ç¤ºæ¨¡å‹æ€§èƒ½çš„æƒè¡¡å…³ç³»ï¼Œä¸ºåç»­ç ”ç©¶æä¾›åŸºç¡€ã€‚",
    "tags": [
      "LLM",
      "Benchmark",
      "Transformer"
    ],
    "key_contributions": [
      "é¦–æ¬¡æ„å»ºæ±‡ç¼–åˆ°æºä»£ç ç¿»è¯‘ä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹ç»¼åˆè¯„ä¼°åŸºå‡†ï¼Œé‡‡ç”¨å¤šç»´åº¦æŒ‡æ ‡ä½“ç³»",
      "æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸­çš„æ€§èƒ½æƒè¡¡å…³ç³»ï¼Œå¹¶æŒ‡å‡ºæ§åˆ¶æµæ¢å¤ã€æ ‡è¯†ç¬¦é‡å»ºç­‰æ ¸å¿ƒæŒ‘æˆ˜"
    ],
    "processed_at": "2025-12-02T09:03:35.207386"
  },
  {
    "id": "2512.01565v1",
    "title": "Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding",
    "abstract": "We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.",
    "authors": [
      "Alex Oshin",
      "Rahul Vodeb Ghosh",
      "Augustinos D. Saravanos",
      "Evangelos A. Theodorou"
    ],
    "published": "2025-12-01",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.01565v1",
    "arxiv_url": "https://arxiv.org/abs/2512.01565v1",
    "fetched_at": "2025-12-02T08:37:20.919733",
    "chinese_title": "Deep FlexQPï¼šåŸºäºæ·±åº¦å±•å¼€çš„åŠ é€Ÿéçº¿æ€§è§„åˆ’æ–¹æ³•",
    "chinese_summary": "è®ºæ–‡æå‡ºåŸºäºäºŒæ¬¡è§„åˆ’ï¼ˆQPï¼‰çº¦æŸç²¾ç¡®æ¾å¼›çš„æ€»æ˜¯å¯è¡Œä¼˜åŒ–å™¨FlexQPï¼Œå¯å¤„ç†å¯è¡Œ/ä¸å¯è¡ŒQPå¹¶ç¨€ç–æœ€å°åŒ–çº¦æŸè¿åï¼›é€šè¿‡æ·±åº¦å±•å¼€å°†å…¶åŠ é€Ÿä¸ºDeep FlexQPï¼Œå­¦ä¹ ç»´åº¦æ— å…³åé¦ˆç­–ç•¥å®ç°æ³›åŒ–ï¼›åœ¨æŠ•èµ„ç»„åˆä¼˜åŒ–ç­‰åŸºå‡†é—®é¢˜ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æä¾›PAC Bayesæ³›åŒ–ä¿è¯ä»¥åŠ é€Ÿéçº¿æ€§æœ€ä¼˜æ§åˆ¶ç­‰ä»»åŠ¡ã€‚",
    "tags": [
      "Deep Learning",
      "Portfolio Optimization",
      "Benchmark"
    ],
    "key_contributions": [
      "æå‡ºæ€»æ˜¯å¯è¡Œçš„QPä¼˜åŒ–å™¨FlexQPï¼ŒåŸºäºçº¦æŸç²¾ç¡®æ¾å¼›æ”¯æŒå¯è¡Œ/ä¸å¯è¡ŒQPï¼Œæ˜“çƒ­å¯åŠ¨ä¸”é²æ£’æ€§å¼º",
      "æ„å»ºDeep FlexQPï¼Œé€šè¿‡æ·±åº¦å±•å¼€å­¦ä¹ ç»´åº¦æ— å…³åé¦ˆç­–ç•¥ï¼Œæ³›åŒ–åˆ°å¤§ç»´åº¦é—®é¢˜ä¸”è¿­ä»£æ•°è¶…è®­ç»ƒè§„æ¨¡",
      "æä¾›PAC Bayesæ³›åŒ–ä¿è¯ï¼Œåœ¨åŸºå‡†é—®é¢˜ä¸Šä¼˜äºç°æœ‰æ–¹æ³•å¹¶åŠ é€Ÿéçº¿æ€§æœ€ä¼˜æ§åˆ¶ç­‰å®é™…ä»»åŠ¡"
    ],
    "processed_at": "2025-12-02T09:03:50.661385"
  },
  {
    "id": "2512.00598v1",
    "title": "Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction",
    "abstract": "Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity.   Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups.   Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.",
    "authors": [
      "Yining Yuan",
      "J. Ben Tamo",
      "Wenqi Shi",
      "Yishan Zhong",
      "Micky C. Nnamdi",
      "B. Randall Brenn",
      "Steven W. Hwang",
      "May D. Wang"
    ],
    "published": "2025-11-29",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.00598v1",
    "arxiv_url": "https://arxiv.org/abs/2512.00598v1",
    "fetched_at": "2025-12-02T08:37:34.448341",
    "chinese_title": "å¼€å‘å…¬å¹³æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£ä»¥æé«˜è„ŠæŸ±èåˆæœ¯åå¹¶å‘ç—‡é¢„æµ‹çš„å…¬å¹³æ€§",
    "chinese_summary": "è®ºæ–‡æå‡ºFAIR-MTLå…¬å¹³æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨çš„éšå¼å­ç¾¤æ¨æ–­ï¼ˆæ— éœ€æ˜¾å¼æ•æ„Ÿå±æ€§ï¼‰ã€é€†é¢‘ç‡åŠ æƒå’Œæ­£åˆ™åŒ–ç¼“è§£å­ç¾¤ä¸å¹³è¡¡ï¼Œåœ¨è„ŠæŸ±èåˆæœ¯åå¹¶å‘ç—‡é¢„æµ‹ä¸­æ—¢æå‡é¢„æµ‹æ€§èƒ½ï¼ˆAUC0.86ã€å‡†ç¡®ç‡75%ï¼‰ï¼Œåˆæ˜¾è‘—é™ä½æ€§åˆ«ã€å¹´é¾„ç›¸å…³çš„å…¬å¹³æ€§åå·®ã€‚",
    "tags": [
      "Deep Learning",
      "Risk Management"
    ],
    "key_contributions": [
      "æå‡ºFAIR-MTLå…¬å¹³æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨æ•°æ®é©±åŠ¨å­ç¾¤æ¨æ–­æœºåˆ¶ï¼ˆæ›¿ä»£æ˜¾å¼æ•æ„Ÿå±æ€§ï¼‰ï¼Œç»“åˆé€†é¢‘ç‡åŠ æƒä¸æ­£åˆ™åŒ–ç¼“è§£å­ç¾¤åå·®é—®é¢˜",
      "åœ¨è„ŠæŸ±èåˆæœ¯åå¹¶å‘ç—‡é¢„æµ‹ä¸­ï¼ŒåŒæ—¶å®ç°é¢„æµ‹æ€§èƒ½æå‡ï¼ˆAUC0.86ã€å‡†ç¡®ç‡75%ï¼‰å’Œå…¬å¹³æ€§æ”¹å–„ï¼ˆæ€§åˆ«/å¹´é¾„å…¬å¹³æ€§æŒ‡æ ‡æ˜¾è‘—é™ä½ï¼‰"
    ],
    "processed_at": "2025-12-02T09:04:05.936586"
  }
]