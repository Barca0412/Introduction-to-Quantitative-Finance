[
  {
    "id": "2601.03794v1",
    "title": "An Algorithmic Framework for Systematic Literature Reviews: A Case Study for Financial Narratives",
    "abstract": "This paper introduces an algorithmic framework for conducting systematic literature reviews (SLRs), designed to improve efficiency, reproducibility, and selection quality assessment in the literature review process. The proposed method integrates Natural Language Processing (NLP) techniques, clustering algorithms, and interpretability tools to automate and structure the selection and analysis of academic publications. The framework is applied to a case study focused on financial narratives, an emerging area in financial economics that examines how structured accounts of economic events, formed by the convergence of individual interpretations, influence market dynamics and asset prices. Drawing from the Scopus database of peer-reviewed literature, the review highlights research efforts to model financial narratives using various NLP techniques. Results reveal that while advances have been made, the conceptualization of financial narratives remains fragmented, often reduced to sentiment analysis, topic modeling, or their combination, without a unified theoretical framework. The findings underscore the value of more rigorous and dynamic narrative modeling approaches and demonstrate the effectiveness of the proposed algorithmic SLR methodology.",
    "authors": [
      "Gabin Taibi",
      "Joerg Osterrieder"
    ],
    "published": "2026-01-07",
    "categories": [
      "q-fin.GN",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03794v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03794v1",
    "fetched_at": "2026-01-08T08:36:30.721911"
  },
  {
    "id": "2601.04191v1",
    "title": "Embedding Autonomous Agents in Resource-Constrained Robotic Platforms",
    "abstract": "Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.",
    "authors": [
      "Negar Halakou",
      "Juan F. Gutierrez",
      "Ye Sun",
      "Han Jiang",
      "Xueming Wu",
      "Yilun Song",
      "Andres Gomez"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04191v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04191v1",
    "fetched_at": "2026-01-08T08:38:32.765136"
  },
  {
    "id": "2601.04176v1",
    "title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation",
    "abstract": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.",
    "authors": [
      "Pietro de Oliveira Esteves"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04176v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04176v1",
    "fetched_at": "2026-01-08T08:38:32.765165"
  },
  {
    "id": "2601.04171v1",
    "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
    "abstract": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
    "authors": [
      "Mohit Raghavendra",
      "Anisha Gunjal",
      "Bing Liu",
      "Yunzhong He"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04171v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04171v1",
    "fetched_at": "2026-01-08T08:38:32.765189"
  },
  {
    "id": "2601.04137v1",
    "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test",
    "abstract": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.",
    "authors": [
      "Chun-Kai Fan",
      "Xiaowei Chi",
      "Xiaozhu Ju",
      "Hao Li",
      "Yong Bao",
      "Yu-Kai Wang",
      "Lizhang Chen",
      "Zhiyuan Jiang",
      "Kuangzhi Ge",
      "Ying Li",
      "Weishi Mi",
      "Qingpo Wuwu",
      "Peidong Jia",
      "Yulin Luo",
      "Kevin Zhang",
      "Zhiyuan Qin",
      "Yong Dai",
      "Sirui Han",
      "Yike Guo",
      "Shanghang Zhang",
      "Jian Tang"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04137v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04137v1",
    "fetched_at": "2026-01-08T08:38:32.765244"
  },
  {
    "id": "2601.04086v1",
    "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
    "abstract": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
    "authors": [
      "Jinbo Hao",
      "Kai Yang",
      "Qingzhen Su",
      "Yifan Li",
      "Chao Jiang"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04086v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04086v1",
    "fetched_at": "2026-01-08T08:38:32.765269"
  },
  {
    "id": "2601.04060v1",
    "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows",
    "abstract": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.",
    "authors": [
      "Jinwei Su",
      "Qizhen Lan",
      "Zeyu Wang",
      "Yinghui Xia",
      "Hairu Wen",
      "Yiqun Duan",
      "Xi Xiao",
      "Tianyu Shi",
      "Yang Jingsong",
      "Lewei He"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04060v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04060v1",
    "fetched_at": "2026-01-08T08:38:32.765302"
  },
  {
    "id": "2601.04052v1",
    "title": "Stable Language Guidance for Vision-Language-Action Models",
    "abstract": "Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \\textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \\textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \\textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.",
    "authors": [
      "Zhihao Zhan",
      "Yuhao Chen",
      "Jiaying Zhou",
      "Qinhan Lv",
      "Hao Liu",
      "Keze Wang",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.RO",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04052v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04052v1",
    "fetched_at": "2026-01-08T08:38:32.765331"
  },
  {
    "id": "2601.03851v1",
    "title": "Rethinking Table Pruning in TableQA: From Sequential Revisions to Gold Trajectory-Supervised Parallel Search",
    "abstract": "Table Question Answering (TableQA) benefits significantly from table pruning, which extracts compact sub-tables by eliminating redundant cells to streamline downstream reasoning. However, existing pruning methods typically rely on sequential revisions driven by unreliable critique signals, often failing to detect the loss of answer-critical data. To address this limitation, we propose TabTrim, a novel table pruning framework which transforms table pruning from sequential revisions to gold trajectory-supervised parallel search. TabTrim derives a gold pruning trajectory using the intermediate sub-tables in the execution process of gold SQL queries, and trains a pruner and a verifier to make the step-wise pruning result align with the gold pruning trajectory. During inference, TabTrim performs parallel search to explore multiple candidate pruning trajectories and identify the optimal sub-table. Extensive experiments demonstrate that TabTrim achieves state-of-the-art performance across diverse tabular reasoning tasks: TabTrim-8B reaches 73.5% average accuracy, outperforming the strongest baseline by 3.2%, including 79.4% on WikiTQ and 61.2% on TableBench.",
    "authors": [
      "Yu Guo",
      "Shenghao Ye",
      "Shuangwu Chen",
      "Zijian Wen",
      "Tao Zhang",
      "Qirui Bai",
      "Dong Jin",
      "Yunpeng Hou",
      "Huasen He",
      "Jian Yang",
      "Xiaobin Tan"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03851v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03851v1",
    "fetched_at": "2026-01-08T08:38:32.765365"
  },
  {
    "id": "2601.03811v1",
    "title": "EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging",
    "abstract": "Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks.",
    "authors": [
      "Jan Tagscherer",
      "Sarah de Boer",
      "Lena Philipp",
      "Fennie van der Graaf",
      "Dré Peeters",
      "Joeran Bosma",
      "Lars Leijten",
      "Bogdan Obreja",
      "Ewoud Smit",
      "Alessa Hering"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03811v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03811v1",
    "fetched_at": "2026-01-08T08:38:32.765398"
  },
  {
    "id": "2601.03731v1",
    "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level",
    "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.",
    "authors": [
      "Jia Li",
      "Yuxin Su",
      "Michael R. Lyu"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03731v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03731v1",
    "fetched_at": "2026-01-08T08:38:32.765418"
  },
  {
    "id": "2601.03630v1",
    "title": "Reasoning Model Is Superior LLM-Judge, Yet Suffers from Biases",
    "abstract": "This paper presents the first systematic comparison investigating whether Large Reasoning Models (LRMs) are superior judge to non-reasoning LLMs. Our empirical analysis yields four key findings: 1) LRMs outperform non-reasoning LLMs in terms of judgment accuracy, particularly on reasoning-intensive tasks; 2) LRMs demonstrate superior instruction-following capabilities in evaluation contexts; 3) LRMs exhibit enhanced robustness against adversarial attacks targeting judgment tasks; 4) However, LRMs still exhibit strong biases in superficial quality. To improve the robustness against biases, we propose PlanJudge, an evaluation strategy that prompts the model to generate an explicit evaluation plan before execution. Despite its simplicity, our experiments demonstrate that PlanJudge significantly mitigates biases in both LRMs and standard LLMs.",
    "authors": [
      "Hui Huang",
      "Xuanxin Wu",
      "Muyun Yang",
      "Yuki Arase"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03630v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03630v1",
    "fetched_at": "2026-01-08T08:38:32.765440"
  },
  {
    "id": "2601.03555v1",
    "title": "SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models",
    "abstract": "Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.   Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.   Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.",
    "authors": [
      "Yuxuan Jiang",
      "Francis Ferraro"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03555v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03555v1",
    "fetched_at": "2026-01-08T08:38:32.765461"
  },
  {
    "id": "2601.03525v1",
    "title": "VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation",
    "abstract": "Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \\textbf{VeRPO} (\\textbf{V}erifiable D\\textbf{e}nse \\textbf{R}eward \\textbf{P}olicy \\textbf{O}ptimization), a novel RL framework for code generation that synthesizes \\textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\\% gain in pass@1 with negligible time cost (< 0.02\\%) and zero GPU memory overhead.",
    "authors": [
      "Longwen Wang",
      "Xuan'er Wu",
      "Xiaohui Hu",
      "Yirui Liu",
      "Yuankai Fan",
      "Kaidong Yu",
      "Qizhen Weng",
      "Wei Xi",
      "Xuelong Li"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03525v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03525v1",
    "fetched_at": "2026-01-08T08:38:32.765493"
  },
  {
    "id": "2601.03513v1",
    "title": "Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day",
    "abstract": "Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.   We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.   Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science.",
    "authors": [
      "Yi Wang",
      "Zhenting Huang",
      "Zhaohan Ding",
      "Ruoxue Liao",
      "Yuan Huang",
      "Xinzijian Liu",
      "Jiajun Xie",
      "Siheng Chen",
      "Linfeng Zhang"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03513v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03513v1",
    "fetched_at": "2026-01-08T08:38:32.765527"
  },
  {
    "id": "2601.03512v1",
    "title": "Bootstrapping Code Translation with Weighted Multilanguage Exploration",
    "abstract": "Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components.",
    "authors": [
      "Yuhan Wu",
      "Huan Zhang",
      "Wei Cheng",
      "Chen Shen",
      "Jingyue Yang",
      "Wei Hu"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03512v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03512v1",
    "fetched_at": "2026-01-08T08:38:32.765554"
  },
  {
    "id": "2601.03948v1",
    "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification",
    "abstract": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.",
    "authors": [
      "Rui Sun",
      "Yifan Sun",
      "Sheng Xu",
      "Li Zhao",
      "Jing Li",
      "Daxin Jiang",
      "Chen Hua",
      "Zuo Bai"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.AI",
      "q-fin.TR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03948v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03948v1",
    "fetched_at": "2026-01-08T08:39:56.231297"
  },
  {
    "id": "2601.04160v1",
    "title": "All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection",
    "abstract": "We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.",
    "authors": [
      "Yuechen Jiang",
      "Zhiwei Liu",
      "Yupeng Cao",
      "Yueru He",
      "Ziyang Xu",
      "Chen Xu",
      "Zhiyang Deng",
      "Prayag Tiwari",
      "Xi Chen",
      "Alejandro Lopez-Lira",
      "Jimin Huang",
      "Junichi Tsujii",
      "Sophia Ananiadou"
    ],
    "published": "2026-01-07",
    "categories": [
      "cs.CL",
      "cs.CE",
      "q-fin.CP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.04160v1",
    "arxiv_url": "https://arxiv.org/abs/2601.04160v1",
    "fetched_at": "2026-01-08T08:42:28.526506"
  },
  {
    "id": "2601.03927v1",
    "title": "A comprehensive review and analysis of different modeling approaches for financial index tracking problem",
    "abstract": "Index tracking, also known as passive investing, has gained significant traction in financial markets due to its cost-effective and efficient approach to replicating the performance of a specific market index. This review paper provides a comprehensive overview of the various modeling approaches and strategies developed for index tracking, highlighting the strengths and limitations of each approach. We categorize the index tracking models into three broad frameworks: optimization-based models, statistical-based models and machine learning based data-driven approach. A comprehensive empirical study conducted on the S\\&P 500 dataset demonstrates that the tracking error volatility model under the optimization-based framework delivers the most precise index tracking, the convex co-integration model, under the statistical-based framework achieves the strongest return-risk balance, and the deep neural network with fixed noise model within the data-driven framework provides a competitive performance with notably low turnover and high computational efficiency. By combining a critical review of the existing literature with comparative empirical analysis, this paper aims to provide insights into the evolving landscape of index tracking and its practical implications for investors and fund managers.",
    "authors": [
      "Vrinda Dhingra",
      "Amita Sharma",
      "Anubha Goel"
    ],
    "published": "2026-01-07",
    "categories": [
      "q-fin.PM",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.03927v1",
    "arxiv_url": "https://arxiv.org/abs/2601.03927v1",
    "fetched_at": "2026-01-08T08:42:28.526571"
  }
]